# 2025｜零基礎 AI 入門！從 Wx+b 到熱門模型的完整之路！

- 系列原址：https://ithelp.ithome.com.tw/users/20152236/ironman/8357
- 預期篇數：30
- 實際整理篇數：30

## 目錄

- [Day 01 - 【Day 1】在2025年的現在我們該怎麼學習AI呢？](#day-01)
- [Day 02 - 【Day 2】前向傳播？單層感知器？WX+b究竟是什麼？](#day-02)
- [Day 03 - 【Day 3】如何找到模型的最佳解？](#day-03)
- [Day 04 - 【Day 4】不可微或梯度為零的函數為何無法在深度學習網路中做使用？](#day-04)
- [Day 05 - 【Day 5】當 Wx+b 不再孤單多層感知器的誕生](#day-05)
- [Day 06 - 【Day 6】單層劃不開的謎題靠多層來解](#day-06)
- [Day 07 - 【Day 7】PyTorch的威力， 20 行程式碼讓神經網路自己跑起來！](#day-07)
- [Day 08 - 【Day 8】卷積神經網路不是深度學習的「原創」？從影像處理出發的 AI 革命](#day-08)
- [Day 09 - 【Day 9】60% 準確率只是起點用 CNN 與 CIFAR-10 探索深度學習優化之路](#day-09)
- [Day 10 - 【Day 10】用一支「通用訓練器」打天下逐行理解開源Trainer的內容](#day-10)
- [Day 11 - 【Day 11】賦予 WX+b 時序感知力神經網路如何理解過去與未來](#day-11)
- [Day 12 - 【Day 12】「你真的懂LSTM嗎？」手刻雙向LSTM讓你從不會到秒懂！](#day-12)
- [Day 13 - 【Day 13】模型真的理解語言嗎？從 Seq2Seq 看 AI 如何學會翻譯](#day-13)
- [Day 14 - 【Day 14】模型記性差？Attention 來幫忙！](#day-14)
- [Day 15 - 【Day 15】Attention is All You Need？先別急來看看 LSTM 的最後一舞](#day-15)
- [Day 16 - 【Day 16】從零開始拆 Transformer，原來 Encoder 是這樣運作的！](#day-16)
- [Day 17 - 【Day 17】只懂 Wx + b 也能搞懂 BERT？當然可以！](#day-17)
- [Day 18 - 【Day 18】一篇文章讓你搞懂BERT預訓練任務與模型實作（MLM + NSP）](#day-18)
- [Day 19 - 【Day 19】看起來很簡單？BERT 實作假新聞分類超簡單教學](#day-19)
- [Day 20 - 【Day 20】Decoder 為何會胡說八道 Transformer 的生成機制與幻覺真相](#day-20)
- [Day 21 - 【Day 21】從 Wx+b 到能寫詩的模型GPT-2 的煉成](#day-21)
- [Day 22 - 【Day 22】不靠 Encoder？用 GPT-2 試試翻譯的可能性](#day-22)
- [Day 23 - 【Day 23】語音模型原來長這樣？Wx+b拆給你看Whisper 架構！](#day-23)
- [Day 24 - 【Day 24】LoRA 是什麼？一篇文章教你 Whisper 中文微調全流程！](#day-24)
- [Day 25 - 【Day 25】語言模型的認知轉向，GPT 系列中的提示學習與指令學習解析](#day-25)
- [Day 26 - 【Day 26】GPT 落伍了嗎？來看看 LLaMA 怎麼反向壓制參數怪獸](#day-26)
- [Day 27 - 【Day 27】RoPE(x) = cosθx + sinθ(-x)？LLaMA 3 的 Wx + b 的完整拆解](#day-27)
- [Day 28 - 【Day 28】弱智吧 is all you need？教AI聽懂亂流語言的奇幻旅程](#day-28)
- [Day 29 - 【Day 29】Decoder-only 模型也能搞定 NER？用 LLaMA3 找出個資](#day-29)
- [Day 30 - 【Day 30】不是模型變強是你變懂 Decoder-only 訓練中的那些事](#day-30)
---

<a id="day-01"></a>

## Day 01｜【Day 1】在2025年的現在我們該怎麼學習AI呢？

- 原文：https://ithelp.ithome.com.tw/articles/10380119
- 發佈時間：2025-09-15 16:57:02

前言
==

身為一個重度拖延症患者，每年參賽前我都立志要先屯好稿再開賽，但結果總是拖到最後一刻一字未動。今年是我第四次參賽，本以為狀況會跟以往一樣，不過這次卻完全不同。已經不像學生時期那樣能靠瘋狂熬夜硬撐，所以我帶著可能會斷賽的心態來面對。好在這次難度應該不算太高，就算時間緊迫，還是想拚一拚。

那麼就從今天開始展開今年的 30 天學習挑戰。

帶著好奇與決心，一起踏入 AI 的世界，邁出屬於我們的第一步吧！

這次所學習到的知識
=========

這次我們的目標，是學習如何正確開啟 `Hugging Face` 社群的程式碼世界。雖然 `Hugging Face` 為使用者帶來極高的便利性，將許多複雜概念高度封裝，但這也讓初學者容易忽略背後真正的原理與邏輯。為了彌補這樣的斷層，我們在接下來的 30 天裡，將採取理論與實作並進的方式，逐步建立 AI 的核心觀念。

這次同樣的會從 AI 的數學基礎「矩陣」談起，並透過 `Numpy` 動手實作，以打好根基。接著我們會進入 `PyTorch` 的世界，從最簡單的模型搭建開始，一步一步堆疊成更完整的深度學習模型。

在這個過程中，我會帶你閱讀多篇經典論文，並以**程式碼實現的方式拆解其中的數學與架構**，幫助你把艱澀的理論轉化為清晰可見的實作內容。**每一章節也會介紹不同的模型優化技巧**，深入分析各種方法的優劣與適用情境。

當你完整走完這 30 天的內容，你將擁有一套扎實的 AI 基礎能力。此時你不僅能夠看懂主流模型，更能有能力去分析並解決實務或研究中遇到的各種問題。而這種思考與拆解問題的能力，正是學習 AI 最核心、也最有價值的地方。

*   [留言 1](http://ithelp.ithome.com.tw/articles/10380119#reply)
*   [追蹤](https://ithelp.ithome.com.tw/users/login)
*   [檢舉](https://ithelp.ithome.com.tw/users/login)

[下一篇 【Day 2】前向傳播？單層感知器？WX+b究竟是什麼？](https://ithelp.ithome.com.tw/articles/10380992)

---

<a id="day-02"></a>

## Day 02｜【Day 2】前向傳播？單層感知器？WX+b究竟是什麼？

- 原文：https://ithelp.ithome.com.tw/articles/10380992
- 發佈時間：2025-09-16 14:39:13

前言
==

在開始學習之前，我們需要先理解一個最基本的概念，再複雜的模型，本質上都可以用 `WX + b` 來表示。這個公式是人工智慧中最基礎的基礎，但往往在任何一門 AI 課程裡，老師都不會直接告訴你這件事。今天我要分享的，就是這個公式究竟如何推理，並一步步導出正確答案。

單層感知器的數學公式
==========

單層感知器（Perceptron）是深度學習中最基礎、也是最早提出的模型之一。它能夠模擬邏輯閘（如 AND 與 OR）的運算。其數學形式可表示為

![Image 11: https://ithelp.ithome.com.tw/upload/images/20250917/20152236zZotsAMQgN.png](images/series-8357/day-02/20152236zZotsAMQgN-5557a2e85f088a26.png)

其中 𝑊 表示權重（weights）、 𝑋 表示輸入向量（inputs）、𝑏為偏置（bias），而在模型初始化時，權重通常會隨機設定，至於**偏置有時可以省略，因為它主要用於調整資料的偏向性**，我們可以先看到以下兩張圖片。

![Image 12: https://ithelp.ithome.com.tw/upload/images/20250917/20152236RqNsEWzrWO.png](images/series-8357/day-02/20152236RqNsEWzrWO-77f6826e881732a6.png)

在左圖中由於引入了偏置，決策邊界得以適當平移，因此能正確區分 OR 邏輯閘的輸入資料。相較之下右圖中的決策邊界被迫通過原點 (0,0)，使得模型無法正確表示 OR 邏輯，因而產生分類錯誤。因此我們可以知道偏置的核心作用在於**賦予模型調整決策邊界位置的彈性**，避免受限於原點，從而更準確地刻劃資料的分布特性，不過讓我們回到剛剛的公式

![Image 13: https://ithelp.ithome.com.tw/upload/images/20250917/20152236aDs8mVevrs.png](images/series-8357/day-02/20152236aDs8mVevrs-78f11f6ff8f17c9e.png)

在公式中可以看到，**權重會直接與輸入相乘**，因此它對模型輸出的影響遠大於偏置。這也代表著**即便偏置的初始設定不理想，模型依然能透過不斷訓練來調整權重，最終收斂到正確的答案。**

但對於模仿邏輯閘的動作，我們通過這樣的計算公式緊緊為很接近於1或0而已，實際上我們並沒有辦法完美貼近於1，因此在單層感知器上通常會使用階躍函數（Step Function）這一個`激勵函數（Activation Function）來進行轉換`，我們可以看到以下的數學式子

![Image 14: https://ithelp.ithome.com.tw/upload/images/20250917/20152236pmfxcezfcU.png](images/series-8357/day-02/20152236pmfxcezfcU-1bc3fab48bfe790b.png)

如此一來，我們便能完整地得到模型的輸出。而上述的計算流程，正是我們所稱的`前向傳播（Forward Propagation）`，這個過程的核心在於將輸入數據傳遞並轉換為輸出結果，進而計算出模型的預測值。以上就是我們在學習深度學習的第一步，理解模型的構造並理解該模型的前向傳播中個參數的含意。

下集預告
====

今天我們理解了「權重」的重要性。不過你可能還不清楚該如何去最佳化這個參數。而這正是我們明天要學習的新數學方法的核心，只需要運用國中階段的知識，就能掌握。

但在進入新內容之前，我們不妨先思考一個問題：如何找到一個參數的變動速度與方向？這個問題的答案，將會是我們理解最佳化的關鍵起點。

---

<a id="day-03"></a>

## Day 03｜【Day 3】如何找到模型的最佳解？

- 原文：https://ithelp.ithome.com.tw/articles/10381929
- 發佈時間：2025-09-17 15:43:43

前言
==

在學習深度學習模型的時候，很多同學常常會有一個疑問，模型是怎麼一步一步變得「更聰明」的呢？其實模型的進步並不是一次到位，而是透過不斷修正與調整來完成的。就好比學生做題目一樣，先嘗試寫答案，再根據老師批改的錯誤慢慢修正，最後才會越來越接近正確答案。

今天我們要談的，就是這個「修正錯誤」的過程，也就是所謂的模型最佳化。它的核心原理在於計算目標值（正確答案）與預測值（模型的答案）之間的誤差，然後利用這些誤差去更新模型的權重。透過這樣的循環，模型會一步一步學會如何更準確地做出預測，這個步驟也是整個深度學習訓練中最關鍵的一環。

模型如何計算誤差
========

昨天我們學習了模型的前向傳播，了解了模型是如何計算出答案的。但僅僅有答案還不夠，我們還需要知道「模型答得好不好」。這時候就需要引入`損失函數（Loss Function）`。

損失函數的作用，是透過比較`模型的預測值（Prediction）`與`真實的目標值（Target）`，計算出它們之間的差距，這個差距就稱為`損失（Loss）`。損失越小，代表模型的預測越接近正確答案；反之損失越大，就表示模型還有很多地方需要改進，而其中我們最基本的損失函數就是`均方誤差(Mean square error，MSE)`，其數學公式為

![Image 14: https://ithelp.ithome.com.tw/upload/images/20250917/20152236ZcYPWaMr8n.png](images/series-8357/day-03/20152236ZcYPWaMr8n-f3b3bf3f896f489a.png)

這種「利用目標值來指導模型學習」的方式，就是`監督式學習（Supervised Learning）`。你可以把它想像成一個老師在批改作業：模型寫出答案後，老師（損失函數）會根據正確答案指出錯誤的地方（損失），再幫助模型一步一步修正，直到能更精準地回答問題。

單層感知器的反向傳播
==========

在單層感知器中我們的模型輸出可以表示為`WX+b`，其中，權重 𝑊 是需要透過訓練不斷更新與優化的核心參數。

優化的依據就是損失函數，也就是模型預測與真實標籤之間的差距。因此訓練的目標就是讓權重找到能夠使損失函數最小化的位置。我們可以將損失函數隨權重變化的關係繪製成如下圖所示的曲線：

![Image 15: https://ithelp.ithome.com.tw/upload/images/20250917/20152236ykstUqvoH7.png](images/series-8357/day-03/20152236ykstUqvoH7-8ce6b099e7293100.png)

從圖中可以看到，損失函數會隨著權重 𝑊 的不同而變化，而我們的目標就是找到曲線的最低點。那麼如何讓權重一步步移動到這個最低點呢？

答案是我們需要知道當前權重的**運動方向**與**速度**，這可以透過**損失函數對權重的偏導數**來獲得，也就是計算出權重的`梯度（Gradient）`其中方向由梯度的正負號決定，指引權重該往左還是往右移動。速度則由梯度的絕對值大小決定，代表當前坡度的陡峭程度。

因此在反向傳播的第一步就是計算出`∂L/∂W`，但因損失函數為複合函數，因此我們先要使用連鎖率展開其算式，因此我們可以整理出以下公式：

![Image 16: https://ithelp.ithome.com.tw/upload/images/20250917/201522365sHn29zGuB.png](images/series-8357/day-03/201522365sHn29zGuB-85490a08a1819185.png)

其中∂L/∂y^

![Image 17: https://ithelp.ithome.com.tw/upload/images/20250917/20152236QLOPYQIl3n.png](images/series-8357/day-03/20152236QLOPYQIl3n-ea866494b9cc7dee.png)

而∂y^/∂W

![Image 18: https://ithelp.ithome.com.tw/upload/images/20250917/20152236bjnHLo2DYi.png](images/series-8357/day-03/20152236bjnHLo2DYi-3ffb41fc98646ec5.png)

因此我們可得單層感知器的梯度公式為

![Image 19: https://ithelp.ithome.com.tw/upload/images/20250917/20152236LD2TRSUsCv.png](images/series-8357/day-03/20152236LD2TRSUsCv-6ad9a9c015e413b6.png)

在推導出梯度公式後，我們便能利用梯度的大小來調整權重的 更新方向 與 更新速度。

但需要特別注意的是，如果更新步伐過大，雖然移動速度很快，卻可能因為跨越谷底而錯失最佳解，甚至在損失函數曲線上不斷震盪、無法收斂。

為了避免這種情況，我們引入一個稱為`學習率 (learning rate)`的超參數，用來控制每次更新的幅度。

基於此，`梯度下降法 (Gradient Descent)`的更新公式為：

![Image 20: https://ithelp.ithome.com.tw/upload/images/20250917/201522366FaXTahx0i.png](images/series-8357/day-03/201522366FaXTahx0i-f6f831347bccf89b.png)

> η：學習率，控制更新步伐大小

因此選擇合適的學習率是梯度下降能否順利找到最佳權重的關鍵。至此我們已完整說明了反向傳播的流程與數學推導。在深度學習的訓練過程中，模型透過不斷重複的前向傳播與反向傳播，不斷更新權重，最終找到最佳解。如此一來，模型便能根據不同的輸入資料，產生正確且對應的輸出結果。

下集預告
====

學習的關鍵在於「眼到、口到、心到、手到」。目前我們仍缺乏「手到」這一步的實踐，因此明天將示範如何使用 NumPy 來模擬各種邏輯閘，並觀察不同學習率與偏置對結果所帶來的影響。這一環節對於後續模型的優化具有至關重要的意義。

---

<a id="day-04"></a>

## Day 04｜【Day 4】不可微或梯度為零的函數為何無法在深度學習網路中做使用？

- 原文：https://ithelp.ithome.com.tw/articles/10382401
- 發佈時間：2025-09-18 11:39:32

前言
==

在我們昨天的做法中其實是後續的改良方法，因為單層感知器並不是透過**損失函數與梯度下降**來更新權重，而是依照`感知器學習規則 (Perceptron Learning Rule)`進行調整。這種規則的邏輯是當預測輸出為 0、而目標值為 1 時，就會增加權重，讓輸出更接近 1。

但是這樣的更新方式其實相對直觀卻顯得粗糙也缺乏靈活性，因為我們不可能對每個標籤產生新的規則，且也缺乏嚴謹的數學意義，因此我將透過程式實作，讓你更直觀地理解昨天的方式為何能夠取代感知器學習規則。

單層感知器實作(以OR為範例)
===============

1. 準備資料
-------

在 Python 中，`[]` 表示的是`列表 (list)`，而不是數學上的`矩陣（Array）`，所以如果要進行矩陣或向量計算，建議使用 `NumPy` 函式庫，讓我我們方便後續的運算，因此我們的輸入與標籤可以如此定義。輸入與輸出資料：

```python
import numpy as np

x_data = np.array([
    [0, 0],
    [0, 1],
    [1, 0],
    [1, 1]
])

y_data = np.array([0, 1, 1, 1])

print("x_data shape:", x_data.shape)
print("y_data shape:", y_data.shape)
```

輸出結果：

```python
x_data shape: (4, 2)
y_data shape: (4,)
```

在這個步驟中，我們必須先弄清楚輸入資料 `x_data` 的實際意義。這一點非常重要，因為在任何模型中，都需要理解每個維度所代表的涵義，否則在設計或使用模型時，可能會因為誤解輸入而導致錯誤。

對於 `x_data`其維度為 `(data_size, feature)`，表示共有 4 筆資料 (data_size=4)，而每筆資料包含 2 個特徵 (feature=2)。對於 `y_data` 則是 4 個對應的標籤值 (答案)，對應 `x_data` 的 4 筆資料。

2. 定義模型前向傳播
-----------

在單層感知器中，模型在前向傳播時需要兩個可訓練的參數權重與偏置。因此在模型初始化時，我們必須先定義這兩個參數以便後續使用。接著我們需要定義前向傳播的計算方式，也就是透過公式`y=f(WX+b)`完成計算，因此我們的模型可以如此定義

```python
class Perceptron:
    def __init__(self, input_size):
        # 隨機初始化w與b
        self.w = np.random.randn(input_size)
        self.b = np.random.randn()           # 可為0
        
    def forward(self, x):
        logit = np.dot(x, self.w) + self.b   # 前向傳播公式
                y = (logit >= 0).astype(int)

        return logit

    def __call__(self, x):
        return self.forward(x)

    
model = Perceptron(input_size=x_data.shape[1])
y_pred = model(x_data)
output = (y_pred < 0).astype(int) # 激勵函數
print(output)
```

輸出結果：

```csharp
[0 0 0 0] # 不一定是這個因為權重是隨機初始化
```

在這裡可以看到，模型的輸出結果並非我們預期的`[0 1 1 1]`，所以接下來的步驟就是調整模型的權重來改善預測表現。在程式設計上我們透過覆寫 `__call__` 方法來呼叫 `forward`，如此一來，就能以 `model(...)` 的形式呼叫，而不需要明確撰寫 `model.forward(...)`，讓程式碼更加簡潔直觀。

3. 計算參數梯度
---------

在這裡我們採用 MSE Loss作為損失函數。它的主要作用是計算模型輸出與真實值之間的誤差，並進一步獲得該誤差對權重與偏置的梯度方向與大小。由於我們在昨日已經推導出完整並簡化後的公式，因此這裡可以直接套用簡化後的過程進行計算：

```python
def perceptron_grad(x, y_pred, y_true):
    err = (y_pred - y_true)
    grad_w = x.T @ err / x.shape[0]   # 平均每個損失
    grad_b = err.mean()               # 平均每個損失
    return grad_w, grad_b

grad_w, grad_b = perceptron_grad(x_data, y_pred, y_data)
print('權重梯度:', grad_w)
print('偏置梯度:', grad_b)
```

輸出結果：

```makefile
權重梯度: [0. 0.]
偏置梯度: 0.25
```

與先前逐筆樣本計算不同，這裡我們採用了`批量（Batch`運算。在批量模式下，模型同時處理多筆樣本資料，因此每一步所回傳的損失值必須取**平均值**，避免因樣本數量而影響梯度的大小。

3. 更新模型參數
---------

前面我們已經成功計算出權重與偏置的梯度，接下來的步驟就是利用梯度下降法來更新參數，這裡我們透過建立一個簡單的類別，將學習率與更新規則封裝起來，並觀察執行前後的差異：

```python
class GD:
    def __init__(self, model, lr=1e-3):
        self.model = model
        self.lr = lr

    def step(self, grad_w, grad_b):
        self.model.w -= self.lr * grad_w
        self.model.b -= self.lr * grad_b
        
optimizer = GD(model, lr=0.1)

y_pred = model(x_data)

print("更新前的 w:", model.w)
print("更新前的 b:", model.b)
optimizer.step(grad_w, grad_b)
print("更新後的 w:", model.w)
print("更新後的 b:", model.b)
```

輸出結果：

```yaml
更新前的 w: [1.25326675 1.9396813 ]
更新前的 b: 1.4974228700042376
更新後的 w: [1.25326675 1.9396813 ]
更新後的 b: 1.4724228700042377
```

在反向傳播完成後，我們可以觀察到偏置已經更新，但權重卻完全沒有變動。這代表傳入的 `grad_w` 幾乎是一個全接近零的陣列。**對於淺層網路來說，這種情況通常與初始化方式、學習率或激勵函數有關**。

在這裡問題的根源來自所使用的階梯函數作為激勵函數。由於**它在 0 處不可微分，且在其他區域梯度為零，導致權重無法透過梯度下降有效更新。** 正因如此這類不可微或梯度為零的函數在深度學習中幾乎已經被淘汰。

4.進行模型訓練
--------

不過由於我們的任務還是較簡單，因此我們還是能夠正常訓練出模型，而在訓練模型的方式我們只需要設定訓練次數並重複使用上述動作即可完成最基本的步驟。

```pyhon
epochs = 100
for epoch in range(epochs):
    y_pred = model(x_data)  # 前向傳播
    grad_w, grad_b = perceptron_grad(x_data, y_pred, y_data)  # 計算梯度
    optimizer.step(grad_w, grad_b)  # 更新參數

print('訓練完成！')
y_pred = model(x_data)  # 預測結果
print(y_pred)
```

輸出結果：

```csharp
訓練完成！
[0 1 1 1]
```

到這裡，我們就已經成功實作出一個符合前兩天所學理論的簡單 AI 模型，而本次的完整程式碼將會放置在[這裡](https://github.com/AUSTIN2526/learning-wx-b-in-30-days)後續有相關內容也會持續更新在這一儲存庫中！

下集預告
====

明天我們將進一步探討單層感知器的延伸以及其背後的數學公式，並更清楚地體現其中的線性組合WX+b的意義。同時我也會介紹另一種激勵函數的使用方式，讓大家比較在 0 處可微與不可微 的差異，進一步理解為什麼選擇適當的激勵函數對模型訓練如此重要。

---

<a id="day-05"></a>

## Day 05｜【Day 5】當 Wx+b 不再孤單多層感知器的誕生

- 原文：https://ithelp.ithome.com.tw/articles/10383078
- 發佈時間：2025-09-19 15:59:35

前言
==

昨天我們只用了一個 `Wx + b` 就能模擬出邏輯閘的運作情況。那麼，如果將多個 `Wx + b` 組合起來呢？這樣是否能處理更複雜的任務？答案是肯定的。第一個採用這種思路的方法，就被稱為`多層感知器（MLP, Multi-Layer Perceptron）`。

多層感知器的前向傳播
==========

1.輸入層->隱藏層
----------

![Image 20: https://ithelp.ithome.com.tw/upload/images/20250919/20152236xAhfscHbTs.png](images/series-8357/day-05/20152236xAhfscHbTs-678e2d5ebc722344.png)

與單層感知器相比，多層感知器中的每一個隱藏層，都可以看作是由多個單層感知器的輸出所組成，就像圖片中的 `h1 ~ h4` 這些節點，其實就是單層感知器的輸出，而在多層感知器裡，這些組合起來的層被稱為隱藏層。在隱藏層中，我們同樣可以選擇不同的激勵函數，其中最常見的就是 `ReLU（Rectified Linear Unit）`。

![Image 21: https://ithelp.ithome.com.tw/upload/images/20250919/20152236hRPLAfaXO6.png](images/series-8357/day-05/20152236hRPLAfaXO6-9c1a29b7b1d1da5a.png)

在上一章節提到的階梯函數中，由於在 0 的位置不可微，會導致在反向傳播過程中導數非常小甚至為 0。這樣一來，梯度在逐層傳遞時會不斷縮小最終幾乎消失。這種情況被稱為 `梯度消失（vanishing gradient）`。當梯度消失時，前層的權重幾乎無法更新，使得神經網路難以學到有效的特徵，讓我們先看看ReLU的公式。

![Image 22: https://ithelp.ithome.com.tw/upload/images/20250919/20152236osxdQoHEGM.png](images/series-8357/day-05/20152236osxdQoHEGM-f7ec79b6d1372e00.png)

在這一個公式中，它的導數x>0 時為 1，在 x≤0 時為 0，這表示在正區域中，梯度不會趨近於 0，能有效避免梯度消失問題，這時我們輸入層->隱藏層(使用ReLU)的公式是

![Image 23: https://ithelp.ithome.com.tw/upload/images/20250919/20152236D7erQ4gW9i.png](images/series-8357/day-05/20152236D7erQ4gW9i-d4bb9ed490ed5eca.png)

2.隱藏層->輸出層
----------

而在隱藏層到輸出層時，常依任務性質選擇不同的激活函數，若是二分類問題，通常使用 sigmoid，因為它能將輸出壓縮到 (0,1)，自然解釋為正類的機率；若是多分類問題，則會使用 softmax，因為它能將輸出轉換成一個總和為 1 的機率分布，表示樣本屬於每一類的相對可能性。

![Image 24: https://ithelp.ithome.com.tw/upload/images/20250919/20152236C6R6u9JxjD.png](images/series-8357/day-05/20152236C6R6u9JxjD-aa9843638270024e.png)

在圖片中我們可以直觀的看到左邊是 Sigmoid 函數，輸入在 (−10,10) 區間，輸出壓縮在 (0,1)；右邊是 Softmax 函數，模擬三個類別的輸出，可以看到隨著輸入變化，三個類別的機率會動態分配，且總和始終為 1。兩者對應的公式如下

![Image 25: https://ithelp.ithome.com.tw/upload/images/20250919/20152236o4BSiHbhZi.png](images/series-8357/day-05/20152236o4BSiHbhZi-b536716bd0614bac.png)

而在這裡我們使用Sigmoid，因此我們的隱藏層到輸出層的數學公式可以寫成

![Image 26: https://ithelp.ithome.com.tw/upload/images/20250919/20152236f8MlRTxkIi.png](images/series-8357/day-05/20152236f8MlRTxkIi-5b45074941b1be4f.png)

多層感知器的反向傳播
==========

在前巷傳播中我們的流程是輸入層(x)->隱藏層(h)->隱藏層激勵函數(a)->輸出層(z)->輸出層激勵函數(y)->損失函數，因此我們在反向傳播時需要將這個返回來計算，因此讓我們看看第一步，在這裡我們損失函數同樣是MSE Loss

1. 損失函數對輸出層激勵函數
---------------

首先計算損失對輸出層輸出的偏導，這一步是誤差的來源，代表我們要最小化的方向。

![Image 27: https://ithelp.ithome.com.tw/upload/images/20250919/20152236lqdAVeyzCj.png](images/series-8357/day-05/20152236lqdAVeyzCj-c86a50251957eccc.png)

2. 損失函數對輸出層權重
-------------

接下來透過連鎖率展開，其中我們會使用到上一層的偏導結果。

![Image 28: https://ithelp.ithome.com.tw/upload/images/20250919/201522368KVMsYrAR9.png](images/series-8357/day-05/201522368KVMsYrAR9-a3dc5332ca460096.png)

3. 損失函數對隱藏層激勵函數
---------------

誤差往前傳，會傳到隱藏層激勵輸出(a)，這是輸出層誤差「回流」給隱藏層的梯度。

![Image 29: https://ithelp.ithome.com.tw/upload/images/20250919/20152236SYpxN3AvVv.png](images/series-8357/day-05/20152236SYpxN3AvVv-b0d0f8993627bff3.png)

4. 損失函數對隱藏層權重
-------------

同樣使用連鎖率展開

![Image 30: https://ithelp.ithome.com.tw/upload/images/20250919/20152236NmKWEgz5xT.png](images/series-8357/day-05/20152236NmKWEgz5xT-4431fed685710225.png)

5. 損失函數對輸入層權重
-------------

如果前面還有更深的層，誤差會繼續往前傳，形式會一直套用這就是「誤差反向傳遞」的一般公式。

![Image 31: https://ithelp.ithome.com.tw/upload/images/20250919/201522365yXpsXOlyP.png](images/series-8357/day-05/201522365yXpsXOlyP-0ad7f3db7b210d49.png)

6.結合上述展開
--------

總結來說整個誤差傳遞可以用以下公式表示

![Image 32: https://ithelp.ithome.com.tw/upload/images/20250919/20152236uqVN5zTEhd.png](images/series-8357/day-05/20152236uqVN5zTEhd-51ea2730fde387cd.png)

這樣就得到了完整的反向傳播公式。其實你會發現我們只是將前向傳播的流程反過來計算，並一步步地帶入各個結果。當你理解了這一點後，就能夠自行推導出各種 AI 模型的傳播與訓練過程。

下集預告
====

今天我們透過數學公式與推導，完整的了解多層感知器的前向傳播與反向傳播流程，理解了梯度如何一層層回傳，讓模型能逐步修正權重。明天我將帶你實際用程式碼一步步實現這個過程，這時你將會發現，數學公式與程式碼是一一對應的，當理解了推導之後，實作就只是把公式「翻譯」成程式語言。到時候你會真正體會到模型是如何一步步學會的。

---

<a id="day-06"></a>

## Day 06｜【Day 6】單層劃不開的謎題靠多層來解

- 原文：https://ithelp.ithome.com.tw/articles/10384245
- 發佈時間：2025-09-20 19:53:49

前言
==

在 Day 4 中，如果你嘗試將 OR 與 AND 換成其他邏輯閘，就會發現 XOR 與 XNOR 無法用單層感知器實現。原因是單層感知器的本質，其實就是在二維平面上劃出一條直線，把資料分開；然而 XOR 與 XNOR 的特性，並無法用單一直線完成區分。這也就是為什麼昨天提到的多層感知器特別重要，它能透過隱藏層增加非線性，使得模型能在更高維度的空間中進行分類，進而解決這類問題，而在今天我要來教你該如何用 NumPy 產生多層感知器的神經網路結構。

多層感知器辨別XOR
==========

1. 準備資料
-------

準備資料的方式與Day 4一樣，不過在這裡我們要將y_data給更換成XOR的邏輯，這樣才能進行監督是學習。

```lua
x_data = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)
y_data = np.array([0,1,1,0], dtype=float)
```

2. 定義模型前向傳播
-----------

在建立神經網路模型時，**網路層數的加深會顯著提高對參數設定的敏感性**。若參數選擇不當，模型可能在訓練過程中出現無法收斂的情況，影響預測效果。在本次實作的多層感知器模型中，我們選用了 ReLU 作為隱藏層的激勵函數。由於 ReLU 的輸出特性會將所有小於零的數值歸零，因此在初始化偏置項時，我們刻意加入一個微小的正值，以減少`神經元死亡（Dead Neuron）`的風險，確保隱藏層的神經元仍能有效參與學習。

> 所謂「神經元死亡（Dead Neuron）」，是指在訓練過程中，某些使用 ReLU 激勵函數的神經元長期輸出為 0，進而無法參與誤差反向傳播，對模型的學習與預測毫無貢獻。

我們可以先定義 ReLU 與輸出層使用的 sigmoid 函數，讓後續模型建構時更加清楚前向傳播的過程。

```python
def relu(x):
    return np.maximum(0, x)

def sigmoid(x):
    x_clip = np.clip(x, -60, 60)
    return 1.0 / (1.0 + np.exp(-x_clip))
```

接下來在初始化權重時我們採用 `He 初始化方法（He initialization）`，這有助於保持訊息在前向傳播過程中的穩定性。He 初始化是專門為 ReLU 類激活函數設計的權重初始化方式，核心想法是由於 ReLU 會將一半輸入壓成 0，若不調整權重分佈，訊號的方差會在逐層傳遞時衰減，因此該設計的目的是經過 ReLU 後，讓輸出的方差大致與輸入保持一致，避免梯度消失或爆炸。

> He, K., Zhang, X., Ren, S., & Sun, J. (2015). Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision (pp. 1026-1034).

```python
class MLP:
    def __init__(self, input_dim, hidden_dim):
        self.W1 = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)
        self.b1 = np.full((1, hidden_dim), 0.01)  # 微小正值，避免神經元死亡
        self.W2 = np.random.randn(hidden_dim, 1) * np.sqrt(2.0 / hidden_dim)
        self.b2 = np.zeros((1, 1))

    def forward(self, X):
        self.x = X
        self.h_pre = X @ self.W1 + self.b1
        self.h = relu(self.h_pre)
        self.z = self.h @ self.W2 + self.b2
        self.y = sigmoid(self.z)
        return self.y

    __call__ = forward
```

而這樣的設計主要就能讓模型可以讓後續有更好的訓練基礎

2. 模型梯度計算
---------

同樣地我們會先逐步拆解出 ReLU 的梯度以及 Sigmoid 的梯度推導公式以方便後續使用。

```python
def relu_grad(x):
    g = np.zeros_like(x)
    g[x > 0] = 1.0
    return g

def sigmoid_grad(s):
    return s * (1.0 - s)
```

接下來我們將昨日推導出的反向傳播公式整合進模型中。但需要注意這次的設計是將計算梯度的邏輯直接寫在模型的內部方法中，這樣做可以讓參數的更新流程更為簡潔和集中。

```python
def compute_grads(self, y_true):
    # 均方誤差 (MSE)：L = (1/N) * sum((y - t)^2)
    y_true = y_true.reshape(-1, 1)
    y_pred = self.y
    N = y_pred.shape[0]

    # 計算 Loss 值
    loss_val = np.mean((y_pred - y_true) ** 2)

    # 反向傳播
    dy = (y_pred - y_true)
    dz = dy * sigmoid_grad(y_pred)

    dW2 = self.h.T @ dz
    db2 = np.sum(dz, axis=0, keepdims=True)

    dh = dz @ self.W2.T
    dh_pre = dh * relu_grad(self.h_pre)

    dW1 = self.x.T @ dh_pre
    db1 = np.sum(dh_pre, axis=0, keepdims=True)

    grads = {"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2}
    return grads, loss_val
```

3. 定義優化器(梯度下降法)
---------------

接下來，我們只需實作梯度下降法來更新模型參數。本模型中需要調整的參數包括輸入層到隱藏層的權重與偏差（W1 與 `b1`），以及隱藏層到輸出層的權重與偏差（`W2` 與 `b2`）。

```python
class GD:
    def __init__(self, model, lr=0.05):
        self.model = model
        self.lr = lr

    def step(self, grads):
        self.model.W2 -= self.lr * grads["dW2"]
        self.model.b2 -= self.lr * grads["db2"]
        self.model.W1 -= self.lr * grads["dW1"]
        self.model.b1 -= self.lr * grads["db1"]
```

4. 開始訓練模型並預測
------------

在這裡與先前相比變化不大，只是將 `compute_grads` 移入模型內部。這樣一來，模型在輸出時就不必再依賴內部參數來進行計算。我們只需將 `y_data` 傳入 `compute_grads`，就能透過優化器直接更新模型參數。

```go
epochs = 5000
for epoch in range(epochs):
    _ = model(x_data)
    grads = model.compute_grads(y_data)
    optimizer.step(grads)
    
print("訓練完成！")
y_pred = model(x_data)
print("Raw predictions:", y_pred.ravel())
print("Pred labels   :", (y_pred >= 0.5).astype(int).ravel())
print("True labels   :", y_data.astype(int))
```

輸出結果：

```less
訓練完成！
Raw predictions: [0.03090754 0.96877249 0.97453328 0.02507739]
Pred labels   : [0 1 1 0]
True labels   : [0 1 1 0]
```

由於我們的模型使用了 Sigmoid 函數作為輸出層的啟用函數，因此預測結果會被壓縮到 0 到 1 之間。正因如此，在進行分類判斷時，我們通常會以 0.5 作為分界點——大於 0.5 的視為正類（1），小於等於 0.5 的則歸為負類（0）。

而這樣子我們就可以看到，模型已經能學會處理比單層感知器更高維度、更複雜的預測結果，該模型不僅能解決 XOR 這類經典的非線性問題，它的設計理念也成為現今許多深度學習模型的重要基礎，這一點我們會在後續持續地看見其方式的身影。

下集預告
====

到目前為止，我們一步步推導出多層感知器如何解決 XOR 問題，從數學公式到 NumPy 實作都完整走過了一遍。雖然這樣能幫助我們理解深度學習的本質，但你可能也發現了 **光是手動推導梯度與更新規則，就已經相當繁瑣且耗時。如果每次要做更複雜的模型都得如此，效率會非常低。**

這也是為什麼我們需要更高效的深度學習框架。因此明天開始我將帶你安裝並使用 PyTorch，重新構建一次 MLP 模型來解決 XOR 問題。透過 PyTorch，你將能體驗到自動微分與高效訓練的便利，並大幅減少程式碼與數學推導的負擔。

---

<a id="day-07"></a>

## Day 07｜【Day 7】PyTorch的威力， 20 行程式碼讓神經網路自己跑起來！

- 原文：https://ithelp.ithome.com.tw/articles/10384445
- 發佈時間：2025-09-21 15:49:11

前言
==

在當前人工智慧與深度學習的開發領域中，PyTorch 已逐漸成為最具影響力的主流框架之一。相較於早期由 Google 推出的 TensorFlow，PyTorch 以更貼近 Python 語言習慣的程式風格，讓研究人員與開發者能夠更直觀地進行模型構建與實驗。此外我們後續會介紹的 Hugging Face 也將 PyTorch 作為其主要架構基礎，可見其在業界的重要地位。

因此今天我們就來看看 PyTorch 的安裝方式與基本使用方法，幫助大家快速上手這個強大的深度學習工具。

Pytorch GPU安裝
=============

在安裝 PyTorch 的時候，如果直接輸入以下指令：

```undefined
pip install torch
```

那麼你安裝到的會是 **CPU 版本** 的 PyTorch，雖然CPU版本一樣能跑模型、做推論，但一旦遇到需要大量計算的大型深度學習模型或龐大的資料集，效能就會變得相當有限。為了真正發揮硬體的效能，建議改裝 **支援 GPU 的版本**來大幅加快訓練與推論的速度。

而安裝GPU的版本時，我們往往會因為個人設備不同驅動程式不同，因此我們需要根據以下步驟一步一步來正確的安裝Pytorch GPU版本。

1. 確認你的 CUDA 版本
---------------

首先我們需要知道你的顯示卡支援哪個版本的 CUDA。打開命令提示字元（CMD），輸入：

```undefined
nvidia-smi
```

接著畫面上會出現一張表格，會顯示你的 GPU 狀態與驅動資訊。在右上角就可以看到「CUDA Version」，這就是你目前可以使用的最高 CUDA 版本。

![Image 9: https://ithelp.ithome.com.tw/upload/images/20250921/20152236LqPsLxnTeT.png](images/series-8357/day-07/20152236LqPsLxnTeT-dd4257f497fe3435.png)

2. 前往 PyTorch 官方網站
------------------

打開瀏覽器進入 [PyTorch 官網](https://pytorch.org/)。你會看到一個安裝指令產生器（可以選擇你的作業系統、Package 管理工具、Python 版本，以及 CUDA 版本。

![Image 10: https://ithelp.ithome.com.tw/upload/images/20250921/20152236wAmWu7EMjp.png](images/series-8357/day-07/20152236wAmWu7EMjp-90f1f98cd7ecb71a.png)

記得根據剛剛查到的 CUDA 版本來選擇對應的安裝方式，這樣才能順利使用 GPU。

3. 執行 GPU 版本安裝指令
----------------

假設你的 CUDA 版本是跟我一樣是12.6，則可以使用下列指令來安裝 GPU 版本的 PyTorch：

```perl
pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu126
```

如果你的 CUDA 版本不同，可以在剛剛的官網頁面中下拉選單選擇其他版本，網站會自動幫你產生對應的指令。

4. 確認安裝是否成功
-----------

安裝完成後，建議測試一下是否真的安裝了 GPU 版本的 PyTorch。在命令提示字元輸入：

```undefined
python
```

進入 Python 的互動模式後，再輸入以下兩行：

```python
import torch
torch.cuda.is_available()
```

如果回傳是 `True`，恭喜你，代表 PyTorch 已經可以正確使用你的 GPU 了，這樣子我們就能夠繼續下一階段的動作。

Pytorch建立DNN
============

現在讓我們根據Pytroch的方式修改昨天的模型訓練過程。

1.建立模型資料
--------

在 PyTorch 裡，模型的核心資料結構不是傳統的「矩陣」，而是更靈活的`張量（Tensor）`。這樣的設計並非隨意而為，其實藏著幾個關鍵的考量。簡單來說**矩陣本質上只是一種二維資料表現形式（由行和列構成）**，而**張量則是矩陣的延伸版本**，可以自由擴展到任意維度從一維、二維，到三維，甚至更高維的資料結構都能涵蓋。

這種靈活性讓我們能夠更自然地處理像影像、語音或時間序列這類多維資料，而不必被侷限在傳統線性代數的框架之中，而在程式碼中，其實與我們先前差不多，只不過改成使用了`torch.tensor`。

```ini
# 製作 XOR 資料
x_data = torch.tensor([[0., 0.],
                       [0., 1.],
                       [1., 0.],
                       [1., 1.]], dtype=torch.float32)
y_data = torch.tensor([0., 1., 1., 0.], dtype=torch.float32).view(-1, 1)
```

此外PyTorch 的張量不只是一種靜態資料容器，**它具備「自動微分」（autograd）功能，能即時追蹤和計算梯度**。這種設計大幅簡化了從資料建構、模型運算到訓練優化的整個流程，讓使用者無需在數據結構與運算邏輯之間來回切換。

2.建立模型與前向傳播
-----------

在 PyTorch 中建立模型時，其實我們只需要關注「前向傳播（forward）」的邏輯就好。你可能會注意到，`forward` 這個方法的名稱是**不能亂改的**。為什麼呢？因為當你呼叫模型本身（例如 `model(x)`）時，PyTorch 內部其實會自動呼叫 `forward()` 方法。這就有點像你自訂了 `__call__` 的行為一樣——如果你把 `forward` 改成其他名字，模型就找不到對應的前向傳播邏輯，直接報錯！

```python
class MLP(nn.Module):
    def __init__(self, input_dim=2, hidden_dim=4):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)
        
        # He 初始化權重 + 加上一點正偏置，避免 ReLU 神經元死掉
        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')
        nn.init.constant_(self.fc1.bias, 0.01)
        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')
        nn.init.zeros_(self.fc2.bias)

    def forward(self, x):
        h = F.relu(self.fc1(x))
        z = self.fc2(h)
        y = torch.sigmoid(z)
        return y

model = MLP(input_dim=2, hidden_dim=4)
```

這裡的 `fc1` 和 `fc2` 分別代表兩層線性轉換。`fc1` 是從輸入層到隱藏層，`fc2` 則是從隱藏層到輸出層。簡單來說，`nn.Linear()` 就是在幫你處理 `Wx + b` 這種線性運算，寫起來比手動計算來得簡潔許多，也讓模型架構一目了然。

所以只要搞懂 `forward()` 為什麼不能亂動，再加上幾層 `nn.Linear()` 結合你要的激勵函數，一個簡單的 MLP 模型就完成了！

3. 訓練模型
-------

在過去手動實作神經網路時，我們可能需要自己動手計算梯度，然後再更新權重。但幸好，PyTorch 幫我們省下了這些麻煩事。它有自動微分（autograd）系統，可以自動追蹤張量之間的運算過程，並在需要時自動計算梯度。

也就是說，我們只要從損失函數出發，PyTorch 就能幫我們沿著計算圖自動向後傳播誤差，計算出每個參數的梯度。接著再交給優化器去根據這些梯度來更新模型權重。首先我們得定義兩個關鍵元件：

```ini
criterion = nn.MSELoss()          # 與原版相同：MSE
optimizer = torch.optim.SGD(model.parameters(), lr=0.05)
```

在進入訓練過程前，先快速複習一下完整流程：

*   前向傳播：用模型計算預測結果。
*   計算損失：把預測值和實際值丟進損失函數。
*   反向傳播：從損失出發自動計算每個參數的梯度。
*   更新參數：優化器根據梯度調整模型的權重。

這一點在Pytorch上也相同，因此我們的程式碼可以如此撰寫

```csharp
# 訓練
# 訓練
epochs = 5000
for epoch in range(epochs):
    y_pred = model(x_data)            # Step 1: 前向傳播
    loss = criterion(y_pred, y_data)  # Step 2: 計算損失

    optimizer.zero_grad()             # Step 3a: 清空舊的梯度
    loss.backward()                   # Step 3b: 反向傳播，計算新的梯度
    optimizer.step()                  # Step 4: 更新模型參數
```

這裡有個超級關鍵但容易被忽略的小細節 `optimizer.zero_grad()`，為什麼我們每次都要先清掉梯度呢？

**因為 PyTorch 的預設行為是會累加梯度的！** 這樣的設計其實是有彈性的，方便我們做像是 mini-batch 或累積多次梯度的進階訓練策略。不過對於大多數情況來說，我們希望每次訓練時的梯度都是「全新的」，否則就會把上一次的梯度也加進來，導致權重更新錯誤，訓練結果就不準了。

與手刻神經網路相比PyTorch 真的是一大解脫，不但省去了繁瑣的數學運算與手動梯度推導，還讓整個訓練流程更直觀、易懂。這也正是為什麼我們只需要專注在「前向傳播」的設計上因為 PyTorch 會自動幫你處理後面的梯度計算與參數更新。

這樣的好處是什麼？就是當你開始接觸更複雜的模型時（像是 CNN、RNN、Transformer 等），你不會被一堆數學推導卡住，反而能夠聚焦在每一層的設計意圖與功能上。

下集預告
====

明天我們即將進入全新的模型架構`卷積神經網路（CNN）`。它其實是從我們熟悉的 Wx + b 線性變換邏輯所延伸出來的，只是設計上更適合處理像是影像這類具有空間結構的資料。

在接下來的章節裡，我會繼續透過 PyTorch 的框架，帶你一步步實作整個模型訓練流程。從資料前處理、模型建構，到訓練與預測，每一步我都會細細拆解。特別是在模型結構上，我們會繼續使用 `nn.Linear()` 搭配其他 PyTorch 元件，來打造出屬於你的完整神經網路。

可以說**這將會是我們整個 30 天深度學習系列中最核心的內容之一**。 透過這個單元，你不只會學會怎麼用 CNN 解決實際問題，也會真正理解每一層模型背後的運作邏輯。

準備好了嗎？從今天開始，我們要正式進入深度學習的主戰場。

---

<a id="day-08"></a>

## Day 08｜【Day 8】卷積神經網路不是深度學習的「原創」？從影像處理出發的 AI 革命

- 原文：https://ithelp.ithome.com.tw/articles/10385289
- 發佈時間：2025-09-22 12:28:15

前言
==

當我們談到圖像深度學習第一個跳出來的技術名詞，往往就是 **CNN（Convolutional Neural Network）卷積神經網路**。這個架構幾乎成了影像識別與分類的代名詞，從自動駕駛到醫學影像分析，處處可見它的身影。

但如果我們把鏡頭拉遠一些，不難發現**CNN 的核心概念其實並非誕生於深度學習時代，而是深深植根於過去的影像處理與訊號分析傳統中。**

換句話說，CNN 與其說是 AI 的全新創造，不如說是「舊瓶裝新酒」：將早已有之的數學技術，如影像卷積、局部特徵提取，結合神經網路與反向傳播等深度學習的精華，再一次推向極致。

卷積從哪來？
======

![Image 12: https://ithelp.ithome.com.tw/upload/images/20250922/20152236QFPSrob0yP.png](images/series-8357/day-08/20152236QFPSrob0yP-fa076fee46f73763.png)

早在深度學習尚未崛起的年代`卷積（convolution）`就已經是影像處理界的基本功。不論是 **邊緣偵測、模糊處理、銳化濾鏡** 或是 **紋理強化**，都少不了這個數學工具的身影。

它的核心概念很簡單：使用一個小小的`濾波器（kernel）`，在圖片上滑動，針對周圍像素做加權運算。如下圖所示：

![Image 13: https://ithelp.ithome.com.tw/upload/images/20250922/20152236qQGZ0GZzSB.jpg](images/series-8357/day-08/20152236qQGZ0GZzSB-ef0032f539717f47.jpg)

舉例來說，當我們用 **Sobel** 或 **Laplacian** 這類經典的邊緣偵測濾波器去掃描圖片時，只要碰到像素值變化劇烈的區域（例如物體邊界），便會產生強烈響應，讓邊緣浮現出來。這種技術，即便在沒有深度學習的時代，也早就是圖像分析的核心工具。

CNN 怎麼進行卷積？
===========

在卷積神經網路中，**卷積層（Convolution Layer）** 是整個架構的基礎，與傳統影像處理不同的是，CNN 不再仰賴人為設計的濾波器，而是讓系統「自己學」，學會該使用什麼樣的卷積核來辨認特徵，這些濾波器可以理解為 AI 的視覺「眼鏡」：

*   初階的濾波器可能只會辨識線條或角落；
*   中階的則能看出紋理、形狀；
*   高階的甚至可以抓出「眼睛」、「車輪」、「臉部輪廓」這類複雜圖樣。

而針對CNN的數學公式我們可以先看到以下寫法

![Image 14: https://ithelp.ithome.com.tw/upload/images/20250922/20152236RzuKjqIBJk.png](images/series-8357/day-08/20152236RzuKjqIBJk-e85b012f38c31351.png)

這裡的 `X` 是輸入的`特徵圖（Feature Map）`，`K` 則是卷積核。運算方式就是簡單的`滑動視窗（Sliding Windows）`每次針對小區域的像素進行乘法加總，生成新的影像特徵。

圖解卷積的實際動作
=========

雖然數學公式能精確描述卷積操作，但對多數人來說，圖像化理解往往更直觀。因此我們不談複雜的運算式，而是用一張圖來說明整個卷積的邏輯：

![Image 15: https://ithelp.ithome.com.tw/upload/images/20250922/20152236oOkKSlolyX.png](images/series-8357/day-08/20152236oOkKSlolyX-ddd1277af6cc8adc.png)

### 1. 輸入影像的局部視窗

首先看圖左邊那個大格子。這代表的是原始輸入影像中的一小塊區域。你可以想像它是一張圖裡的某個「局部」，例如某隻貓咪耳朵上的一小塊畫素矩陣。這些數字代表像素的強度（通常是灰階或 RGB 數值），是電腦眼中「看見」影像的方式。這樣的格子稱為特徵圖的一部分。

### 2. 卷積核（濾波器）

接下來中間的小格子就是所謂的卷積核(也常被稱為濾波器)，這個小小的矩陣裡面裝的是一組數字權重，它們的功能就像是一副特定的眼鏡——可能專門用來偵測邊緣、水平線、紋理等等。而這些權重是 CNN 自己學出來的。

### 3. 逐元素相乘與加總

現在這個濾波器會疊在原始影像的某個區塊上。它們之間會一一對應，也就是每個位置的像素值，會和濾波器對應位置的權重值相乘。完成這一輪逐元素相乘後，接下來會把所有的乘積加總起來。這個加總結果，就是這次濾波後的輸出值。

用白話說這個數字反映了原始影像在這個區塊裡，有多符合濾波器關注的特徵，你可以把它想像成一個圖像掃描器或偵測器，會專注在某一種視覺特徵上。

### 4. 像拼圖一樣滑動整張圖

濾波器不會只做一次操作，它通常是往右移一格或往下移一格，然後在每一個新的位置上，重複剛才的乘法與加總步驟，這個「滑動」的動作，稱為捲積運算中的`步長（stride）`。透過這種方式，整張輸入圖就會被掃過一遍。

### 5. 產生特徵圖

每一次滑動產生的輸出值，會被放到一個新的矩陣中。這些數值組成的結果，就是所謂的`特徵圖（Feature Map）`。這張特徵圖是 CNN 對原圖的一次詮釋，會強調出原圖中符合濾波器特徵的區塊。

例如這個濾波器專門抓水平邊緣，那麼在有邊緣的地方，特徵圖上會出現高值；反之則是低值或零。可以說，CNN 就是透過這一層層的特徵圖疊加，逐步抽象出圖像中的重要資訊。

池化層
===

在 CNN 的架構中，除了卷積層之外，另一個不可或缺的角色就是 `池化層（Pooling Layer）`。它的功能可以被視為一種資訊的濃縮器，**用來簡化資料，同時保留最具代表性的特徵**。具體來說池化可以降低特徵圖的尺寸，進而減少模型的運算負擔；它也能過濾掉多餘的細節與雜訊，讓神經網路聚焦在關鍵特徵上。

此外池化還具有強化「平移不變性」的效果，意思是即使影像中的物體稍微位移、旋轉或變形，模型仍能正確辨識出相同的特徵，其中最常見的是`最大池化（Max Pooling）`與`平均池化（Average Pooling）`。

![Image 16: https://ithelp.ithome.com.tw/upload/images/20250922/20152236ZSfsagAzrY.png](images/series-8357/day-08/20152236ZSfsagAzrY-613ee63b117e6564.png)

如圖中顯示**最大池化會在小區塊中取出數值最大的那個，聚焦於最強烈的特徵訊號**；而**平均池化則取該區塊中的平均值，使特徵圖整體趨於平滑**，兩者本質上都是為了讓網路在保留關鍵訊息的同時，降低圖像維度與計算成本。

全連接層
====

經過前面的卷積與池化步驟後CNN已經完成了它的任務，它抽取了像是邊緣、紋理、形狀，甚至更抽象的高階概念，但接下來，一個問題浮現了：**我們該如何根據這些特徵，做出具體的判斷？** 這個答案就是`全連接層（Fully Connected Layer）`。

所謂全連接其實就是我們的DNN，只不過接收的是特徵圖，而該層也是在 CNN 中進行「決策」的地方。例如，當我們訓練一個用來分類狗、貓、人臉的模型時，前面的卷積層與池化層負責抽取特徵，而全連接層則會根據這些特徵值，最終輸出「這張圖是哪一類」的判斷結果。

> 在實作上 **CNN 中的特徵圖會先被攤平成一維向量，再送入一個或多個全連接層進行加權計算**，並搭配**激勵函數（如 ReLU、Sigmoid 或 Softmax）**產出最終輸出。

簡單來說，卷積層與池化層像是圖像的觀察者，而全連接層則是做決定的人。這三者各司其職，構成了一個能夠學習、理解並判斷視覺資訊的完整神經網路架構。

下集預告
====

到這裡，我們已經完整理解了 CNN 的基本架構從卷積層如何提取特徵、池化層如何濃縮資訊、到全連接層如何完成分類判斷。明天我們將透過一個實際的圖像分類任務，一步步帶你用 PyTorch 從零建構 CNN 架構，讓你親手實踐今天所學的內容。

不只是學理更是實戰！但一旦動手實作，就會變得具體而清晰。

---

<a id="day-09"></a>

## Day 09｜【Day 9】60% 準確率只是起點用 CNN 與 CIFAR-10 探索深度學習優化之路
- 原文：https://ithelp.ithome.com.tw/articles/10386111

前言
--

在深度學習的實務應用中，圖像數據的處理與模型的優化是非常重要的環節。今天將帶你了解如何在 Python 中讀取圖像，並且利用 PyTorch 進行模型的訓練、優化與儲存，同時解釋訓練、驗證、測試資料集的差異與使用場警，最終告訴你該如何觀察損失值來評估模型的表現。

CIFAR-10 是電腦視覺領域中非常經典的影像資料集，包含 10 個類別（如飛機、汽車、貓、狗等）。每張圖片的大小為 32×32 的彩色影像。在進行深度學習任務時，輸入資料的理解與處理是關鍵的一環，因此我們首先要學會如何在 PyTorch 中正確準備資料集。

1. 定義資料正規化方法
------------

在深度學習的影像處理任務中，「正規化」是一個關鍵步驟。它的主要目的，是讓輸入數據的數值範圍保持一致。以常見的影像資料來說，像素值原本落在 `[0, 255]` 之間，而透過正規化，我們會將這些值縮放至 `[0, 1]` 或 `[-1, 1]` 的範圍。這麼做不只是為了美觀的數據分佈，更能有效提升模型訓練的穩定性，減少像是梯度爆炸或梯度消失等常見問題，也能加速優化器的收斂速度。

在 PyTorch 框架中，`torchvision` 是一個非常實用的套件，不僅能快速下載如 CIFAR-10 等常用的影像資料集，還內建支援資料增強與標準化的功能。在載入資料之前，我們可以先定義好一組轉換方式，例如以下這段程式碼：

```
import torch
import torchvision
import torchvision.transforms as transforms

# 定義影像轉換流程，包括 Tensor 化與正規化
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 將像素值縮放至 [-1, 1]
])
```

這裡的 `Normalize` 函式會針對每個顏色通道（紅、綠、藍）分別進行正規化。這個處理除了提升數據一致性外，也能減少不同通道之間因尺度差異所導致的學習偏差。換句話說，模型在這樣的條件下，更能「公平」地學習各種影像特徵。整體來看正規化不僅讓訓練過程更穩定高效，也有助於模型在面對不同光照或色彩變化的情境時，展現更好的泛化能力。

2. 載入訓練資料集
----------

**載入與處理資料集**是訓練模型的重要前置步驟。我們使用 `torchvision.datasets.CIFAR10` 套件來下載 CIFAR-10 資料集，並搭配前面設定好的 `transform` 進行資料預處理，確保圖片在進入模型前已具備良好格式與特徵分佈。

```
# 下載完整訓練集（共 50,000 張），再進一步拆分為訓練集與驗證集
full_train = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=transform
)

train_size = int(0.9 * len(full_train))   # 90% 作為訓練資料
val_size   = len(full_train) - train_size # 剩餘 10% 作為驗證資料
trainset, valset = random_split(full_train, [train_size, val_size])

trainloader = DataLoader(trainset, batch_size=64, shuffle=True,  num_workers=0)
valloader   = DataLoader(valset,   batch_size=64, shuffle=False, num_workers=0)
```

在預處理階段，我們將原始的訓練資料拆分為兩個子集：**訓練集（train）**與**驗證集（validation）**，它們在模型訓練過程中扮演著不同角色：

*   **訓練集**：用來調整模型參數，讓模型學習圖片與對應標籤之間的關聯。
*   **驗證集**：雖然來自同一批資料，但在訓練過程中不參與模型學習，主要用來檢查模型是否過擬合，並協助調整超參數。

這樣的劃分方式能幫助我們更有系統地控制訓練流程，進一步提升模型的泛化能力。除了訓練與驗證之外，在更嚴謹的實驗或比賽中，通常還會使用第三組資料：**測試集（test）**。測試集完全不參與模型訓練，只在模型訓練結束後用來評估最終的性能表現，模擬模型在實際應用中的預測效果。

| 資料類型 | 功能 | 是否用於訓練 | 是否打亂順序 |
| --- | --- | --- | --- |
| 訓練集 | 學習模型參數 | ✅ 是 | ✅ 是 |
| 驗證集 | 監控訓練過程、調整模型 | ❌ 否 | ❌ 否 |
| 測試集 | 最終模型評估 | ❌ 否 | ❌ 否 |

我們也同樣下載了測試資料集：

```
# 載入測試資料集（共 10,000 張）
testset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=transform
)
testloader = torch.utils.data.DataLoader(
    testset, batch_size=64, shuffle=False, num_workers=0
)
```

在程式中，資料的讀取與分批由 **`DataLoader`** 負責。由於一次將所有資料送入模型會超出 GPU 記憶體限制，因此透過 mini-batch 的方式，每次僅處理部分資料，不僅有效率，也利於模型收斂。

而`DataLoader` 也支援 **資料隨機化（shuffle）**。在訓練階段開啟 `shuffle=True` 可以避免模型過度記住資料順序，提高泛化能力；而在驗證與測試階段則關閉隨機化，以確保結果的穩定性與可重現性。

3. 建立 CNN 模型
------------

對於初學者來說，在設計卷積神經網路時，最常見的疑問之一莫過於：**每一層的輸入與輸出尺寸到底是怎麼算出來的？那全連接層的參數數量又該如何推導？** 要釐清這些問題，我們得從最基本的輸入特徵結構與各層之間的轉換邏輯談起。

就拿本次資料集來說，每張圖像的尺寸都是固定的：

```
(Height=32, Width=32, Channels=3)
```

也就是說，原始影像的格式為 `(H, W, C)`。不過在 PyTorch 的卷積層中，模型預期的張量格式則是 `(C, H, W)`。幸好這樣的轉換在 `torchvision.datasets.CIFAR10` 裡已經幫我們處理好了，所以不必手動調整。而實際訓練時，資料通常會以「批次」的形式餵給模型，因此整體的輸入格式會變成：

```
(Batch_size, Channels, Height, Width) → (N, C, H, W)
```

這種設計使得模型能夠分別處理 R、G、B 三個顏色通道，也就是說進而在不同的色彩維度中萃取出關鍵特徵，因此我們可以這樣設計模型架構：

```
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(3, 6, 5)     # 第一層卷積：從 RGB 三通道轉成 6 個特徵圖
        self.conv2 = nn.Conv2d(6, 16, 5)    # 第二層卷積：再從 6 個特徵圖提取出 16 個
        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 第一層全連接，輸入來自 flatten 後的特徵圖
        self.fc2 = nn.Linear(120, 84)
        self.fc3 = nn.Linear(84, 10)        # 最終輸出對應 10 個分類

    def forward(self, x):
        x = F.max_pool2d(F.relu(self.conv1(x)), 2)
        x = F.max_pool2d(F.relu(self.conv2(x)), 2)
        x = x.view(-1, 16 * 5 * 5)          # 將三維特徵圖展平
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)                     # 最後一層直接輸出 logits
        return x
```

在進入全連接層前，有一個重要的細節需要注意：**`16 × 5 × 5`** 這個數字到底怎麼來的？它其實是前面經過兩次卷積與池化操作後，最後一層輸出特徵圖的維度計算結果。我們可以透過以下的數學公式來推導每一層輸出大小

![Image 1: 輸出計算公式](images/series-8357/day-09/20152236YrDqZzTAU1-b04b01122cbc629a.png)

具體的過程可以整理成一張表：

| 層級 | 輸入尺寸 | 輸出尺寸 |
| --- | --- | --- |
| conv1 | (3, 32, 32) | (6, 28, 28) |
| maxpool1 | (6, 28, 28) | (6, 14, 14) |
| conv2 | (6, 14, 14) | (16, 10, 10) |
| maxpool2 | (16, 10, 10) | (16, 5, 5) |
| flatten | - | 400 |
| fc1 | 400 | 120 |
| fc2 | 120 | 84 |
| fc3 | 84 | 10 |

而經過這樣的轉換後，我們會得到 16 張大小為 5×5 的特徵圖。這就是為什麼在進入第一個全連接層時，我們需要先把三維的 `(16, 5, 5)` 展平成一個長度為 400 的向量，作為後續神經網路層的輸入來源。

4. 模型訓練流程與最佳模型保存
----------------

而這次訓練的核心目標，是根據驗證集的表現來監控模型學習進度，並在表現最優時儲存對應的參數設定。由於本次任務屬於分類問題，因此我們選擇了常見的損失函數 `交叉熵損失（CrossEntropyLoss）`，而優化器則採用 `隨機梯度下降法（SGD）`，並加入`動量（momentum）`來加速收斂。

```
import torch.optim as optim

criterion = nn.CrossEntropyLoss()  # 分類任務標配的損失函數
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)  # 可調整學習率與動量
```

而正常的訓練中通常會在每個訓練週期使用訓練集資料更新模型權重，再透過驗證集來觀察泛化能力，而這樣的設計是為了早偵測是否發生過擬合，並在訓練過程中或是後續的驗證查看模型訓練是否產生Overfitting的問題。

```
import torch
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
net.to(device)

num_epochs = 20
best_val_loss = float('inf')

train_losses, val_losses = [], []

for epoch in range(num_epochs):
    # -------- Training --------
    net.train()
    running_loss = 0.0
    for inputs, labels in trainloader:
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

    # -------- Validation --------
    net.eval()
    val_loss = 0.0
    correct, total = 0, 0
    with torch.no_grad():
        for inputs, labels in valloader:
            inputs, labels = inputs.to(device), labels.to(device)
            outputs = net(inputs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()

            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    avg_train_loss = running_loss / len(trainloader)
    avg_val_loss = val_loss / len(valloader)
    val_acc = 100 * correct / total

    train_losses.append(avg_train_loss)
    val_losses.append(avg_val_loss)

    print(f"Epoch [{epoch+1}/{num_epochs}] "
          f"Train Loss: {avg_train_loss:.4f} "
          f"Val Loss: {avg_val_loss:.4f} "
          f"Val Acc: {val_acc:.2f}%")

    # -------- Save Best Model --------
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        torch.save(net.state_dict(), "best_cifar10_model.pth")
        print(">> Model saved with Val Loss:", best_val_loss)

# -------- Plotting Training Curve --------
plt.figure(figsize=(10, 5))
plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')
plt.plot(range(1, num_epochs+1), val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Loss Curve')
plt.legend()
plt.grid(True)
plt.show()
```

輸出結果：

```
...
Epoch [15/20] Train Loss: 1.1974 Val Loss: 1.2446 Val Acc: 55.56%
>> Model saved with Val Loss: 1.2446476824675934
Epoch [16/20] Train Loss: 1.1670 Val Loss: 1.2554 Val Acc: 55.52%
Epoch [17/20] Train Loss: 1.1447 Val Loss: 1.2216 Val Acc: 56.44%
>> Model saved with Val Loss: 1.2215879235086562
Epoch [18/20] Train Loss: 1.1205 Val Loss: 1.1978 Val Acc: 58.04%
>> Model saved with Val Loss: 1.1978471565850173
Epoch [19/20] Train Loss: 1.1039 Val Loss: 1.1918 Val Acc: 57.80%
>> Model saved with Val Loss: 1.1918227981917466
Epoch [20/20] Train Loss: 1.0851 Val Loss: 1.1921 Val Acc: 57.70%
```

![Image 2: https://ithelp.ithome.com.tw/upload/images/20250923/20152236kdWmNz1b7P.png](images/series-8357/day-09/20152236kdWmNz1b7P-800e685f1dc34272.png)

在訓練過程中，我們可以透過程式碼自動在驗證損失下降時保存模型，並繪製損失曲線圖來觀察模型的學習狀況。當我們看到**訓練損失持續下降**，就表示模型正在一步步學會資料中的特徵；如果**驗證損失也跟著下降**，那代表模型不只記住訓練資料，還能在新資料上有不錯的表現，顯示出良好的泛化能力。不過在訓練後期，若出現**訓練損失持續下降，但驗證損失開始上升**的情況，就要小心這可能是過擬合。遇到這種狀況時，可以考慮提早停止訓練，或是透過更換優化器、修改模型架構、加入正則化或資料增強等方法來改善。

5. 測試與使用模型
----------

當模型訓練告一段落，接下來最關鍵的，就是進行最終測試。我們只需要載入訓練期間表現最好的模型權重，接著透過測試資料看看它在真實環境下的表現如何。

```
# 載入訓練期間表現最佳的模型
best_model = Net().to(device)
best_model.load_state_dict(torch.load("best_cifar10_model.pth", map_location=device))
best_model.eval()
```

這邊我們會用先前準備好的 `testloader` 來進行測試。理論上如果模型沒有發生 overfitting，那它在測試集上的準確率應該會和驗證集差不多。反之，如果測試結果落差很大，那就有可能是訓練過程中出了些狀況——可能是資料分布不平均，也有可能是模型架構或訓練方式需要重新檢視。現在就讓我們實際測試看看成果如何吧：

```
import torch
import torch.nn as nn

criterion = nn.CrossEntropyLoss()

test_loss = 0.0
correct, total = 0, 0

best_model.eval()
with torch.no_grad():
    for inputs, labels in testloader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = best_model(inputs)
        loss = criterion(outputs, labels)
        test_loss += loss.item()

        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

avg_test_loss = test_loss / len(testloader)
test_acc = 100.0 * correct / total
print(f"[Test] Loss: {avg_test_loss:.4f}  |  Acc: {test_acc:.2f}%")
```

輸出結果如下：

```
[Test] Loss: 1.1549  |  Acc: 59.14%
```

從結果可以看出，我們的模型最終達到了大約 60% 的準確率。雖然不是特別高，但這其實也代表模型已經進入了收斂狀態換句話說，它已經學到了它目前能學到的最佳表現。那接下來呢？這就是深度學習中最有挑戰性的部分了：如何更進一步優化模型。

這時候我們可以開始探索像是：

*   改良模型架構（例如引入更深的卷積層、使用正規化技巧）
*   使用資料增強（Data Augmentation）來擴充訓練資料的多樣性
*   嘗試不同的學習率策略或優化器

每一個微小的改動，都有可能讓模型再往上突破一點點。雖然這條路沒有捷徑，但每一次的測試與調整，都是朝著更成熟的人工智慧邁進的重要一步。

下集預告
----

訓練的流程基本上大同小異，目前我們都是透過手動的 for 迴圈來跑完整個訓練過程，包括訓練、驗證、測試，還有手動儲存表現最好的模型。這種做法雖然能讓我們一目了然每個細節，但隨著模型變得越來越複雜、需要測試的超參數組合越來越多，這樣的方式就顯得冗長且不易維護。

因此從明天開始，我們會動手打造一個通用的 Trainer 類別，讓整個訓練流程變得更有系統。我們希望能把訓練邏輯包裝成一個模組化、可重複使用的工具，讓後續不論是改模型、換資料集，甚至更新章節內容，都可以沿用同一套訓練器。這樣一來，我們可以把重心放回在「模型的設計與優化」上，而不是被大段樣板程式碼拖住進度。

---

<a id="day-10"></a>

## Day 10｜【Day 10】用一支「通用訓練器」打天下逐行理解開源Trainer的內容
- 原文：https://ithelp.ithome.com.tw/articles/10386818

前言
--

今天終於進入第 10 天的學習了！雖然我們已經準備好了資料與模型，但卻還不太清楚如何將整個訓練流程封裝成一個乾淨、可重複使用的訓練器。一般來說一個基礎的 `Trainer` 類別至少需要具備以下功能：完整的訓練/驗證迴圈、最佳模型的保存機制、`Early Stopping（提前停止）`、學習率排程器，甚至還要能處理 LoRA 的載入與保存。接下來我會分段說明這些設計的考量，以及為什麼必須這樣做。

這支訓練器在解決什麼問題？
-------------

簡單來說我們今天的內容就是要**把「單個 Epoch 內要完成的工作」與「跨 Epoch 之間需要比較或保存的工作」清楚拆開**。在這樣的設計下，訓練器應該提供：

*   可插拔的資料載入器（`train_loader`、`valid_loader`）
*   任意的模型與最佳化器（`model`、`optimizer`）
*   可選擇性啟用的學習率排程器（`scheduler`）
*   Early Stopping 與最佳權重保存（支援 general / LoRA 兩種模式）
*   訓練與驗證損失的可視化

也就是說這樣的訓練器就是一個樣板化事務的自動化框架，你只需要準備好資料並丟進來，其餘的重複性流程都能交給它處理，大幅減少額外的程式碼負擔。

在設計訓練器的時候，第一步就是把所有「可能會變動的東西」都丟進 `__init__`，這樣之後要改參數或實驗就不用大改程式。

```
class Trainer:
    def __init__(self, epochs, train_loader, valid_loader, model, optimizer,
                 device=None, scheduler=None, early_stopping=10, save_dir='./checkpoints',
                 load_best_model=False, grad_clip=None, is_lora=False):
        self.epochs = epochs
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        self.optimizer = optimizer
        self.scheduler = scheduler
        self.early_stopping = early_stopping
        self.load_best_model = load_best_model
        self.grad_clip = grad_clip
        self.is_lora = is_lora

        if device is None:
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            print('Using device:', self.device)
        else:
            self.device = device

        self.model = model.to(self.device)

        self.save_dir = save_dir
        self.save_name = 'best_model.ckpt'
        if not os.path.exists(self.save_dir):
            os.makedirs(self.save_dir)
```

這裡我特別考慮了三個問題：

*   裝置選擇：GPU 還是 CPU？

在深度學習裡，GPU 幾乎是標配，因為它能大幅加速矩陣運算。不過這裡還有一個容易忽略的細節，當 PyTorch 在 GPU 上報錯時訊息往往很模糊，會讓人不知道是哪個層或張量出了問題。

相較之下放在 CPU 上執行，雖然速度慢但錯誤訊息更清晰。因此我在設計 `Trainer` 的時候，把 `device` 留作可選參數，如果使用者沒有指定就自動檢查 CUDA 是否可用，能用就跑 GPU，否則退回 CPU。這樣的設計兼顧效能與除錯便利。

### **LoRA 模型保存與載入的分流**

`LoRA（Low-Rank Adaptation）`是一種高效的微調方法，**它的特點是只訓練小部分權重，而不是整個大模型**，這也導致它的保存與載入方式和一般模型不太一樣一般模型可以直接 `torch.save(model.state_dict())`，但 LoRA 模型則需要透過 Hugging Face 的 `peft` 套件，僅保存 adapter 權重，並在載入時附加到基礎模型上。

為了兼容這兩種情境，我在 `Trainer` 的初始化裡增加了一個布林參數 `is_lora`，讓保存與載入流程能夠自動分流，不需要使用者額外操心。

### **學習率排程器**

固定學習率往往不是最佳選擇，若設得過高模型容易震盪甚至無法收斂；設得過低則可能導致訓練進展緩慢，甚至停留在某個`次佳解（local minimum）`。這正是 **學習率排程器（Scheduler）** 。

它存在的理由就是能根據訓練進度動態調整學習率。常見做法包括：`StepLR` 每隔幾個 epoch 將學習率乘上一個係數、`CosineAnnealingLR` 讓學習率以餘弦函數方式週期性下降、`ReduceLROnPlateau` 則在驗證表現停滯時才降低學習率。

這樣的設計能幫助模型在陷入次佳解時，透過調整學習率跳脫卡住的區域，從而提升最終的收斂效果。此外Scheduler 也分為兩種更新頻率：有些在 **每個 step** 更新，有些則在 **每個 epoch** 更新。因此在 `Trainer` 中，我只保留一個 `scheduler` 參數，讓使用者自由決定要採用哪一種策略。

> 在訓練器的設計中，我也加入了一些輔助功能來提升實用性。像是 **`early_stopping`**，可以設定當模型在驗證集上連續若干個 epoch 沒有進步時就自動停止訓練。接著是 **`save_dir` / `save_name`**，用來統一管理模型檔案的保存位置，避免不同實驗互相覆蓋。**`load_best_model`** 則決定在訓練結束後，是否要自動載入表現最佳的權重，省去手動切換的麻煩。最後還有 **`grad_clip`**，這是梯度裁剪的設定，用來防止梯度爆炸，特別是在訓練深層模型時非常實用。

單一訓練迴圈的方式
---------

在訓練模型時，使用 `self.model.train()` 是為了啟用訓練模式，確保像 `BatchNorm` 和 `Dropout` 這些特定模組能正確運作，這兩個模組在訓練和推論時的行為不同：

*   **BatchNorm**：訓練時根據每個 mini-batch 的數據進行標準化，幫助模型更快收斂；推論時則使用訓練期間累積的統計值，以避免結果不穩定。
*   **Dropout**：訓練時會隨機關閉部分神經元，降低過擬合風險；推論時則保留所有神經元提供穩定輸出。

所以我們在訓練與驗證時需要透過 `model.train()` 和 `model.eval()` 的切換，讓模型在不同階段用對的方式運作，確保訓練有效、推論穩定。

```
def train_epoch(self, epoch):
    train_loss = 0
    train_pbar = tqdm(self.train_loader, position=0, leave=True)
    self.model.train()

    for input_datas in train_pbar:
        self.optimizer.zero_grad()
        input_datas = {k: v.to(self.device) for k, v in input_datas.items()}
        outputs = self.model(**input_datas)
        loss = outputs[0]
        loss.backward()

        if self.grad_clip is not None:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)

        self.optimizer.step()

        if self.scheduler is not None:
            self.scheduler.step()

        train_pbar.set_description(f'Train Epoch {epoch}')
        train_pbar.set_postfix({'loss': f'{loss.item():.3f}'})

        train_loss += loss.item()

    return train_loss / len(self.train_loader)
```

由於我們無法確定模型使用的損失函數或輸入資料格式，因此採用兩個做法來增加彈性：

1.   **模型回傳損失值**，方便後續處理；
2.   **用 `dict` 傳入輸入參數**，例如 `{k: v.to(self.device) for ...}`，這樣寫法簡潔，能確保每個張量都正確移到指定設備上，避免遺漏。

此外為了防止梯度爆炸，我們用 `clip_grad_norm_` 來限制梯度大小，而學習率則透過排程器動態調整，每個 batch 更新一次，根據設定來決定更新時機（step 或 epoch），讓訓練過程更穩定靈活。

乾淨的驗證流程
-------

在驗證階段，許多訓練時需要的設定其實都不再需要。此時我們只需要模型具備基本的前向傳播功能，因此會透過 `self.model.eval()` 將模型切換到驗證模式，確保像 Dropout、BatchNorm 這類模組能以推論時的行為運作。

```
def validate_epoch(self, epoch):
    valid_loss = 0
    valid_pbar = tqdm(self.valid_loader, position=0, leave=True)
    self.model.eval()

    with torch.no_grad():
        for input_datas in valid_pbar:
            input_datas = {k: v.to(self.device) for k, v in input_datas.items()}
            outputs = self.model(**input_datas)
            loss = outputs[0]
            valid_pbar.set_description(f'Valid Epoch {epoch}')
            valid_pbar.set_postfix({'loss': f'{loss.item():.3f}'})
            valid_loss += loss.item()

    return valid_loss / len(self.valid_loader)
```

而通常為了提升推理效率，還會搭配 `torch.no_grad()` 停用梯度運算，這麼做可以節省記憶體並加快運算速度，因為驗證過程中並不需要進行反向傳播或更新權重。

主回圈進行Early Stopping 與最佳權重保存
---------------------------

在模型訓練中有幾個實用的技巧能提升效果與效率。首先是 `Early Stopping`，透過計數器 `stop_cnt` 追蹤驗證表現是否持續進步。**只要連續幾個 epoch 沒有改善，就會提前停止訓練**，這不僅能有效避免過擬合，還能節省時間與資源。

接著是昨日提到的最佳權重保存，每當 `valid_loss` 出現新低時就會立即覆蓋並儲存目前的模型狀態，但要注意在使用 LoRA 時會是要透過 `save_pretrained()` 儲存，若是一般模型則採用 `state_dict()` 方式。

```
def train(self, show_loss=True):
    best_loss = float('inf')
    loss_record = {'train': [], 'valid': []}
    stop_cnt = 0

    for epoch in range(self.epochs):
        train_loss = self.train_epoch(epoch)
        valid_loss = self.validate_epoch(epoch)

        loss_record['train'].append(train_loss)
        loss_record['valid'].append(valid_loss)

        # Save best model
        if valid_loss < best_loss:
            best_loss = valid_loss
            if self.is_lora:
                self.model.save_pretrained(self.save_dir)
            else:
                save_path = os.path.join(self.save_dir, self.save_name)
                torch.save(self.model.state_dict(), save_path)
            print(f'Saving Model With Loss {best_loss:.5f}')
            stop_cnt = 0
        else:
            stop_cnt += 1

        print(f'Train Loss: {train_loss:.5f} | Valid Loss: {valid_loss:.5f} | Best Loss: {best_loss:.5f}\n')

        if stop_cnt == self.early_stopping:
            msg = "Model can't improve, stop training"
            print('-' * (len(msg) + 4))
            print(f'| {msg} |')
            print('-' * (len(msg) + 4))
            break

    if show_loss:
        self.show_training_loss(loss_record)

    if self.load_best_model:
        if self.is_lora:
            from peft import PeftModel
            self.model = PeftModel.from_pretrained(self.model, self.save_dir)
            print(f'Best LoRA model loaded from {self.save_dir}')
        else:
            best_model_path = os.path.join(self.save_dir, self.save_name)
            self.model.load_state_dict(torch.load(best_model_path))
            print(f'Best model loaded from {best_model_path}')
```

而我們若設定 `load_best_model=True`，訓練結束後會自動載入表現最好的模型，方便後續的測試操作。

繪製損失曲線看趨勢
---------

這部分和昨天的做法一樣，我們可以透過繪製曲線圖來觀察訓練與驗證的損失變化。如果你是在 Colab 或 Jupyter Notebook 上操作，這樣的圖表呈現已經非常直觀且實用。

```
def show_training_loss(self, loss_record):
    train_loss, valid_loss = [i for i in loss_record.values()]
    plt.plot(train_loss)
    plt.plot(valid_loss)
    plt.title('Training Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['train', 'valid'], loc='upper left')
    plt.show()
```

不過注意一點若是在命令列介面（CLI）或遠端伺服器上訓練就建議將圖表儲存成檔案（使用 `plt.savefig()`），或者輸出到像 TensorBoard 或 Weights & Biases 這類工具中做更進一步的視覺化與紀錄，這樣會較為方便查看時實的動態。

以下是將原本的教學以更完整、條理清晰的文章形式呈現，適合用作部落格、筆記或專案說明文件：

* * *

AMP、自動混合精度、梯度累積與多指標支援
---------------------

在深度學習的訓練過程中，**隨著模型規模與資料量的提升，計算效能與記憶體使用變得越來越關鍵**。因此我們還可以這麼的優化現有的訓練器。

1. 自動混合精度（AMP: Automatic Mixed Precision）
-----------------------------------------

AMP 是 PyTorch 提供的功能，讓你能在不犧牲模型精度的情況下，自動在 float16 與 float32 之間切換，顯著加速訓練，並節省 GPU 記憶體。我們可以在在 `train_epoch` 函式中，修改訓練 loop。

```
scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast():
    outputs = self.model(**input_datas)
    loss = outputs[0]

scaler.scale(loss).backward()
if self.grad_clip is not None:
    scaler.unscale_(self.optimizer)
    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)
scaler.step(self.optimizer)
scaler.update()
```

2. 梯度累積（Gradient Accumulation）
------------------------------

在實際訓練中，如果因 GPU 記憶體限制無法使用較大的 batch size，那麼使用 `optimizer.zero_grad()` 搭配梯度更新的傳統方式可能會導致訓練不穩定。這時，`梯度累積（gradient accumulation）`是一種非常實用的技巧。

其核心概念是將多個小 batch 的梯度累加起來，等累積一定步數後再進行一次參數更新。這樣就能模擬大 batch 的效果，同時避免顯存爆炸。

```
accumulate_steps = 4
for step, input_datas in enumerate(train_pbar, start=1):
    ...
    (loss / accumulate_steps).backward()
    if step % accumulate_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

在這裡，`accumulate_steps` 設為 4，代表每 4 個 mini-batch 才更新一次權重，而每次的 loss 都會除以 4 以保持梯度的 scale 一致。

3. 多指標回傳與模型保存策略
---------------

在驗證階段，我們當然可以根據任務需求靈活地更換評估指標。實際上你可能希望同時計算多個指標（如 Accuracy、F1-score、BLEU 等），以更全面地觀察模型表現。而一個實用的做法是在 `validate_epoch()` 中建立一個包含多項指標的 `metrics` 字典。

```
def validate_epoch(self, ...):
    metric_dict = {
        "accuracy": ...,
        "f1": ...,
        "loss": ...
    }
    return metric_dict
```

接著在訓練主流程中，你可以指定其中某個主指標作為儲存模型的依據

```
val_metrics = self.validate_epoch(...)
main_metric = val_metrics["f1"]
if main_metric > self.best_score:
    self.best_score = main_metric
    self.save_model()
```

雖然 AMP、梯度累積與多指標支援等技巧能有效提升訓練效率，也確實讓模型開發流程更加穩定且具可擴展性，但對我而言，這類設計其實也帶來了更多超參數的調整成本，進一步增加了實作的複雜度。

因此在實務上，我並不常主動採用這些擴充技巧，除非有明確的需求，例如希望縮短訓練時間，或在資源受限下提升表現。這類情境下，再引入這些優化手段，通常能顯著改善整體專案的品質與效率。

下集預告
----

明天我會帶你實作如何使用 `nn.Linear()` 建立像 RNN 這樣的神經網路，並在後續的所有章節都會透過我們自定義的 `Trainer` 啟動訓練流程。這個 `Trainer` 的設計其實與後續會使用的 Hugging Face 訓練架構是相容的，因此非常值得理解怎麼撰寫。未來你也可能會擴充資料增強、分布式訓練、AMP/Apex、自定義評估指標等等。但只要有了這個 `Trainer`路就算鋪好了剩下的，就是你在這基礎上，慢慢搭建屬於自己的訓練方式。

---

<a id="day-11"></a>

## Day 11｜【Day 11】賦予 WX+b 時序感知力神經網路如何理解過去與未來

- 原文：https://ithelp.ithome.com.tw/articles/10387444
- 發佈時間：2025-09-25 09:11:03

前言
==

現在做資料分析或機器學習，選模型這件事真的很重要。除了那些大家常聽到的分類、回歸這類基本模型，其實還有一種比較特別的模型，它專門拿來處理時間序列資料。

這類資料的特性在於，**數據之間是有時間順序的**，前後資料會互相影響不是單純的靜態資訊。像是股票走勢、氣象預報，甚至是病患的心跳紀錄甚至是文字，都是典型的時間序列。如果用傳統模型來處理這些資料，往往會忽略時間的關聯性，導致效果不佳。所以這些專門為時間序列設計的模型就變得越來越重要，也越來越常被拿來解決這類問題。而今天我就會來告訴你該怎麼從DNN延伸到時間序列模型

RNN
===

在我們日常生活中最常見的時間資料就是文字，因為文字是有順序、有上下文的，簡單來說，**前面出現的詞會影響後面詞的理解**。想要讓模型搞懂這種時間上的連續性，我們得用一種會記憶的網路結構，而 `RNN（Recurrent Neural Network）`就是基於這一點而成的。

![Image 10: RNN 結構圖](images/series-7467/day-13/2015223645cw9vkbj7-c95d699fb2d40bbd.png)

RNN 的核心想法其實不難，它用一個叫 `隱藏層狀態(hidden state)` 的東西，把前面時間步的資訊傳到下一步。每看到一個詞，就把它轉成向量，再結合上一個隱藏層狀態做些運算，更新出新的隱藏層狀態，你可以把它想成是一張小小的便條紙，從句子開頭一路寫到結尾，記錄下語意的脈絡。

![Image 11: RNN 數學圖](images/series-7467/day-13/20152236jYlTBPFrvP-d25345514b5aff17.png)

當我們從數學的角度來看這個過程，可以把它拆成兩個部分第一部分是計算當前時間點的輸入 `x(t)`，而第二部分則是處理之前幾個時間點所累積的隱藏層狀態。這兩個結果會被結合起來，然後透過 tanh 這個激勵函數把值壓縮在 -1 到 1 之間。簡單來說就是把輸入和保留下來的記憶狀態拿來做一個 Wx 加上 b 的運算。

在這裡我們使用 `nn.Linear` 來自行建造一個 RNN 模型，讓你更直觀地理解數學概念：

```python
class LinearTanhRNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        # 輸入 -> 隱層
        self.i2h = nn.Linear(input_size, hidden_size, bias=True)
        # 前一隱狀態 -> 隱層
        self.h2h = nn.Linear(hidden_size, hidden_size, bias=True)
        # 隱層 -> 輸出
        self.h2o = nn.Linear(hidden_size, output_size, bias=True)

        self.hidden_size = hidden_size
```

首先我們理解一下數學公式，在每一層輸入的 `o(t)` 都是由 `i2h` 與 `h2h` 這兩個 Wx+b 運算構成的，因此我們首先要定義這兩個 `nn.Linear`。而最終的輸出通常會在最後加入一個 `h2o`，這個作法是為了將整個複雜的網路做線性運算，這和我們在 CNN 結尾接上全連接層的概念是一樣的。

```python
def forward(self, x, h0=None):
        B, T, _ = x.shape
        # 初始化隱狀態
        h = x.new_zeros(B, self.hidden_size) if h0 is None else h0
        for t in range(T):    # 逐時間步展開
            # 線性累加後做 tanh 非線性
            h = torch.tanh(self.i2h(x[:, t, :]) + self.h2h(h))
        # 輸出層：將最後隱狀態投影到目標維度
        y_last = self.h2o(h)
        return y_last, h
```

在前向傳播方面，我們需要初始化一個隱狀態的單元，這個單元會提供模型初始的隱藏資訊（畢竟在 x(0) 的時候還沒有任何記憶）。接著我們用一個 for 迴圈，逐步取出時間序列的每個時間步進行運算。由於我們的輸入是三維的`（batch_size, seq_len, feature）`，所以使用 `seq_len` 作為時間長度進行迴圈運算。這個流程就是最簡單的時序模型原型。

LSTM
====

LSTM 可以把它想成替 RNN 裝上一條能長距離搬運訊息的「傳送帶」，名字叫 cell state。每一步模型先決定要把舊資料擦掉多少，再決定新東西要不要寫進傳送帶，最後才決定當下要露出哪一部分當成輸出。因為 cell state 的更新以加法為主，不是層層相乘，所以重要訊號不會在長序列裡被稀釋到幾乎看不見，梯度也比較能往回傳。

![Image 12: 圖](images/series-7467/day-13/20152236yb1ncszjoy-8d4551f14e126603.png)

直觀地說，追劇追到第十季時，你不會把第一季的所有細節硬背在腦中而是留下一本長期筆記。每一集先把過時的備註劃掉再把新的劇情補進去，輪到要回答朋友問題時才翻出相關段落。LSTM 就是把這三步學起來忘多少、寫多少、秀多少。對應到數學式就是先算出三個 0 到 1 的比例，再用一個候選內容去更新筆記，最後把更新後的筆記過一層非線性變成當前的隱狀態，他基本上可以歸類以下幾個元件。

*   遺忘門：負責刪掉沒用的舊資訊

![Image 13: 圖](images/series-7467/day-13/201522368QYwcTFQO1-7e13f1ee5893486e.png)
*   輸入門：決定要不要寫入新資訊

![Image 14: 圖](images/series-7467/day-13/20152236WVKy996hRE-f3491436f0183b08.png)
*   輸出門：控制要輸出哪些東西

![Image 15: 圖](images/series-7467/day-13/20152236dZAKUWzMPM-c253dfdd0363ad74.png)
*   Cell State: 記憶管理

![Image 16: 圖](images/series-7467/day-13/20152236ZXcp0bICxA-08d34e940dbead8c.png)

LSTM 裡面數學式很多，邏輯有點像你在過濾郵件該刪的就丟掉，重要的就留下來，最後再決定要不要回覆，而同樣的我們一樣用`nn.Linear()`展開，讓你更直觀的理解LSTM在做些什麼。

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class LinearTanhLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super().__init__()
        H = hidden_size

        # x 路徑（含 bias_ih）
        self.x_i = nn.Linear(input_size, H, bias=True)
        self.x_f = nn.Linear(input_size, H, bias=True)
        self.x_g = nn.Linear(input_size, H, bias=True)
        self.x_o = nn.Linear(input_size, H, bias=True)

        # h 路徑（含 bias_hh）
        self.h_i = nn.Linear(H, H, bias=True)
        self.h_f = nn.Linear(H, H, bias=True)
        self.h_g = nn.Linear(H, H, bias=True)
        self.h_o = nn.Linear(H, H, bias=True)

        # 輸出層
        self.h2o = nn.Linear(H, output_size, bias=True)
        self.hidden_size = H
```

用程式碼讀起來也很直白，每個時間步把輸入 `x` 和前一刻的隱狀態 `h` 各丟進四個線性層，得到四組向量，再套上 `sigmoid` 或 `tanh`。`i` 表示寫入比例，`f` 表示遺忘比例，`g` 是候選內容，`o` 決定要輸出多少。

```python
def forward(self, x, h0=None, c0=None):

        B, T, _ = x.shape
        H = self.hidden_size
        device = x.device

        h = torch.zeros(B, H, device=device) if h0 is None else h0
        c = torch.zeros(B, H, device=device) if c0 is None else c0

        for t in range(T):
            xt = x[:, t, :]

            i = torch.sigmoid(self.x_i(xt) + self.h_i(h))
            f = torch.sigmoid(self.x_f(xt) + self.h_f(h))
            g = torch.tanh(   self.x_g(xt) + self.h_g(h))
            o = torch.sigmoid(self.x_o(xt) + self.h_o(h))

            c = f * c + i * g
            h = o * torch.tanh(c)

        y_last = self.h2o(h)
        return y_last, (h, c)
```

而其餘計算也與RNN相似，差別在於要將cell state與隱狀態更新也就是`c = f*c + i*g`，`h = o*tanh(c)`。這樣一來關鍵資訊可以沿著 `c` 這條通道跨很多步而不崩壞，`h` 則負責提供當下的可見表徵，同樣地最後丟進 `h2o` 產生你要的輸出大小即可。

而LSTM最大的問題其實就是運算速度過慢，每一個 `cell` 都要等上一個 `h_t, c_t` 算完才能動，等於把整條序列綁在一個長 for 迴圈裡，而GPU 最怕這種細碎依賴鏈很長的工作，每步只做幾個中小型矩陣乘法，且核心限制仍在「下一步必須等上一步」，因此後續雖然有著結構相似用於改善速度的GRU，但種結構性問題是沒辦法解決此類問題的。

下集預告
====

明天我們將運用今天介紹的 LSTM 模型，來捕捉句子中那些細微卻關鍵的語意轉折。你將看到一段文字如何被模型逐層拆解、理解，再被重組成一種「機器的詮釋」。**這將引領你正式踏入自然語言處理（NLP）的領域**，並親手構建一個情緒分析器，體驗從數據到洞察的完整流程。

---

<a id="day-12"></a>

## Day 12｜【Day 12】「你真的懂LSTM嗎？」手刻雙向LSTM讓你從不會到秒懂！
- 原文：https://ithelp.ithome.com.tw/articles/10388170

前言
--

昨天你學過 LSTM但你肯定還搞不清楚它到底在做什麼，而今天我會帶你從零手刻一個雙層 LSTM，並套用在經典的 IMDB 影評情緒分類任務中。這篇重點不在於訓練出一個超強、超快的模型，而是幫你搞懂 LSTM 的真正運作邏輯與自然語言處理的最基礎，從Embedding、到手動實作 forward 傳播流程，通通自己動手來一遍。

而今天重點會在 Padding 怎麼處理才不會影響模型學習？為何要自己 Embedding 是在幹嘛用的？BiLSTM 又是怎麼同時捕捉前後文語意的？透過完整拆解與 PyTorch 實作，我們要的不只是會用，而是真正理解每一層背後的運算邏輯與設計。

IMDB 資料集是情感分析任務中最經典的語料之一，通常包含兩欄：一欄是使用者撰寫的電影評論（`review`），另一欄是該評論的情感標籤（`sentiment`），標示為 `positive` 或 `negative`。

我們之所以能從中分析情緒，是因為使用者在撰寫評論時往往會自然表達感受，例如「好看」、「無聊」等詞語隱含了正面或負面的情緒傾向。這些語言特徵能被電腦模型學習，透過大量標註資料建立語意與情感之間的對應關係，進而判斷新評論的情緒屬性。

檔案連結：[點我](https://github.com/AUSTIN2526/learning-wx-b-in-30-days)

1. 準備資料集
--------

因此第一步就是對資料進行前處理，其中我們需要將這些文字標籤轉換成數值型態，例如把 `positive` 映射成 1，`negative` 映射成 0，這樣做的目的是為了讓模型能夠計算損失函數，而在這裡由於我把檔案轉換成csv文件，因此我們會使用 `pandas` 來讀取其檔案，並用`values`轉換成numpy格式。

```
import pandas as pd

df = pd.read_csv('imdb_data.csv')
reviews = df['review'].values
sentiments = df['sentiment'].values
sentiments = (sentiments == 'positive').astype(int)  # positive→1, negative→0
print(f'review: {reviews[0][:30]}...\nsentiment label：{sentiments[0]}')
```

> 在資料前處理方面，雖然建議先移除評論中的 HTML 標籤或特殊換行符號，以提升模型的穩定性與表現，但由於這在本任務中並非重點，因此此處先略過相關處理。

2. 使用 Tokenizer
---------------

接下來我們要讓文字變成電腦看得懂的格式，也就是把它轉成 ID 序列。這裡我們直接用 Hugging Face 提供的 `AutoTokenizer`，選的是 `bert-base-uncased` 這個模型所搭配的 **WordPiece** 斷詞器（有時候大家口語上會叫它 BPE，但嚴格來說 BERT 用的是 WordPiece），這個 tokenizer 會幫你做好以下事情：

1.   它會把不認得的`單字(Word)`拆成`子詞(subword)`；
2.   自動加上模型需要的特殊標記（此章節用不到)
3.   也會幫你自動做截斷和 padding，讓每筆資料長度一致。

簡單來說就是一句話丟進去，它會處理好一切，把東西變成模型可以使用的格式。

```
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
input_datas = tokenizer(
    reviews[:2].tolist(),
    max_length=10,
    truncation=True,
    padding="longest",
    return_tensors='pt'
)

print('Tokenizer 輸出:')
print(input_datas)
print('還原文字:')
print(tokenizer.decode(input_datas['input_ids'][0]))
print(tokenizer.decode(input_datas['input_ids'][1]))
```

當我們把句子丟進 `tokenizer` 後，它會輸出像這樣的結果：

```
Tokenizer輸出:
{'input_ids': tensor([[  101, 22953,  2213,  4381,  2152,  2003,  1037,  9476,  4038,   102],
        [  101, 11573,  2791,  1006,  2030,  2160, 24913,  2004,  2577,   102]]), 
 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
                           [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
```

這裡的 `input_ids` 就是把原始文字轉成了一串 ID，也就是模型可以理解的形式。像 `[CLS]` 是 101、`[SEP]` 是 102每個單字或子詞也會對應到唯一的編號。`token_type_ids` 則是用來區分兩個句子的標記（像做句子配對任務時會用到），而 `attention_mask` 則是用來告訴模型哪些位置是實際的字哪些只是 padding。而當我們用 `decode()` 還原這些 ID，可以看到結果像這樣：

```
還原文字:
[CLS] bromwell high is a cartoon comedy [SEP]
[CLS] homelessness ( or houselessness as george [SEP]
```

通常我們會加上像 `truncation=True`、`padding="longest"` 這類參數，是為了讓每筆輸入長度一致、不會爆記憶體，又能有效利用資源。而 `decode(...)`中如果不想看到[CLS]、[SEP]被還原出來還可以加上 `skip_special_tokens=True`，還可以讓還原結果更乾淨，看起來就像單純的原始句子。

3. 建立 DataLoader
----------------

這是我們第一次真的動手做 DataLoader，所以邊做邊解釋一下。PyTorch 裡的 `Dataset` 是用來定義一筆資料長什麼樣子，而 `DataLoader` 則是負責怎麼把多筆資料湊成一個 batch。

在用 DataLoader 的時候它會先透過 `__getitem__` 回傳原始的文字跟標籤，再透過 `__len__` 來知道整筆資料有多長，判斷什麼時候跑完。通常除了 `Dataset` 跟 `DataLoader`，我們還會搭配 `collate_fn` 一起用。

這個 `collate_fn` 的功能就是在每次組 batch 的時候，可以動態地處理資料，像是做 padding 或是隨機資料增強之類的操作都會放在這裡。所以我們這邊也是把**真正的斷詞放在 `collate_fn` 裡面做**，這樣可以一次處理整個 batch 的文本。

```
from sklearn.model_selection import train_test_split
from torch.utils.data import Dataset, DataLoader
import torch

class IMDB(Dataset):
    def __init__(self, x, y, tokenizer):
        self.x = x
        self.y = y
        self.tokenizer = tokenizer

    def __getitem__(self, index):
        return self.x[index], self.y[index]
       
    def __len__(self):
        return len(self.x)
```

至於 `collate_fn` 的實際做法，其實就是根據 `__getitem__` 回傳的 `(text, label)` 把資料拿出來做進一步處理。在這裡我們會先把文字轉成張量，然後把 `[CLS]` 跟 `[SEP]` 這兩個 token 拿掉（這兩個是什麼意思我們後面會講），只留下中間的 token。最後我們會回傳一個字典，裡面包含 `input_ids` 跟 `labels`。

為什麼要用字典格式呢？因為我們在 Day 10 做的訓練器設計是動態的，也就是說它會根據這些 key 自動抓對應的輸入來餵模型。所以這邊這兩個欄位，剛好就是模型訓練時要用的兩個輸入。

```
def collate_fn(self, batch):
        batch_x, batch_y = zip(*batch)
        ids = self.tokenizer(
            batch_x,
            max_length=128,
            truncation=True,
            padding="longest",
            return_tensors='pt'
        ).input_ids
        # 移除 [CLS] 與 [SEP]（通常在頭尾）
        input_ids = ids[:, 1:-1]
        labels = torch.tensor(batch_y, dtype=torch.long)
        return { 'input_ids': input_ids, 'labels': labels }
```

在切分資料跟建立 DataLoader 的時候，**為什麼在 Windows 上 `num_workers` 一定要設成 0 呢？**

因為 Windows 跑`多線程（multi-processing）`預設是用 spawn 的方式開子程序，簡單來說就是它會重新執行一次主程式。如果你的程式沒有包在 `if __name__ == "__main__":` 裡就很容易整個炸掉，出現奇怪的錯誤。

```
x_train, x_valid, y_train, y_valid = train_test_split(
    reviews, sentiments, train_size=0.8, random_state=46, shuffle=True
)

trainset = IMDB(x_train, y_train, tokenizer)
validset = IMDB(x_valid, y_valid, tokenizer)

valid_loader = DataLoader(
    validset, batch_size=32, shuffle=True, num_workers=0,
    pin_memory=True, collate_fn=validset.collate_fn
)
train_loader = DataLoader(
    trainset, batch_size=32, shuffle=True, num_workers=0,
    pin_memory=True, collate_fn=trainset.collate_fn
)
```

當然如果你之後是在 Linux 或 WSL 環境下跑的話，就可以放心把 `num_workers` 調高，加速資料載入速度。

4. 手刻 Embedding
---------------

在自然語言處理（NLP）中，**Embedding 是一個非常關鍵的模型層**。它的本質其實就是一張「`vocab_size × emb_dim`」的表格，也可以想成是一個詞彙查詢表。每個 token（詞或字）對應到表格中的一列向量，而這個向量的長度就是 `emb_dim`，通常是模型設定的參數。

這些向量在一開始是隨機初始化的，也就是說每個 token 剛開始都只是亂數對應到某個位置。但在模型訓練的過程中，這些向量會不斷被調整。最終模型會學到讓**語意相近的 token 對應到相近的向量**，也就是說，它能幫助模型理解詞與詞之間的語意關係。

你可以把 Embedding 想成一個查表的機制只要輸入 token 的 ID，就能快速查到對應的語意向量，而這張表不只是存資料，更會隨著模型學習自動調整。在實作的時候有個小細節要特別注意：為了讓每個 batch 裡的句子長度一樣，我們會在比較短的句子後面補上一些 padding token。但這些 padding 其實只是拿來對齊格式，**它們本身沒有任何語意**。

所以當我們把這些 token 丟進 embedding 裡時，不能讓它們產生真正的特徵值。也就是說，padding 對應的那一列向量應該是全 0，而且在訓練過程中也不能被更新。

```
import torch
import torch.nn as nn
import torch.nn.functional as F

class MyEmbedding(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, padding_idx=None):
        super().__init__()
        self.weight = nn.Parameter(torch.empty(num_embeddings, embedding_dim))
        self.padding_idx = padding_idx
        nn.init.normal_(self.weight, mean=0.0, std=embedding_dim ** -0.5)
        if padding_idx is not None:
            with torch.no_grad():
                self.weight[padding_idx].zero_()
            # 確保反傳不更新 padding 列
            self.weight.register_hook(self._zero_pad_grad)
```

為了做到這件事，我們會幫模型加一個小小的 hook，裡面用 `_zero_pad_grad` 這個做法，在反向傳播的時候手動把 `padding_idx` 對應那一列的梯度清成 0。像是 `self.weight[padding_idx].zero_()` 這行，就是確保這個 padding 向量一開始就是 0，而且以後也不會被動到。這樣模型在訓練時就不會誤以為 padding 有什麼實際意義，能讓學到的語意表示更乾淨。

```
def _zero_pad_grad(self, grad):
        if self.padding_idx is None:
            return grad
        grad = grad.clone()
        grad[self.padding_idx].zero_()
        return grad
```

```
def forward(self, input_ids):  # [B,T] -> [B,T,E]
        out = self.weight[input_ids]
        if self.padding_idx is not None:
            mask = (input_ids == self.padding_idx).unsqueeze(-1)
            out = out.masked_fill(mask, 0.0)
        return out
```

> 那有人可能會問，這跟直接用 nn.Embedding 有什麼不一樣？ 其實我們這邊手動實作，是為了讓大家更清楚地看到怎麼處理 padding_idx，還有搭配 mask 的邏輯。畢竟這些細節在實際應用中很重要，理解原理才能靈活調整。但如果是實務上其實直接用 PyTorch 內建的 nn.Embedding(num_embeddings, embedding_dim, padding_idx=...) 就可以了，不用自己手動寫。

5.建立單向 LSTM（忽略 padding 版）
-------------------------

padding token 沒有實際語意，那麼對於像 LSTM 這種一個時間步一個時間步處理的模型，我們該怎麼讓它**跳過 padding 呢？**這邊我們就用昨天的 LSTM進行改寫讓大家看得更清楚。

```
class LinearTanhLSTMCore(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        H = hidden_size
        self.x_i = nn.Linear(input_size, H, bias=True)
        self.x_f = nn.Linear(input_size, H, bias=True)
        self.x_g = nn.Linear(input_size, H, bias=True)
        self.x_o = nn.Linear(input_size, H, bias=True)

        self.h_i = nn.Linear(H, H, bias=True)
        self.h_f = nn.Linear(H, H, bias=True)
        self.h_g = nn.Linear(H, H, bias=True)
        self.h_o = nn.Linear(H, H, bias=True)

        self.hidden_size = H

        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.zeros_(m.bias)
```

因此我們要看forward時每個時間步我們都會看一下當下這個 token 是不是 padding，這是靠 `mask` 來判斷的。有效的 token 對應到 `mask` 裡是 1，padding 的地方則是 0。

```
def forward(self, x, mask=None, h0=None, c0=None):
        B, T, _ = x.shape
        H = self.hidden_size
        device = x.device

        h = torch.zeros(B, H, device=device) if h0 is None else h0
        c = torch.zeros(B, H, device=device) if c0 is None else c0

        if mask is None:
            mask = torch.ones(B, T, dtype=torch.bool, device=device)

        for t in range(T):
            xt = x[:, t, :]
            valid = mask[:, t].unsqueeze(1).to(x.dtype)  # [B,1] 1.0 有效, 0.0 padding

            i = torch.sigmoid(self.x_i(xt) + self.h_i(h))
            f = torch.sigmoid(self.x_f(xt) + self.h_f(h))
            g = torch.tanh(   self.x_g(xt) + self.h_g(h))
            o = torch.sigmoid(self.x_o(xt) + self.h_o(h))

            c_new = f * c + i * g
            h_new = o * torch.tanh(c_new)

            # 只在有效 token 上更新狀態
            h = valid * h_new + (1.0 - valid) * h
            c = valid * c_new + (1.0 - valid) * c

        return h, (h, c)
```

所以我們在更新 LSTM 的狀態（hidden state 跟 cell state）時，就能根據這個 `mask` 決定要不要更新。如果是 padding，那我們就**保留原本的狀態**，不讓它參與學習。這樣模型就能專注在真正有內容的部分，不會被 padding 影響。簡單來說就是在每個時間步都問一句：「這個 token 有沒有意義？」如果沒有，就當作沒看到，LSTM 的狀態維持原樣不變。

> 同樣的這樣的做法雖然不是最快的，只是為了展示基礎原理，在實務上如果要快可以搭配Pytorch的 PackedSequence 或其他更進階的做法來優化。

6.結合雙單向 LSTM（BiLSTM）
--------------------

當我們在處理自然語言時，一個詞的意思常常不只是看它本身，而是要搭配前後文一起理解。但如果模型只能從頭讀到尾，就可能錯過那種「看到後面才恍然大悟」的情況。這時候，雙向 LSTM（BiLSTM）就派上用場了。它的做法是一邊用正向 LSTM 從前面讀過去，一邊用反向 LSTM 從後面讀回來，然後把這兩邊的資訊合起來，讓模型能同時抓住上下文的意思。

這時候如果我們只用單向的 LSTM，也就是模型**只能從左讀到右**（正向），那它就只能看到目前 token 的過去，沒辦法預測或理解後面可能發生的事。

而在實作方式上我們可以用剛剛建立的兩個 `LinearTanhLSTMCore`各自跑完，把兩個 LSTM 的最後 hidden state 接起來最後丟進一個全連接層，做二分類任務

```
class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_size, num_classes=2, padding_idx=0, dropout=0.2):
        super().__init__()
        self.embed = MyEmbedding(vocab_size, emb_dim, padding_idx=padding_idx)
        self.fwd = LinearTanhLSTMCore(emb_dim, hidden_size)
        self.bwd = LinearTanhLSTMCore(emb_dim, hidden_size)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Linear(hidden_size * 2, num_classes)
        nn.init.xavier_uniform_(self.fc.weight)
        nn.init.zeros_(self.fc.bias)
        self.padding_idx = padding_idx
        self.criterion = nn.CrossEntropyLoss()  # 二分類用 CE 配 2-logits

    def forward(self, input_ids, labels=None):  # input_ids: [B,T], labels: [B]
        mask = input_ids != self.padding_idx               # [B,T] bool
        x = self.embed(input_ids)                          # [B,T,E]

        h_fwd, _ = self.fwd(x, mask=mask)                  # [B,H]
        x_rev = torch.flip(x, dims=[1])
        mask_rev = torch.flip(mask, dims=[1])
        h_bwd, _ = self.bwd(x_rev, mask=mask_rev)          # [B,H]

        h_cat = torch.cat([h_fwd, h_bwd], dim=1)           # [B,2H]
        logits = self.fc(self.dropout(h_cat))              # [B,2]

        loss = None
        if labels is not None:
            loss = self.criterion(logits, labels)          # labels: int64 in {0,1}
        return loss, logits

model = BiLSTMClassifier(
    vocab_size=len(tokenizer),
    emb_dim=256,
    hidden_size=128,
    num_classes=2,
    padding_idx=tokenizer.pad_token_id or 0,
    dropout=0.2
)
```

這裡我們把 loss 放在輸出的第 0 個位置，預測結果放在第 2 個位置，這樣設計是為了配合我們之前設定好的訓練器。另外，輸入的 `input_ids` 跟 `labels` 的名稱也不能亂改，因為這些名稱是根據 Dataloader 產生的索引來對應資料的。如果換了名字，整個流程就對不上了。

7.使用 Trainer 訓練
---------------

到了這一步我們就是把資料跟模型交給你現成提供的 Trainer 來訓練就好。至於優化器的部分，直接用大家最常用的 Adam 起手式就可以了簡單又夠用。

```
from trainer import Trainer
import torch.optim as optim

optimizer = optim.Adam(model.parameters(), lr=1e-3)
trainer = Trainer(
    epochs=100,
    train_loader=train_loader,
    valid_loader=valid_loader,
    model=model,
    optimizer=optimizer,
)
trainer.train(show_loss=false)
```

輸出結果：

```
Train Epoch 0: 100%|██████████| 1250/1250 [17:25<00:00,  1.20it/s, loss=0.487]
Valid Epoch 0: 100%|██████████| 313/313 [01:19<00:00,  3.96it/s, loss=0.249]
Saving Model With Loss 0.44163
Train Loss: 0.50866 | Valid Loss: 0.44163 | Best Loss: 0.44163

Train Epoch 1: 100%|██████████| 1250/1250 [17:21<00:00,  1.20it/s, loss=0.412]
Valid Epoch 1: 100%|██████████| 313/313 [01:21<00:00,  3.83it/s, loss=0.495]
Saving Model With Loss 0.38657
Train Loss: 0.31322 | Valid Loss: 0.38657 | Best Loss: 0.38657

Train Epoch 2: 100%|██████████| 1250/1250 [17:16<00:00,  1.21it/s, loss=0.113]
Valid Epoch 2: 100%|██████████| 313/313 [01:20<00:00,  3.90it/s, loss=0.144]
Train Loss: 0.19454 | Valid Loss: 0.45054 | Best Loss: 0.38657
```

在訓練的過程中，有幾個實用的小建議可以參考。首先要注意觀察 `train` 和 `valid loss` 是否出現分岔的情況，如果可能代表模型過擬合，這時可以在模型加入 `dropout` 或設定 優化器的`weight_decay` 來做正規化。或是我們用`trainer`的`grad_clip`防止梯度爆炸的問題。

下集預告
----

現在你應該已經更了解LSTM這個模型到底在做什麼了。今天我們也順便介紹了Embedding的架構，這其實就是自然語言處理裡的核心基礎之一。不過在AI模型的應用裡，除了我們常見的分類模型之外，還有一種很重要的生成式模型，明天我會帶你一步步理解一個簡單的文字生成框架，讓你知道怎麼讓模型「寫出文字」。

---

<a id="day-13"></a>

## Day 13｜【Day 13】模型真的理解語言嗎？從 Seq2Seq 看 AI 如何學會翻譯
- 原文：https://ithelp.ithome.com.tw/articles/10389023

前言
--

模型大致上可以分成兩大類**分類型**的跟**生成型**的。通常分類的模型會用到 `Encoder` 架構，也就是我們前面幾個章節提到的那些內容，其實都是在講 Encoder 的應用。那如果我們想要讓模型具備生成的能力，就需要導入 Decoder 架構來處理。

Decoder 架構又可以分成兩種做法**一種是單純使用 Decoder**，另一種則是先**透過 Encoder 把輸入轉換成比較複雜的特徵，再交給 Decoder 來解讀**並產生輸出。

後面的章節會一直圍繞在這個主題上，介紹不同類型的 Encoder 和 Decoder 架構，它們之間有什麼差別、又是怎麼演進的。而今天我們會先來看看一個滿經典的 Encoder-Decoder 架構`Seq2Seq`。

`Seq2Seq（Sequence to Sequence）`是一種經典的架構，主要是拿來處理輸入跟輸出都是「序列」的任務，最典型的例子就是**機器翻譯**。比如說我們給它一句英文，它就會輸出一段對應的中文翻譯，**輸入是一串文字，輸出也是一串文字。**這個模型背後的概念其實不難它基本上是由`Encoder（編碼器）`和一個 `Decoder（解碼器）`。

![Image 1: https://ithelp.ithome.com.tw/upload/images/20241001/20152236Odl0dYeLrQ.png](images/series-7467/day-17/20152236Odl0dYeLrQ-8f9951c1ef417611.png)

Encoder
-------

Encoder 的目的是先把輸入的句子「看過一遍」然後把它壓縮成一個高維向量（通常會叫它 `context vector` 或 `hidden state`），也可以把它想像成是模型對整個輸入句子的「理解」。接著這個向量會傳給 Decoder，Decoder 再根據這個資訊一步一步地產生輸出。

簡單來說當我們在做英文翻譯任務的時候，模型會先讀取大量的文字，然後根據這些內容產生一個 context vector。如果你還記得我們之前在情緒分析那一章提到的東西，那個時候模型在最後進入線性分類器之前，其實也會產生一個類似的向量。

![Image 2: https://ithelp.ithome.com.tw/upload/images/20241001/20152236tHQ42mnqUR.png](images/series-7467/day-17/20152236tHQ42mnqUR-a2088e48fd7eee34.png)

但這兩者其實有一點差別，在情緒分析裡我們的目標是根據 context vector 判斷出這句話是正面還是負面，也就是說，我們是用這個向量來分類。但在 Encoder-Decoder 架構裡，context vector 是要給 Decoder 使用的，它不是用來分類，而是幫助 Decoder 去理解輸入的句子，然後一步步地產生正確的翻譯。

簡單來說情緒分析就像是看完一整部電影後，寫下一句**這部電影讓我感到很感動**或**這部片子真無聊**，你不是要講出電影的內容，只是把整體的感受濃縮成一句話而**翻譯任務**則比較像是你看完這部電影後，要跟一個不懂這部語言的朋友轉述整個劇情。你腦中先把電影的內容理解一遍（這就是 Encoder 做的事），再用你朋友聽得懂的語言，把故事重新講一遍（這就是 Decoder 的工作）。

![Image 3: https://ithelp.ithome.com.tw/upload/images/20241001/20152236NkLwBiNw7m.png](images/series-7467/day-17/20152236NkLwBiNw7m-380fc44fbe937c08.png)

其實這背後的數學邏輯你早就學過了，就是我們在 Day 11 學時間序列模型的時候用到的那些概念。所以你會發現，現在寫這個 Encoder 程式碼，其實也沒什麼特別神祕的地方。說穿了我們只是把原本時間序列模型裡面最後那個線性分類器拿掉而已，剩下的結構幾乎都一樣。而這邊我們是用 PyTorch 的方式來實作，基本上就是沿用你已經熟悉的那套寫法。

```
import torch
import torch.nn as nn

class LSTMEncoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers=1, bidirectional=False, dropout=0.0):
        super(LSTMEncoder, self).__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers,
                            bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0)

        self.hidden_dim = hidden_dim
        self.bidirectional = bidirectional

    def forward(self, src):
        # src shape: [seq_len, batch_size]
        embedded = self.embedding(src)  # [seq_len, batch_size, emb_dim]
        outputs, (hidden, cell) = self.lstm(embedded)
        # outputs shape: [seq_len, batch_size, hidden_dim * num_directions]
        # hidden, cell shape: [num_layers * num_directions, batch_size, hidden_dim]
        return outputs, (hidden, cell)
```

不過這邊有幾個地方要特別注意一下，通常在做 Encoder 處理的時候，我們會加上一些**對齊**的設計，目的就是幫助模型更清楚地知道一句話的開始和結束。

為了達成這個目的，我們常常會在輸入序列的開頭加上一個 `SOS token（也叫 BOS token，Start/Beginning of Sentence）`，然後在結尾加上一個 `EOS token（End of Sentence）`。這樣模型在讀取context vector的時候，就能比較有方向感，知道什麼時候是句子的起點，什麼時候是終點。

Decoder
-------

Decoder 的做法就跟 Encoder 有點不一樣了。你還記得我們在 Day 11 學時間序列的時候，通常是怎麼初始化 hidden state 嗎？那時候我們會在 t=0 的時候給一個全 0 的陣列，或者是用隨機的值來當作起始狀態。

![Image 4: https://ithelp.ithome.com.tw/upload/images/20241001/20152236waSbMTLMGf.png](images/series-7467/day-17/20152236waSbMTLMGf-f7a3121792fd150e.png)

但在 Decoder 裡，t=0 的起點不是隨便給的，而是**直接拿 Encoder 最後輸出的 context vector**，也就是它對整段輸入文字的理解，來當作 Decoder 的第一個 hidden state，這時候我們會再加上一個 **SOS token**代表我要開始生成了，來讓 Decoder 開始產出 t=1 的第一個文字，我們可以用以下數學式大表示。

![Image 5: https://ithelp.ithome.com.tw/upload/images/20241001/20152236w685BPE8vV.png](images/series-7467/day-17/20152236w685BPE8vV-bf3f2157b3a77230.png)

所以這其實也代表，Decoder 在一開始可能會先產出「我」，接著根據它剛剛自己輸出的「我」，再產出下一個字「喜歡」，然後是「你」，最後當它判斷整句話已經完成，就會輸出一個 **EOS token**，整個句子就結束了。

```
import torch
import torch.nn as nn

class LSTMDecoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers=1, dropout=0.0):
        super(LSTMDecoder, self).__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, num_layers=num_layers, dropout=dropout if num_layers > 1 else 0)
        self.fc_out = nn.Linear(hidden_dim, output_dim)

    def forward(self, input_token, hidden, cell):
        # input_token: [batch_size] → 只是一個 token 的 ID
        input_token = input_token.unsqueeze(0)  # → [1, batch_size]
        embedded = self.embedding(input_token)  # → [1, batch_size, emb_dim]

        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))
        # output: [1, batch_size, hidden_dim]
        
        prediction = self.fc_out(output.squeeze(0))  # → [batch_size, output_dim]
        return prediction, hidden, cell
```

如果用比較程式化的角度來看，Decoder 的每一步基本上都是這樣運作的它會接收一個來自 Encoder 的 **context vector**，再加上一個當前的輸入 token，然後根據這些資訊算出一個 **機率分布**，這個分布就是模型對所有字彙表（embedding 中的詞）的預測。最後透過 **softmax** 函數，我們就能得到每一個詞在這個時間點被選中的機率，而機率最高的那個詞，就會被 Decoder 當作這一步的輸出。

Encoder-Decoder架構
-----------------

雖然我們剛剛提到 Decoder 是一個時間點接著一個時間點往下產生字，像是先輸出「我」，再輸出「喜歡」，然後「你」這樣接續下去。但其實在訓練的時候通常不會真的拿模型自己在前一個時間點產生的字來當作下一個時間點的輸入。因為生成任務本身就比分類困難很多你可以想像，分類只是選「對或錯」，但生成是要從上千個詞裡挑一個字，還要文意通順、語法正確。如果模型在早期訓練時，輸出的字就錯了，那後面一整串就會跟著歪掉，完全走偏，這在長序列特別明顯。

所以我們在訓練的時候，通常會用一種技巧叫做 **Teacher Forcing**。這個方法很簡單，就是在每個時間點，不管模型剛剛自己預測了什麼，我們都**強制餵給它正確答案**，也就是 ground truth 的那個字，當作它下一步的輸入。這樣做可以幫助模型看清楚理想的路線，不用一開始就承擔自己的錯誤後果，也更快學會正確的語言模式。

```
class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device, sos_token_id):
        super(Seq2Seq, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device
        self.sos_token_id = sos_token_id

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        # src: [seq_len_src, batch_size]
        # trg: [seq_len_trg, batch_size]
        batch_size = trg.shape[1]
        trg_len = trg.shape[0]
        output_dim = self.decoder.fc_out.out_features

        outputs = torch.zeros(trg_len, batch_size, output_dim).to(self.device)

        encoder_outputs, (hidden, cell) = self.encoder(src)

        # 第一步的輸入是 SOS token
        input_token = torch.tensor([self.sos_token_id] * batch_size).to(self.device)

        for t in range(trg_len):
            output, hidden, cell = self.decoder(input_token, hidden, cell)
            outputs[t] = output

            teacher_force = torch.rand(1).item() < teacher_forcing_ratio
            top1 = output.argmax(1)  # 選出機率最高的 token

            input_token = trg[t] if teacher_force else top1

        return outputs
```

不過**完全依賴 Teacher Forcing** 也不是一件好事，雖然這種做法在訓練初期能加快收斂速度，但它也有一個很明顯的缺點模型變得太依賴「正確答案」來幫它導路。這會導致什麼問題？就是當模型真正上場、要自己一路生成句子的時候，它可能一下就迷路了。因為它在訓練時從來沒有**犯錯後自我修正**的機會，一旦輸出錯了一個字，它根本不知道該怎麼把句子拉回來。

所以比較理想的做法是什麼？就是**適度地加入一點模型自己的輸出**，當作下一步的輸入，讓它學會在雜訊中也能找到方向。這種策略有時候會用 **scheduled sampling** 來實作，也就是慢慢降低 Teacher Forcing 的比例，讓模型逐步學習自立自強。簡單說就是你一開始牽著它的手走，但久了你要放手，讓它自己練習怎麼走，即使跌倒，也得學會站起來。

同時這也是一個很重要的步驟，因為在推論（inference）時，它就不能再靠正確答案了這樣一步錯、步步錯，就可能完全偏離主題，開始產出不相關、胡說八道、邏輯不通的文字，這就是所謂的「幻覺」。這也是為什麼現在越來越多的研究都強調，要在訓練中加入一定程度的雜訊或不確定性，讓模型學會面對錯誤情境，才不會一到真實應用就崩潰。

下集預告
----

讓一個 context vector 承擔整段輸入句子的所有資訊，其實對 Decoder 來說是滿吃力的。尤其當句子越來越長時，後面的資訊就越容易被壓縮掉甚至遺忘，這也導致模型在產生句尾的時候，常常會出現內容不清楚、語意走偏的狀況。

為了改善這個問題早期的方法會用一種叫 sliding window 的技巧，簡單來說就是把長句子切成一段一段的固定長度，分批進行翻譯。但這樣做其實還是有侷限，因為它沒辦法真正解決「資訊集中在一個向量裡」這個根本問題。

所以後來就出現了一個非常重要的技術`Attention`所以明天我們就要來進一步看看：要怎麼在 Seq2Seq 裡面加上 Attention，讓它更聰明、更靈活地生成文字。

---

<a id="day-14"></a>

## Day 14｜【Day 14】模型記性差？Attention 來幫忙！

- 原文：https://ithelp.ithome.com.tw/articles/10389847
- 發佈時間：2025-09-28 20:56:28

前言
==

不管是 LSTM 還是 RNN，只要時間步太多，就很容易遇到梯度消失的問題——這點我們在 Day 11 也有提過。當資料一路傳到最後一個 context vector 的時候，原本的資訊可能早就已經失真了。這也意味著在 Seq2Seq 模型裡，Encoder 最後輸出的那個 context vector，其實能提供的有用資訊可能很有限。

接著 Decoder 就只能靠這個 context vector 當作初始狀態開始生成文字。但這樣一來就很容易出現一個狀況：模型會逐漸忘記前面的輸入內容，導致產生到後段文字時容易出錯。那要怎麼解決這個問題呢？這就是 Attention（注意力機制）出場的原因了。

Seq2Seq + Attention
===================

當我們講到 Attention 的核心概念其實可以這樣想，Decoder 在每次產生一個字或詞的時候，並不是死板地依賴某一個固定的上下文，而是會**根據當下的情境，動態地去「挑選」Encoder 所給的那些資訊**裡，哪些比較重要、該多看一點，哪些相對次要、可以少關注。這就像人在聽人講話時會根據對方說的內容有選擇性地去注意某些重點一樣。

![Image 9: https://ithelp.ithome.com.tw/upload/images/20241002/201522364w6Y3g5GjW.png](images/series-7467/day-18/201522364w6Y3g5GjW-f1f036087782c843.png)

以下為了方便解說我們稱Encoder的輸出為context vector而Deocder則為hidden state。

Attention
=========

它的計算方式其實不難理解。基本上就是把 Encoder 目前的context vector ( c(t) ) 和 Decoder 上一個時間點的context vector ( c(t-1) ) 拿來做運算，這個運算可以有很多種做法，比如說直接把兩個向量加在一起、拼接起來，或者互相相乘，在這麼多做法當中，最有名的就是 Bahdanau Attention 這個方式。

![Image 10: https://ithelp.ithome.com.tw/upload/images/20241002/20152236BNkNpNwjhv.png](images/series-7467/day-18/20152236BNkNpNwjhv-e580bb67888b9d4a.png)

其實這個公式的邏輯不難，簡單講就是把 Encoder 的輸出 ( c(t) ) 跟 Decoder 當下的狀態 ( h(t) ) 拿來湊一湊，變成一個上下文向量。然後這個組合資訊會先丟進一個全連接層，也就是 ( e(t) )，做個線性轉換。接著，再把這些轉換後的結果丟進 Softmax，算出一組機率分布，也就是 Attention Score ( a_t(i) )，這樣一來我們就能得到一個 Attention Weights 的矩陣，也就是每個時間點該注意哪一部分輸入的程度，全部都列出來了。

在程式上我們可以這樣寫要注意的是，Decoder 的 hidden state 通常只有一個，因為我們的模型架構設計是讓 Decoder 根據當下的狀態，去找出 Encoder 裡面最關鍵的 context vector。

```python
class BahdanauAttention(nn.Module):
    def __init__(self, hidden_size):
        super(BahdanauAttention, self).__init__()
        self.encoder_projection = nn.Linear(hidden_size, hidden_size)
        self.decoder_projection = nn.Linear(hidden_size, hidden_size)
        self.attention_v = nn.Linear(hidden_size, 1)
        self.tanh = nn.Tanh()
        self.softmax = nn.Softmax(dim=-1)

    def forward(self, encoder_hidden, decoder_hidden):
        # encoder_hidden: (batch, time, hidden)
        # decoder_hidden: (batch, 1, hidden)
        energy = self.tanh(self.encoder_projection(encoder_hidden) + self.decoder_projection(decoder_hidden))
        scores = self.attention_v(energy)                 # (batch, time, 1)
        scores = scores.squeeze(2).unsqueeze(1)           # (batch, 1, time)
        attn = self.softmax(scores)                       # (batch, 1, time)
        context = torch.bmm(attn, encoder_hidden)         # (batch, 1, hidden)
        return context
```

所以Encoder 的部分會對每個時間步都算出一個 context vector，而 Decoder 則是只關注**當下這一刻**的 hidden state，來做對應的注意力計算。換句話說Decoder 是根據目前的位置，去決定該把注意力放在哪些 Encoder 的輸出上。

Encoder
=======

至於 Encoder 的部分寫法其實跟之前差不多，不過這次有個小地方不太一樣，我們這次用的是模型的 `output`，不是 `hidden`。為什麼呢？因為 `output` 會包含每一個時間步的資訊，也就是整個序列的 context vectors，而 `hidden` 只會給你最後一個時間點的狀態，對注意力機制來說就不夠用了。

```python
class EncoderLSTM(nn.Module):
    def __init__(self, vocab_size, hidden_size, padding_idx):
        super(EncoderLSTM, self).__init__()
        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=padding_idx)
        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)
        self.dropout = nn.Dropout(0.1)

    def forward(self, token_ids):
        embedded = self.dropout(self.embedding(token_ids))
        # embedded: (batch_size, time_step, emb_dim)
        output, (h, c) = self.lstm(embedded)
        # output: (batch_size, time_step, hidden_size)
        # h, c: (1, batch_size, hidden_size)
        return output, (h, c)
```

Decoder
=======

那在 Decoder 這邊，除了原本輸入的 Embedding 後的 Token，我們還要把 Attention 機制算出來的 context vector 加進來。這兩個向量我們這裡就直接用 concatenate 的方式把它們接起來。

當然啦也可以選擇先用一個 Linear 層來把維度壓縮一下，這樣可以讓模型的 hidden state 不會變太大。不過這種做法沒有一個標準答案，看實作需求而定。我們這裡就先用 `cat` 來拼接，做法簡單直觀一點。

```python
class DecoderLSTM(nn.Module):
    def __init__(self, attention, hidden_size, output_size, padding_idx):
        super(DecoderLSTM, self).__init__()
        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=padding_idx)
        self.lstm = nn.LSTM(2 * hidden_size, hidden_size, batch_first=True)
        self.output_projection = nn.Linear(hidden_size, output_size)
        self.dropout = nn.Dropout(0.1)
        self.attention = attention

    def forward(self, encoder_outputs, decoder_hidden, decoder_input_ids):
        # decoder_hidden: (h, c), each (1, batch, hidden)
        embedded = self.dropout(self.embedding(decoder_input_ids))  # (batch, 1, emb_dim)
        h, c = decoder_hidden
        decoder_state = h.permute(1, 0, 2)                          # (batch, 1, hidden)
        context = self.attention(encoder_outputs, decoder_state)    # (batch, 1, hidden)
        lstm_in = torch.cat((embedded, context), dim=-1)            # (batch, 1, 2*hidden)
        output, (h, c) = self.lstm(lstm_in, (h, c))                 # output: (batch, 1, hidden)
        logits = self.output_projection(output)                     # (batch, 1, vocab)
        return logits, (h, c)
```

這兩種做法的差別其實蠻有意思的用 `cat` 的方式，意思就是我們單純把兩個向量並排起來，讓它們分開保留各自的資訊，沒有做太多加工。

但如果是用 Linear 層來處理，那代表我們想要把這些資訊融合起來，讓模型自己去學哪些特徵比較重要。這時候，通常還會搭配像 `tanh` 這種非線性函數，來讓輸出變得更平滑、更接近一種平均分佈的 hidden state。

下集預告
====

明天我會帶大家實際操作，看看怎麼把這些元件組合起來，完成一個簡單的機器翻譯任務。我也會講解，該怎麼用評估指標來判斷模型生成的結果到底好不好。

這點其實蠻重要的，因為我們不能只看 Loss 值來決定模型表現。Loss 只是模型在訓練資料上的損失，跟實際應用的品質可能不完全對應。實際上，我們要根據任務的需求來選擇適合的評估標準。

比如說，有些任務特別在意準確率，而生成模型又會因為策略（像是 greedy、beam search 或 sampling）不同而產生不同的輸出結果。所以如果我們只看 Loss，是沒辦法全面掌握模型表現的。

---

<a id="day-15"></a>

## Day 15｜【Day 15】Attention is All You Need？先別急來看看 LSTM 的最後一舞
- 原文：https://ithelp.ithome.com.tw/articles/10389873

前言
--

昨天我們把 Seq2Seq 搭配 Attention 的模型結構完整實作出來，而今天的重點就放在訓練與應用，讓它能處理基本的中英翻譯。那問題來了怎麼判斷這些翻譯結果到底好不好？我們會先聊各種量化生成品質的方法，接著還會討論生成策略這個關鍵主題，並用實際翻譯範例對照，看看不同策略會讓輸出產生哪些差異。

文字翻譯模型
------

1. 固定隨機亂數
---------

當我們在訓練深度學習模型時，特別是在處理高準確度的任務時，一個常見卻經常被忽略的問題就是**隨機性**帶來的結果波動。簡單來說如果我們今天用同樣的資料、同樣的模型、甚至同樣的訓練設定重跑一次，卻得到了不一樣的結果，這就讓我們很難判斷模型的實際效能。這時固定隨機亂數種子就派上用場了。

在深度學習中有許多環節會用到隨機性，例如：初始化權重、資料打亂（shuffling）、Dropout 機制等，這些隨機因素會導致每次訓練時模型的行為略有不同。為了確保結果具有`可重現性（reproducibility）`，我們通常會在程式的一開始就鎖定這些亂數來源，讓訓練的每一步都能夠照著一樣的隨機路徑走，這對於實驗的比較與調參工作極為重要，而程式碼我們可以如此撰寫。

```
import torch
import numpy as np
import random

def set_seeds(seed):
    random.seed(seed)  # 設定 Python 標準庫的亂數生成器種子
    np.random.seed(seed)  # 設定 NumPy 亂數生成器種子
    torch.manual_seed(seed)  # 設定 PyTorch 的 CPU 亂數生成器種子
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)  # 設定 PyTorch 在單個 GPU 上的亂數種子
        torch.cuda.manual_seed_all(seed)  # 設定 PyTorch 在所有 GPU 上的亂數種子
    torch.backends.cudnn.benchmark = False  # 禁用 cuDNN 的基準測試功能
    torch.backends.cudnn.deterministic = True  # 強制 cuDNN 使用確定性算法

set_seeds(2526)
```

而在這程式中特別要注意的是最後兩行 `torch.backends.cudnn` 的設定，這是在使用 GPU 訓練時的一個小技巧。因為 cuDNN 在預設情況下會自動尋找最佳化的算法來加速運算，但這種最佳化有時候是非確定性的，也會造成結果的不同，因此我們關閉這個功能改用比較穩定、可重現的算法。

2. 資料前處理
--------

我已經先把中英文對照的資料整理好，存在 `translate.csv` 這個檔案裡。用 Pandas 讀進來之後，我們就能很快把這些資料轉成好操作的格式：

```
import pandas as pd
df = pd.read_csv('translate.csv')
input_texts = df['chinese'].values
target_texts = df['english'].values
```

這邊的 `input_texts` 裡面放的是中文原文，而 `target_texts` 則是對應的英文翻譯。這種格式對於訓練像 Transformer 這類的 Seq2Seq 模型來說非常實用。

在翻譯任務中，我們會需要兩個不同的 Tokenizer，分別處理中英文。這時就可以用 Hugging Face 提供的 `AutoTokenizer`，像下面這樣：

```
from transformers import AutoTokenizer

def process_texts(tokenizer, texts):
    ids = tokenizer(texts[20]).input_ids
    return tokenizer.decode(ids)

src_tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')
tgt_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast=False)

cn_text = process_texts(src_tokenizer, input_texts)
en_text = process_texts(tgt_tokenizer, target_texts)

print('中文轉換後的結果:', cn_text, '\n英文轉換後的結果:', en_text)
```

輸出看起來像這樣：

```
中文轉換後的結果: [CLS] 我 沒 事 。 [SEP] 
英文轉換後的結果: [CLS] i'm ok. [SEP]
```

可以看到，Tokenizer 已經自動幫我們把 BERT 所需的特殊符號 `[CLS]` 跟 `[SEP]` 加上去了，這對於後面模型的輸入格式來說是很關鍵的。

3. Pytorch DataLoader包裝
-----------------------

當我們準備好中英文對照資料，要拿來訓練模型之前，會先自己寫一個叫做 `TranslateDataset` 的類別，幫助我們處理像是 Tokenizer 還有把資料打包成 batch 這些步驟。

```
from torch.utils.data import Dataset, DataLoader
from sklearn.model_selection import train_test_split

class TranslateDataset(Dataset):
    def __init__(self, x, y, src_tokenizer, tgt_tokenizer):
        self.x = x
        self.y = y
        self.src_tokenizer = src_tokenizer
        self.tgt_tokenizer = tgt_tokenizer

    def __getitem__(self, index):
        return self.x[index], self.y[index]
       
    def __len__(self):
        return len(self.x)
    
    def collate_fn(self, batch):    
        batch_x, batch_y = zip(*batch)
        src_tokens = self.src_tokenizer(
            list(batch_x),
            max_length=256,
            truncation=True,
            padding="longest",
            return_tensors='pt'
        )
        tgt_tokens = self.tgt_tokenizer(
            list(batch_y),
            max_length=256,
            truncation=True,
            padding="longest",
            return_tensors='pt'
        )
       
        return {
            'input_ids': src_tokens.input_ids,
            'labels': tgt_tokens.input_ids
        }

        
x_train, x_valid, y_train, y_valid = train_test_split(
    input_texts,
    target_texts,
    train_size=0.8,
    random_state=46,
    shuffle=True
)

trainset = TranslateDataset(x_train, y_train, src_tokenizer, tgt_tokenizer)
validset = TranslateDataset(x_valid, y_valid, src_tokenizer, tgt_tokenizer)

train_loader = DataLoader(
    trainset,
    batch_size=64,
    shuffle=True,
    num_workers=0,
    pin_memory=True,
    collate_fn=trainset.collate_fn
)
valid_loader = DataLoader(
    validset,
    batch_size=64,
    shuffle=True,
    num_workers=0,
    pin_memory=True,
    collate_fn=validset.collate_fn
)
```

不過在正式開始之前，有一個超級關鍵的地方一定要注意，我們在用 `train_test_split` 把資料分成訓練集和驗證集的時候，**一定要記得把 `shuffle` 設成 `True`**！這點為什麼這麼重要呢？因為我們的資料原本是照句子長度排好的從短的排到長的。如果你沒有打亂順序直接切一刀，那訓練集可能就全是短句，驗證集就會變成一堆長句。

這樣一來問題就大了模型在訓練階段只看過簡單的短句，根本沒機會學習怎麼處理比較長的句子。等到驗證階段突然丟給它一堆沒見過的長句，它當然會表現得很爛。

4. 建立Seq2Seq模型
--------------

在這裡我們使用昨天已經準備好的 LSTM Encoder、Decoder 還有 Attention 模組，來組成一個完整的 Attention-based Seq2Seq 模型。我們把這個包成一個 `Attentionseq2seq` 類別。

```
class Attentionseq2seq(nn.Module):
    def __init__(
        self,
        src_vocab_size: int,
        tgt_vocab_size: int,
        hidden_size: int,
        src_pad_idx: int,
        tgt_pad_idx: int,
        bos_token_id: int,
        eos_token_id: int,
        max_decode_len: int = 128
    ):
        super().__init__()
        self.encoder = EncoderLSTM(src_vocab_size, hidden_size, src_pad_idx)
        self.attn = BahdanauAttention(hidden_size)
        self.decoder = DecoderLSTM(self.attn, hidden_size, tgt_vocab_size, tgt_pad_idx)
        self.bos_id = bos_token_id
        self.eos_id = eos_token_id
        self.max_decode_len = max_decode_len
        # 用 CrossEntropyLoss 直接吃 raw logits
        self.criterion = nn.CrossEntropyLoss(ignore_index=tgt_pad_idx)

        self.src_pad_idx = src_pad_idx
        self.tgt_pad_idx = tgt_pad_idx
```

在模型的前向傳遞過程中，主要分為幾個關鍵步驟。首先輸入的句子會被送進 Encoder，透過其處理後得到對應的輸出（`enc_out`）以及最後的隱藏狀態`（h, c）`。接著會根據輸入的 padding 資訊建立 source mask，這是為了讓後續的Attention能夠忽略掉那些不應參與計算的 padding 位置。

進入 Decoder 階段後，模型會從起始標記 `<BOS>` 開始，逐步產生目標句子中的每個詞。這裡會用到 teacher forcing 技術，也就是在每一步決定是用 Ground truth 作為下一步的輸入，還是用模型自己預測出來的詞。這個選擇是透過機率來控制的。

```
def forward(self, src_ids, tgt_ids, teacher_forcing_ratio: float = 1.0):
        device = src_ids.device
        enc_out, (h, c) = self.encoder(src_ids)             # enc_out: (B, T, H)
        src_mask = (src_ids != self.src_pad_idx)            # (B, T), True=valid

        T = tgt_ids.size(1)
        logits_steps = []
        cur = tgt_ids[:, 0:1]  # BOS

        for t in range(1, T):
            step_logits, (h, c) = self.decoder(enc_out, (h, c), cur, src_mask)
            logits_steps.append(step_logits)

            # 每 step 的隨機 teacher forcing
            use_tf = torch.rand((), device=device) < teacher_forcing_ratio
            next_in = tgt_ids[:, t:t+1]
            pred = step_logits.argmax(-1)
            cur = next_in if use_tf else pred

        logits = torch.cat(logits_steps, dim=1)  # (B, T-1, V)

        target = tgt_ids[:, 1:]
        loss = self.criterion(
            logits.reshape(-1, logits.size(-1)),
            target.reshape(-1)
        )
        return loss, logits
```

在訓練階段，我們會把每個時間步驟模型輸出的 logit 都收集起來，然後跟對應的正確目標詞比對，用 CrossEntropyLoss 來計算整體的損失。這部分沒什麼懸念。

但到了生成階段情況就不一樣了，我們不再依賴 ground truth也就是說模型得靠自己一步一步地生出接下來的詞，這就是所謂的`自回歸（autoregressive）`生成過程。

```
@torch.no_grad()
    def generate(self, input_ids, max_len=50):
        self.eval()
        batch_size = input_ids.size(0)

        # Encoder
        encoder_outputs, decoder_hidden = self.encoder(input_ids)
        src_mask = (input_ids != self.src_pad_idx)   # (B, T_src)

        # 初始化 decoder 輸入為 BOS
        decoder_input = torch.full(
            (batch_size, 1),
            self.bos_id,
            dtype=torch.long,
            device=input_ids.device
        )

        generated_ids = []
        finished = torch.zeros(batch_size, dtype=torch.bool, device=input_ids.device)

        for _ in range(max_len):
            # decoder: (B,1,V)
            step_logits, decoder_hidden = self.decoder(
                encoder_outputs, decoder_hidden, decoder_input, src_mask
            )

            # 直接 argmax 拿下一個 token
            next_token = step_logits.argmax(dim=-1)  # (B,1)

            # 對已完成序列固定輸出 EOS
            next_token = next_token.masked_fill(finished.unsqueeze(1), self.eos_id)

            generated_ids.append(next_token)
            decoder_input = next_token

            finished |= next_token.eq(self.eos_id).squeeze(1)
            if finished.all():
                break

        generated_ids = torch.cat(generated_ids, dim=1)  # (B, L)
        return generated_ids
```

Decoder 的輸入一開始是 `<BOS>`，然後每次迭代都把上一步產生的 token 當成下一步的輸入，這就是模型在自己跟自己對話的過程。模型每步會輸出 logits，我們直接對它做 argmax，挑出最有機會的下一個 token。要注意的是，如果某個樣本已經產生了 `<EOS>`，那後面的步驟就會固定讓它繼續產生 `<EOS>`，這是透過 `masked_fill` 這一行達成的。

為了追蹤哪些句子已經結束，我們用一個 `finished` 的布林張量記錄每個樣本的狀態。只要全部樣本都完成了，我們就可以提早跳出回圈，省一點計算資源。最後把每一輪產生的 token 串起來，組成完整的輸出序列，這就是模型最終生成的結果。

> 我們在每個時間步都直接用 argmax 選出機率最大的那個 token，這種做法就叫做 Greedy Decode。它的意思很簡單**每一步都貪婪地挑目前看起來最有可能的選項**，完全不回頭、也不考慮全局最優。雖然簡單快速，但有可能錯過更好的整體序列。

最後我們就可以實際把模型建起來了，這裡比較特別的地方是，我們直接把 tokenizer 裡的特殊符號拿來當成生成任務的起點和終點：也就是把 CLS 當作 `<BOS>`，SEP 當作 `<EOS>`。雖然這些符號原本不是設計來這樣用的，但它們在語意上其實也挺接近，實務上也很常這樣處理。

```
model = Attentionseq2seq(
    src_vocab_size=len(src_tokenizer),
    tgt_vocab_size=len(tgt_tokenizer),
    hidden_size=512,
    src_pad_idx=src_tokenizer.pad_token_id,
    tgt_pad_idx=tgt_tokenizer.pad_token_id,
    bos_token_id=tgt_tokenizer.cls_token_id,          # 目標 BOS
    eos_token_id=tgt_tokenizer.sep_token_id,          # 目標 EOS
    max_decode_len=128
)
```

其他設定就比較直觀了我們把來源和目標語言的 vocabulary 長度、padding token 的位置，以及最大生成長度這些資訊都傳進模型裡，讓它能正確處理序列的開始、中止、以及忽略不重要的 padding 區塊。

4. 建立Seq2Seq模型
--------------

有了模型之後，接下來就是進入訓練階段啦。這裡我們使用 AdamW 這個優化器來更新模型參數，學習率設成 1e-3。設定上同樣沒有太多華麗的花招但該考慮的都有顧到：我們讓訓練最多跑 100 個 epoch，每一輪都會經過訓練集和驗證集的評估；同時設置了 early stopping 的機制，若模型在驗證集上的表現連續五輪沒有進步，就會自動停下來，避免過擬合或多餘的運算。而這次還加入了 grad_clip=1.0 來限制梯度的最大值，這可以防止訓練過程中出現梯度爆炸的情況。

```
from trainer import Trainer
import torch.optim as optim

optimizer = optim.AdamW(model.parameters(), lr=1e-3)
trainer = Trainer(
    epochs=100,
    train_loader=train_loader,
    valid_loader=valid_loader,
    model=model,
    optimizer=optimizer,
    early_stopping=5,
    load_best_model=True,
    grad_clip=1.0,
)

trainer.train(show_loss=True)
```

輸出結果：

```
Train Epoch 10: 100%|██████████| 374/374 [00:26<00:00, 13.86it/s, loss=0.330]
Valid Epoch 10: 100%|██████████| 94/94 [00:03<00:00, 29.75it/s, loss=1.707]
Train Loss: 0.30791 | Valid Loss: 2.14102 | Best Loss: 2.06168

Train Epoch 11: 100%|██████████| 374/374 [00:26<00:00, 13.91it/s, loss=0.275]
Valid Epoch 11: 100%|██████████| 94/94 [00:03<00:00, 30.11it/s, loss=2.064]
Train Loss: 0.25543 | Valid Loss: 2.14524 | Best Loss: 2.06168

--------------------------------------
| Model can't improve, stop training |
--------------------------------------
```

![Image 1: https://ithelp.ithome.com.tw/upload/images/20250929/2015223636tChPg3L7.png](images/series-8357/day-15/2015223636tChPg3L7-5815d6574952617f.png)

從圖中可以看出，train loss 隨著 epoch 增加穩定下降，表示模型對訓練資料的學習效果不斷提升；但相對地，valid loss 則是在前幾個 epoch 有所下降後便趨於平緩甚至略有上升，最後大致停留在 2.1 附近。

這種現象代表模型雖然學會了訓練資料的特徵，但泛化能力卻沒有同步跟上**是典型的Overfitting**。這些原因有可能是**模型容量過大，相對於資料規模來說太複雜**，而這裡其實最有可能的是**資料本身的多樣性不足，使得模型很快就掌握了可泛化的部分**。

為了解決這個問題我可以試著加一些正規化的方法，讓模型不要太記得訓練資料的細節，也可以用資料增強來讓樣本看起來更有變化或是直接把模型縮小一點。甚至也可以用AI產生一些新的資料，混進去再重新訓練看看。

5. 評估模型效果
---------

當你訓練好一個翻譯模型後，第一個冒出來的問題大概就是：「它翻得好嗎？」這時，`BLEU 分數（Bilingual Evaluation Understudy）` 就是一個常見又方便的自動評估指標，能快速幫你判斷模型在測試資料上的表現。這裡我們會用 `sacrebleu` 套件來幫忙計算：

```
import torch
import sacrebleu

def translate_and_eval(model, tokenizer, loader):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device).eval()

    hyps, refs = [], []
    with torch.no_grad():
        for batch in loader:
            batch = {k:v.to(device) for k,v in batch.items()}
            out = model.generate(input_ids=batch['src_ids'])
            hyps += tokenizer.batch_decode(out, skip_special_tokens=True)
            refs += tokenizer.batch_decode(batch['tgt_ids'], skip_special_tokens=True)

    bleu = sacrebleu.corpus_bleu(hyps, [refs], lowercase=True)
    print(f"Corpus BLEU: {bleu.score:.2f}")

translate_and_eval(model, tgt_tokenizer, valid_loader)
```

輸出結果會長這樣：

```
Corpus BLEU: 23.23
```

從這個分數來看，模型的翻譯表現已經有一定水準。雖然還談不上可以直接應用在真實場景，但至少比亂猜好得多了。而且這次訓練資料其實不多，還能拿到這樣的 BLEU 分數，說真的已經是個不錯的起步點了。

下集預告
----

我們花了大約兩個禮拜的時間，一步步介紹過去那些經典的模型架構。今天終於實作了基於 LSTM 的 Seq2Seq 模型，還加上了 Attention 機制。老實說，這套架構雖然經典又實用，但現在已經慢慢不那麼流行了。原因很簡單**當資料量一大、模型一複雜，LSTM 的訓練效率就會變得很差**。

這是因為它的運算方式只能一個步驟接著一個步驟來，沒辦法平行處理，每個時間點都得等前一個跑完。這樣的特性對 GPU 來說很不友善，訓練速度也自然受限。

所以從接下來的章節開始，我們就要正式進入 Transformer 的世界了。會從它的核心架構講起，一步步帶大家拆解 Self-Attention 的原理和實作，並說明它為什麼這麼快、這麼受歡迎。下一站我們就從 Attention 前進到 Self-Attention，從線性序列的處理邏輯，跨進一口氣可以同時處理整個矩陣的世界！

---

<a id="day-16"></a>

## Day 16｜【Day 16】從零開始拆 Transformer，原來 Encoder 是這樣運作的！
- 原文：https://ithelp.ithome.com.tw/articles/10391136

前言
--

這幾天我會陸續和大家介紹 Transformer 模型的結構細節。老實說這個模型的重要性真的不容小覷，**它幾乎可以說是現在 AI 世界的核心**。不誇張地說只要你搞懂了 Transformer，基本上就掌握了現今大多數主流 AI 模型的運作邏輯。過去那些模型（像是 RNN、LSTM）當然也有它們的貢獻，不過你不需要太執著於它們的細節，因為 Transformer 的出現，某種程度上已經統一了這個領域的主流架構。

所，為了讓大家能更扎實地理解這套系統，我會把整個 Transformer 拆解成幾個章節來慢慢講，每一部分都會盡量用清楚、直白的方式來說明，讓你不用被艱澀的數學或名詞卡住也能理解核心概念。

今天我們就從 Transformer 裡的一個關鍵模組Encoder開始談，可以說Encoder 是整個架構的基石。如果你能搞清楚 Encoder 的邏輯和它是怎麼處理資訊的，後面在看 Decoder 或更複雜的應用（像是 GPT 或 BERT）時就會順很多。所以這篇我會一步步拆解 Encoder 的基本組成、每個模組的功能，還有它們背後的設計理念，希望能幫助你真正理解這個影響深遠的系統到底是怎麼運作的。

Transfomer Encoder
------------------

Transformer 是一種很有意思的深度學習模型架構，核心是所謂的`注意力機制（Attention Mechanism`。這個架構最早是 2017 年由 Vaswani 等人在一篇叫《Attention is All You Need》的論文中提出的。雖然一開始是專門為自然語言處理（像是翻譯、對話生成）設計的，但後來也慢慢被用在其他領域，比如電腦視覺，甚至現在很多最強的 AI 模型，幾乎都是靠這個架構做出來的。

![Image 1: https://ithelp.ithome.com.tw/upload/images/20241004/20152236lsopWP4Zlm.png](images/series-7467/day-20/20152236lsopWP4Zlm-fd548ee1be7970c6.png)

Transformer 最厲害的一點就是它處理「序列資料」的效率特別高，尤其是面對很長的句子或段落時，表現依然穩定。那接下來我們可以先來看看它的 Encoder，也就是整個模型裡負責讀懂輸入資料的部分，到底是怎麼運作的。

Positional Encoding
-------------------

傳統的時間序列模型像是 RNN，本身就具備遞迴結構，所以它會自然而然保留輸入資料的順序，也就是前後文的關聯。但 Transformer 完全不是這樣設計的，它靠的是平行運算，意思就是它在處理輸入時，根本不知道每個字是排在第幾個。因此為了讓 Transformer 也能理解順序，就需要額外的機制來補上這一塊資訊。

因此我們就會需要 `Positional Encoding（位置編碼）`，它的做法是把每個詞在句子裡的位置，用一組數學方式編碼進詞向量裡。這個編碼會跟原本的`embedding`加在一起送進模型。

![Image 2: https://ithelp.ithome.com.tw/upload/images/20241004/20152236Oz6THlEMXd.png](images/series-7467/day-20/20152236Oz6THlEMXd-df46b846785392ba.png)

而這個位置資訊的編碼方式，是靠`正弦（sin`）和`餘弦（cos）`函數來實現的，而其原因很簡單，因為這兩個函數的波動有週期性，可以用來表示變化的節奏，在偶數編號的維度用 sin 函數來表示

![Image 3: https://ithelp.ithome.com.tw/upload/images/20250930/20152236zATbzQhVLf.png](images/series-8357/day-16/20152236zATbzQhVLf-f980e8af8cea8e98.png)

奇數編號的維度用 cos 函數來表示

![Image 4: https://ithelp.ithome.com.tw/upload/images/20250930/20152236bUG2CxJRyK.png](images/series-8357/day-16/20152236bUG2CxJRyK-80aa8f9854bfec05.png)

公式中的 `pos` 表示詞在整個句子中的位置，而 `i` 是詞向量的第幾個維度，`d_model` 是整個詞向量的總維度，其中當 i 越大，分母中的值也會越大，這**會讓 sin/cos 的變化變得比較慢**。這種設計會讓不同的維度以不同的頻率在震盪，進而讓模型能更精細地感受到每個詞在句子中所處的相對位置。

在公式裡那個看起來有點突兀的 10000，其實不是隨便挑的數字，它是一個`縮放因子（scaling factor）`，目的是讓不同維度的變化頻率有所區隔。舉個簡單的比喻你可以把這整個 Positional Encoding 想像成一個「頻率混音器」，每個維度像是一條獨立的聲音軌，頻率高低不同但混在一起可以幫助模型聽出句子中每個詞的位置。

具體來說低維度的變化比較劇烈（頻率高），高維度則變化得比較慢（頻率低），這種設計能讓模型從不同角度感受到詞序的影響，就像同一個場景用廣角與長焦鏡頭各拍一張照片一樣，提供多層次的空間訊息。而在程式碼中我們可以這樣撰寫

```
import torch
import torch.nn as nn
import math

class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()

        # 建立 (max_len, d_model) 大小的位置編碼矩陣
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)  # shape: (max_len, 1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                             (-math.log(10000.0) / d_model))

        # 偶數維度用 sin，奇數維度用 cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # 增加 batch 維度方便加到輸入上
        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)

        # 註冊 buffer，不會更新參數但會一起移到 GPU
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        x shape: (batch_size, seq_len, d_model)
        """
        seq_len = x.size(1)
        # 加上對應長度的 positional encoding
        return x + self.pe[:, :seq_len, :]
```

再來講講 `max_len` 的角色，**這個參數其實是告訴模型你最多會處理多長的句子**。在初始化位置編碼時，我們會先建立一個尺寸為 `(max_len, d_model)` 的矩陣，意思是我先準備好所有從第 0 個詞到第 max_len-1 個詞的所有位置編碼。不管你之後輸入的句子多長，我都有辦法從這個表裡撈出對應的那一段位置資訊來加進去。如果你的模型只會處理短句子，比如最多 128 個 token，那就可以把 `max_len` 設成 128。相對地，如果你在處理長篇文本（像是摘要、小說等），那就要把 max_len 設得大一點，避免在 forward 時出現超出範圍的錯誤。

還有一點比較進階但很重要的`register_buffer`，這不是普通的變數註冊，而**是 PyTorch 中用來儲存不參與訓練但又需要跟著模型移動（像是轉到 GPU 上）」的資料**。也就是說我們並不希望這個位置編碼在訓練過程中被改動，但又不能把它當成一般常數，因為它得跟著模型搬到 CUDA 裡才能正確運作。`register_buffer` 就是解這個問題的標準做法。

Multi-Head Self-Attention
-------------------------

在 Transformer 架構裡最核心的技術就是 `Self-Attention（自注意力機制）`，這個機制有點像是模型在讀一段文字時會自己去看整個句子，判斷哪些詞跟現在這個詞有關係，然後把注意力放在那些比較重要的詞上。也因為這樣，Transformer 才能慢慢取代像 Seq2Seq 那種比較傳統、需要 Encoder 跟 Decoder 不斷互動的架構，變得又快又準。

講到自注意力，會牽扯到三種向量`查詢（Query, Q）`、`鍵（Key, K）`、`值（Value, V）`，每個輸入的詞（也就是 Token）都會被轉換成這三種向量。怎麼轉？其實就是把原本的詞向量（通常是 embedding）乘上三個不同的權重矩陣，分別是`W_Q`、`W_K`、`W_V`，你可以想像成是先經過一層 embedding，再用三個不同的線性層（nn.Linear）做運算。

![Image 5: https://ithelp.ithome.com.tw/upload/images/20241004/20152236gfynWO25qZ.png](images/series-7467/day-20/20152236gfynWO25qZ-24285b89e1f1822c.png)

而在上圖中的動作簡單來說，Q 就是拿來問問題的，K 是拿來比對的，而 V 是答案內容。接下來的流程是這樣：我們會拿查詢向量 Q 去跟所有詞的鍵向量 K 做點積，算出一個數值這個數值代表兩個詞之間的關聯程度，稱為 Attention Score。

![Image 6: https://ithelp.ithome.com.tw/upload/images/20250930/20152236HR0OXV5Zrr.png](images/series-8357/day-16/20152236HR0OXV5Zrr-dd917e5c31e6189f.png)

然後我們會把這些 Score 丟進 Softmax，把它們變成一組機率，這組機率就是所謂的 `Attention Weights`，也就是我現在該多關注哪個詞的分數。![Image 7: https://ithelp.ithome.com.tw/upload/images/20250930/20152236B40ltCePbH.png](images/series-8357/day-16/20152236B40ltCePbH-ce2f6f2bb37c081a.png)

最後我們用這些機率去加權每個詞的值向量 V，加總後就得到這次注意力機制的輸出。

![Image 8: https://ithelp.ithome.com.tw/upload/images/20250930/20152236IAquFJaBOf.png](images/series-8357/day-16/20152236IAquFJaBOf-b665018cc2b9f62e.png)

其中 `√𝑑` 這是 Q 和 K 向量的維度大小開根號(也就是詞向量空間)。為什麼要除這個？因為當向量維度太高時，Q 和 K 的點積結果可能會變得非常大，導致 Softmax 結果變得很極端，模型就學不好了。所以我們會用這個值來做縮放，把結果拉回合理的範圍。

![Image 9: https://ithelp.ithome.com.tw/upload/images/20241004/20152236wWAreaIFOw.png](images/series-7467/day-20/20152236wWAreaIFOw-59c6e36be9c7215b.png)

講到這裡其實 Transformer 真正厲害的地方在於它用的是`Multi-Head Self-Attention（多頭自注意力機制）`，不是只有一個頭在做注意力運算，而是會把 Q、K、V 拆成好幾組，每組都各自計算注意力，最後再把這些結果合起來，也就是以下的公式

![Image 10: https://ithelp.ithome.com.tw/upload/images/20241004/20152236x6yM0z6b9V.png](images/series-7467/day-20/20152236x6yM0z6b9V-654acf8ae2f0ef50.png)

每個 attention head 本質上就是一次 `Attention(Q, K, V)` 的運算，而為什麼要用多個 head 呢？簡單來說，這樣做的好處是，每個頭可以專心」輸入句子的不同面向。像是有的 head 可能比較關注句子的語法結構，有的可能在抓語氣或情緒，還有的也許專注在主題相關的詞，這種設計讓模型能從多個角度來理解整個句子。

不過這邊有個小細節要注意，我們在計算 Attention 分數的時候有一個除以 `√d` 的操作，這個 `d`代表的是每個 head 的向量維度，既然我們把整體的 `d_model` 切成多個 head，那每個 head 的向量空間就要平均分配，這樣 `scale` 才會算得對。所以在程式碼裡，可以看到我們是把整個詞向量的維度平均分成多份：

```
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, nhead, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % nhead == 0
        
        self.d_model = d_model
        self.nhead = nhead
        self.d_k = d_model // nhead
        
        self.w_q = nn.Linear(d_model, d_model)
        self.w_k = nn.Linear(d_model, d_model)
        self.w_v = nn.Linear(d_model, d_model)
        self.w_o = nn.Linear(d_model, d_model)
        
        self.dropout = nn.Dropout(dropout)
        self.scale = math.sqrt(self.d_k)
```

而在前向傳播時這邊不使用 `torch.cat` 來合併，而是用了 `view + transpose`，目的是把整個向量拆成多個子空間，好讓每個 head 處理屬於自己的那一份資料。如果使用`cat`會需要使用迴圈，而選擇 `view` 和 `transpose` 只是比較快的做法。

```
def forward(self, query, key, value, mask=None, key_padding_mask=None):
        batch_size = query.size(0)
        seq_len_q = query.size(1)
        seq_len_k = key.size(1)
        
        # 將 Q/K/V 映射後 reshape 成 (batch_size, nhead, seq_len, d_k)
        Q = self.w_q(query).view(batch_size, seq_len_q, self.nhead, self.d_k).transpose(1, 2)
        K = self.w_k(key).view(batch_size, seq_len_k, self.nhead, self.d_k).transpose(1, 2)
        V = self.w_v(value).view(batch_size, seq_len_k, self.nhead, self.d_k).transpose(1, 2)
        
        # 計算注意力分數
        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale
        
        # 套用 attention mask
        if mask is not None:
            # 確保 mask 的形狀正確
            if mask.dim() == 2:
                mask = mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, seq_len)
            elif mask.dim() == 3:
                mask = mask.unsqueeze(1)  # (batch_size, 1, seq_len, seq_len)
            scores = scores.masked_fill(mask, float('-inf'))
        
        # 套用 padding mask
        if key_padding_mask is not None:
            # key_padding_mask: (batch_size, seq_len_k)
            # 需要擴展為 (batch_size, 1, 1, seq_len_k)
            key_padding_mask = key_padding_mask.unsqueeze(1).unsqueeze(2)
            scores = scores.masked_fill(key_padding_mask, float('-inf'))
        
        # 計算注意力權重
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # 計算 weighted sum 後 reshape 回原始形狀
        context = torch.matmul(attn_weights, V)
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len_q, self.d_model
        )
        
        # 最終輸出線性變換
        output = self.w_o(context)
        return output
```

而在 Attention 分數計算中，我們其實是把每個詞對其他所有詞都看一遍，但我們不希望模型「偷看」，這時就要用到 mask，舉個例子：

*   在語言模型裡，我們不希望模型在預測第 t 個詞的時候，看到第 t+1、t+2 的詞，所以要遮住未來。
*   在處理不同長度句子的時候，為了讓它們長度一致，我們會加上 `<pad>` token，但這些其實沒有意義，也要遮起來，不然模型可能會把注意力浪費在這些 padding 上。

這兩種情況分別會用到：

*   **Attention Mask**：避免看到未來的詞。
*   **Key Padding Mask**：忽略 padding 的位置。

遮的方式其實也不難，就是把不想看的地方換成 `-inf`，這樣在做 softmax 的時候，那些位置就會變成 0，模型自然就不會理它們。

FeedForward
-----------

當我們在講 Transformer 的架構時，除了大家常提到的 Attention，其實還有一個很重要但常被忽略的部分`前饋神經網路（FeedForward Network,  FFN）`。

這一層的設計其實不複雜，就像是兩個線性層夾一個非線性函數，你可以把它想像成每個詞在經過 Attention 和其他詞聊天溝通後，還需要回過頭來自己想一想，把剛剛收到的資訊消化一下、重新組織，提煉出更有代表性的內部特徵。

```
import torch
import torch.nn as nn
import torch.nn.functional as F

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1, activation='gelu'):
        super(FeedForward, self).__init__()
        self.linear1 = nn.Linear(d_model, d_ff)
        self.linear2 = nn.Linear(d_ff, d_model)
        self.dropout = nn.Dropout(dropout)
        
        if activation == 'relu':
            self.activation = F.relu
        elif activation == 'gelu':
            self.activation = F.gelu
        else:
            raise ValueError('Unsupported activation function: {}'.format(activation))
    
    def forward(self, x):
        return self.linear2(self.dropout(self.activation(self.linear1(x))))
```

簡單來說，它就是：

1.   把輸入丟進第一層線性轉換（通常維度會變大）；
2.   接個非線性函數讓資料「彎一下」；
3.   dropout 避免 overfitting；
4.   再轉回原本的維度。

那為什麼需要這一層？因為 Attention 在處理的是詞與詞之間的「關係」，比如說「你說了什麼、我怎麼回應」這種互動。而 FFN 則是讓每個詞有自己的「內心戲」，可以獨立思考、加工資訊，強化自己的表示能力。

這種搭配其實就像是你開完會（Attention），回到座位還是要自己整理筆記、做功課（FFN），這樣整體的表現才會更強。

Layer Normalization
-------------------

另一個值得注意的地方是，在 Transformer 裡面引入了所謂的 `Layer Normalization`。簡單講它的作用就是幫每一層的輸入做個標準化處理，讓輸出的結果更穩定，這樣整個模型在訓練時就比較容易收斂，也能跑得更快、更穩定。雖然它聽起來有點像 Batch Normalization，但其實兩者做法不太一樣 `Layer Norm` 是針對每個樣本自己做正規化，而不是像 Batch Norm 那樣，是一整批資料一起處理。

![Image 11: https://ithelp.ithome.com.tw/upload/images/20241004/20152236z6j2q0LooM.png](images/series-7467/day-20/20152236z6j2q0LooM-66142494d478e2ce.png)

你可能有注意到公式裡會出現個 `ε`這個小符號，它的主要功能其實就是防止在運算過程中除以零這種尷尬情況發生。所以它通常會被設得非常小，基本上就是個保險機制，確保計算的穩定性。而 `γ`則是控制輸出要放大多少的參數，它其實會在每一層都被調整，讓模型可以學會不同特徵的重要性。至於 `β`，它的角色是偏移量，讓模型可以微調輸出的整體分佈，讓結果更貼近真實數據的樣貌。

Encoder Skip Connection
-----------------------

現在我們來把這些零件組起來看一下。你會發現在 Encoder 的程式碼裡有兩次出現 `output + layer['dropout'](src2)` 這樣的寫法，這其實就是大家常提到的 `Skip Connection（跳躍連接）`，也有人叫它 `Residual Connection（殘差連接）`。

那這東西到底有什麼用？你可以想像一下，神經網路一層一層往下走，訊息每經過一層就會被改寫一次，但有時候改著改著，原本的重要訊息可能就不見了或者變得模糊了。Skip Connection 的概念就是不要把原始訊息整個丟掉，讓它繞個小路走旁邊，再回來跟處理後的結果合在一起，這樣做有兩個好處：

*   **幫助梯度流動**：尤其是網路層很多的時候，這種跳躍連接可以減少梯度消失的問題，讓整個模型比較好訓練。
*   **保留原始資訊**：讓後面的層還能看到一點原本輸入的樣子，不會完全被加工得面目全非。

所以Encoder 大致上會長這樣：

```
class Encoder(nn.Module):
    def __init__(self, d_model, nhead, d_ff, num_layers, dropout=0.1, norm=None):
        super(Encoder, self).__init__()
        self.layers = nn.ModuleList([
            nn.ModuleDict({
                'self_attn': MultiHeadAttention(d_model, nhead, dropout),
                'feed_forward': FeedForward(d_model, d_ff, dropout),
                'norm1': nn.LayerNorm(d_model),
                'norm2': nn.LayerNorm(d_model),
                'dropout': nn.Dropout(dropout)
            }) for _ in range(num_layers)
        ])
        self.num_layers = num_layers
        self.norm = norm

    def forward(self, src, mask=None, src_key_padding_mask=None):
        output = src
        for layer in self.layers:
            # 自注意力 + Skip Connection + LayerNorm
            src2 = layer['self_attn'](output, output, output, mask, src_key_padding_mask)
            output = layer['norm1'](output + layer['dropout'](src2))

            # 前饋網路 + Skip Connection + LayerNorm
            src2 = layer['feed_forward'](output)
            output = layer['norm2'](output + layer['dropout'](src2))

        if self.norm is not None:
            output = self.norm(output)
        return output
```

整個流程就是這樣`src2` 是注意力學出來的新訊息，而 `output` 是進來這一層的輸入，兩個加在一起，再丟進 LayerNorm，這樣就可以把新的資訊跟原本的訊息自然地融合起來。這個設計真的很關鍵，可以說是 Transformer 成功的秘密武器之一，因為這樣不只穩定訓練，還不太容易出現梯度爆炸或訓練發散的問題。

下集預告
----

明天我們會來聊聊 Transformer Encoder的經典代表`BERT`，這個模型從推出以來一直是自然語言處理領域的主力選手，至今仍被廣泛應用在各種任務上。明天會聚焦在它的整體架構、與原始 Transformer 的差異，以及它為什麼能夠這麼強大。而明天不會有數學而是基於今天內容的模型講解，目標是讓你真正理解 BERT 究竟做對了什麼，才讓它能紅這麼久、用得這麼廣。

---

<a id="day-17"></a>

## Day 17｜【Day 17】只懂 Wx + b 也能搞懂 BERT？當然可以！
- 原文：https://ithelp.ithome.com.tw/articles/10391682

前言
--

在深度學習的世界裡，從頭開始訓練一個模型，不只費時，還非常燒資源。更不用說為了讓訓練有效果，還得準備大量資料，這正是許多人卡關的地方。畢竟資料不是想收就收得到的。這時候一個很聰明的策略就派上用場了：**如果已經有一個表現不錯的模型，能不能稍微改一下，讓它去處理我們的新任務？**

當然有這個方法，而這就是所謂的`遷移式學習（Transfer Learning）`，所以今天的內容就會帶大家從最基本的 Wx + b 開始，一步步走到如何建立一個完整的 BERT 預訓練模型。

遷移式學習跟預訓練模型是什麼？
---------------

講白一點就是把一個已經有訓練過的模型拿來做別的事，尤其是你手上的資料不多的時候特別好用。這不只可以省下一堆時間，效果通常還比你自己從頭訓練來得更穩。而像 BERT 這種模型，就是所謂的`預訓練模型（Pre-trained Model）`。這類型的模型在訓練時不是只學某一種任務，而是什麼都學一點、學得廣。它可以拿來做翻譯、摘要、甚至是文本生成，因為它本身在超大量的資料上訓練過，對各種語言特徵都有概念。

可以把它想像成一個很博學的人，雖然不是每一科都超強，但什麼都懂一點。你只要教它一點點新的東西，它就能舉一反三。這也是為什麼現在很多研究機構或公司會直接用這些預訓練模型，不用自己從零開始練一個，省時又省力。

![Image 1: 圖示：預訓練流程](images/series-7467/day-22/201522360i8OKyRLyF-c542159cb2ea4c2a.png)

整個預訓練模型的使用流程，通常會分成預訓練跟`微調（fine-tuning）`兩個階段。預訓練這階段基本上都是大公司或研究機構在使用，因為他們會設計一個大型的模型架構，丟進超大量的資料裡面去訓練讓模型學會不同資料的特徵。接下來就是微調也就是我們這些一般使用者的重點，**我們拿一小筆資料集，針對某個任務去調整這個預訓練模型**，因為大部分的知識模型早就學好了，所以我們只需要動一些權重，讓它配合我們的任務就可以。

> 而在這裡模型的主體部分通常是共用權重的，而在後續的模型部分通常會不使用或不公開，而我們只需要自己加入一層分類器，讓它重新學習分類新資料，這樣效果會比較好。

不過預訓練模型的架構通常是固定好的，我們想改會有點麻煩。而且它原本訓練的資料，可能跟我們的任務不完全一樣。舉例來說如果一個模型根本沒學過怎麼做摘要，我們卻硬要拿它來做，那效果可能就不怎麼樣。所以我們在用之前，最好還是去看看它的架構是怎麼設計的、相關的論文怎麼說，這樣比較能掌握它的優缺點。

`BERT(Bidirectional Encoder Representations from Transformers)`是2018年由Google提出的，其模型參數設計與原始的Transformer模型並未有太多的改動，而最大的改動是它只保留了Transformer的Encoder部分與其特殊的預訓練方式，而在開始之前我們先深度理解一下BERT的模型架構

Embedding
---------

昨天我們有提到，Transformer 會用一種 Positional Encoding 的方式來處理輸入的資料，讓模型知道每個詞出現的位置，不過那個位置資訊是固定的，也就是模型本身不會去改它。但在 BERT 裡，位置的資訊是可以學習的他不只是吃進去詞語的內容，還會自己學會每個詞出現在句子中不同位置時，應該要有什麼樣的特徵。

![Image 2: https://ithelp.ithome.com.tw/upload/images/20251001/20152236toL31h5Kke.png](images/series-8357/day-17/20152236toL31h5Kke-6a92f8a19754689b.png)

而 BERT 會用三種embedding來把句子轉換成數值資料給模型讀：

1.   **Token Embedding**：就是把每個字或詞轉成對應的向量，跟我們之前學詞向量的概念差不多。
2.   **Segment Embedding**：因為 BERT 一次可以處理兩個句子，所以會加個編號標記這個字是來自第一句還是第二句。
3.   **Position Embedding**：這就是用來表示每個詞在句子中的「位置」。

這三個 embedding 加起來之後，還會再經過一層正規化（LayerNorm）和 dropout 處理，才會送進 BERT 模型的下一層。因此在 Huggingface 的 BERT 架構中，你會看到類似這樣的設計：

```
(embeddings): BertEmbeddings(
    (word_embeddings): Embedding(30522, 768, padding_idx=0)
    (position_embeddings): Embedding(512, 768)
    (token_type_embeddings): Embedding(2, 768)
    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
    (dropout): Dropout(p=0.1, inplace=False)
)
```

如果我們要自己寫一個 `BertEmbeddings` 類別的話，就得依照它需要的參數大小來設定。不過在這裡我們是透過 `config` 的方式來設定這些參數。這麼做的好處是因為 BERT 有很多不同版本的模型，我們就可以根據所選的版本，快速載入對應的權重，不用每次都手動調整。

```
class BertEmbeddings(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
        self.token_type_embeddings = nn.Embedding(getattr(config, "type_vocab_size", 2), config.hidden_size)
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
```

在實作 `forward` 的時候，有兩點要特別留意。第一是 `token_type_ids` 這個參數有可能沒被傳進來，這時候我們就要預設它的值全部是 0。第二是 `position_ids`，這部分要根據實際輸入的長度來動態產生，例如如果輸入是 10 個字，那位置編號就會是 `[0, 1, ..., 9]`。這樣才能確保每個 token 都有正確的位置資訊。接下來就是整個 `forward` 方法的完整寫法。

```
def forward(self, input_ids, token_type_ids=None):
    B, T = input_ids.size()
    if token_type_ids is None:
        token_type_ids = torch.zeros_like(input_ids)

    position_ids = torch.arange(T, device=input_ids.device, dtype=torch.long).unsqueeze(0).expand(B, T)

    w = self.word_embeddings(input_ids)
    p = self.position_embeddings(position_ids)
    t = self.token_type_embeddings(token_type_ids)

    x = w + p + t
    x = self.LayerNorm(x)
    x = self.dropout(x)
    return x
```

BERT Encoder
------------

我們現在看到的是 BertEncoder 的整體架構，**它的核心就是一個有 12 層的 Transformer Encoder堆疊**，每一層就是一個 BertLayer。乍看之下可能有點可怕，但其實可以拆解成幾個重複的小模組，而且這些模組大多就是 Transformer 裡的經典元件。

```
(encoder): BertEncoder(
    (layer): ModuleList(
      (0-11): 12 x BertLayer(
        (attention): BertAttention(
          (self): BertSelfAttention(
            (query): Linear(in_features=768, out_features=768, bias=True)
            (key): Linear(in_features=768, out_features=768, bias=True)
            (value): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (output): BertSelfOutput(
            (dense): Linear(in_features=768, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          )
        )
        (intermediate): BertIntermediate(
          (dense): Linear(in_features=768, out_features=3072, bias=True)
          (intermediate_act_fn): GELU(approximate='none')
        )
        (output): BertOutput(
          (dense): Linear(in_features=3072, out_features=768, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
        )
      )
    )
  )
```

### 1. BertSelfAttention

這個模組負責看哪裡重要，也就是我們常說的 Attention 機制。它裡面有三個 Linear layer，分別做出 Query, Key, Value，這跟我們昨天提到的 Self-Attention 概念完全一樣。

它會把這三個東西 reshape 成Multi-head Attention 需要的格式。接著就是計算 Attention Score，並經過 Softmax，再加上一點 Dropout處理Attention Weight，最後會把算出來的Attention Weight跟 Value 做乘法，得到我們的Attention結果。

```
class BertSelfAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0:
            raise ValueError("hidden_size must be divisible by num_attention_heads")
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = config.hidden_size // config.num_attention_heads
        self.all_head_size = self.num_attention_heads * self.attention_head_size

        self.query = nn.Linear(config.hidden_size, self.all_head_size)
        self.key = nn.Linear(config.hidden_size, self.all_head_size)
        self.value = nn.Linear(config.hidden_size, self.all_head_size)

        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)

    def transpose_for_scores(self, x):
        new_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)
        x = x.view(*new_shape)  # [B, T, nh, hd]
        return x.permute(0, 2, 1, 3)  # [B, nh, T, hd]

    def forward(self, hidden_states, attention_mask=None):
        q = self.query(hidden_states)
        k = self.key(hidden_states)
        v = self.value(hidden_states)

        q = self.transpose_for_scores(q)
        k = self.transpose_for_scores(k)
        v = self.transpose_for_scores(v)

        attn_scores = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(self.attention_head_size)  # [B, nh, T, T]
        if attention_mask is not None:
            attn_scores = attn_scores + attention_mask

        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.dropout(attn_probs)

        context = torch.matmul(attn_probs, v)  # [B, nh, T, hd]
        context = context.permute(0, 2, 1, 3).contiguous()
        new_context_shape = context.size()[:2] + (self.all_head_size,)
        context = context.view(*new_context_shape)  # [B, T, H]
        return context
```

### 2. BertSelfOutput：Add & Norm 區塊

其實在 BertSelfOutput 裡，有個很關鍵的步驟，就是 Transformer 裡常見的 Add & Norm。簡單來說，它就是先把 Attention 的輸出再經過一層 Linear，然後加上 Dropout，接著再把這個結果跟原本進來的輸入做個 skip connection，最後再做 Layer Normalization。這樣做的目的是為了讓訓練過程更穩定。

```
class BertSelfOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        # name: attention.output.dense
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        # name: attention.output.LayerNorm
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor) # Skip Connection
        return hidden_states
```

### 3. BertIntermediate與BertOutput(FFN)

每層 Transformer 結尾的一個重要部分，主要是用來進一步轉換和處理前面得到的資訊。它被拆成兩個部分來進行，首先是 BertIntermediate，這裡會先用一層 Linear 把原本的 768 維向量放大成 3072 維，接著通過一個GELU來處理它的線性變換(這裡也是唯一跟Transformer不同的地方)。

```
class BertIntermediate(nn.Module):
    def __init__(self, config):
        super().__init__()
        # name: intermediate.dense
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        self.intermediate_act_fn = nn.GELU()

    def forward(self, hidden_states):
        return self.intermediate_act_fn(self.dense(hidden_states))
```

然後是 BertOutput，它再把剛剛拉高的 3072 維壓回到原來的 768 維。最後跟之前 Attention Output 的流程很像先做 Dropout，再加上原始輸入的 Skip Connection，然後做 Layer Normalization，這整個流程有助於模型更穩定地學習。

```
class BertOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        # name: output.dense
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        # name: output.LayerNorm
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.LayerNorm(hidden_states + input_tensor) # Skip Connection
        return hidden_states
```

### 4. BERT Encoder

你會發現整個 BERT Encoder 的定義基本上與我們的Transformer根本沒有差異，而現在我們只需要建立BertLayer與BertEncoder就能完成整個模型的建立了，而這樣的設計只是為了方便動態調動多層Transformer

```
class BertLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.attention = BertAttention(config)
        self.intermediate = BertIntermediate(config)
        self.output = BertOutput(config)

    def forward(self, hidden_states, attention_mask=None):
        attention_output = self.attention(hidden_states, attention_mask)
        intermediate_output = self.intermediate(attention_output)
        layer_output = self.output(intermediate_output, attention_output)
        return layer_output
```

而我們經過前面的建立現在就能夠只使用簡單幾行就完成 Transformer Layer 的精髓了，雖然你在這邊沒看到像昨天的明顯加法或 LayerNorm，這是因為再HF架構中的BERT把skip connection 都藏在 Attention 跟 Output 裡面(我上面程式碼註解的地方)。

而`BertEncoder`則是進行實際堆疊的地方，不過在HF風格上通常會有一個 `output_hidden_states=True`，它也會幫你把每一層的輸出都存下來，這對於分析模型或做視覺化非常有用。

```
class BertEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        # name path: encoder.layer.0 ... encoder.layer.N
        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])

    def forward(self, hidden_states, attention_mask=None, output_hidden_states=False):
        all_hidden_states = [] if output_hidden_states else None
        for layer_module in self.layer:
            if output_hidden_states:
                all_hidden_states.append(hidden_states)
            hidden_states = layer_module(hidden_states, attention_mask)
        if output_hidden_states:
            all_hidden_states.append(hidden_states)
        return hidden_states, all_hidden_states
```

BERT Encoder 看起來很複雜，但其實就是 12 層完全一樣的 Transformer 結構堆起來。每層做注意力 → 前饋網路 → skip，加起來就可以學到上下文關係。

BertPooler
----------

當我們把一句話丟進 BERT 模型裡，它其實不會馬上就開始「理解」文字，而是會先幫句子加上一些特別的符號，例如 `[CLS]` 和 `[SEP]`，簡單來說：

```
單一句子： [CLS] 句子 [SEP]  
兩句話：   [CLS] 句子A [SEP] 句子B [SEP]
```

而`[SEP]` 是用來分隔句子的，像是句子中間的逗號，也順便當作句尾。那開頭的 `[CLS]` 呢？這就比較特別了。雖然一開始它只是個空白 token，沒什麼意思，但 BERT 會訓練它變成整句話的代表，**就像是一個總結整句意思的代言人。**

為什麼它可以代表整句話？這就要靠 Transformer 裡厲害的東西**Self-Attention 機制**。它的概念有點像是每個詞都會去注意整句話裡其他詞，看看彼此的關聯性。就算詞在句首或句尾，都會把整句的資訊融合進來，只是每個詞吸收的重點可能不同。

數學上在做什麼？其實就是幫每個詞算出對 [CLS] 來說，它有多重要，也就是`softmax((Q · K.T) / sqrt(d))`，而些注意力分數會用來加權每個詞的 Value（V），再全部加總起來，因此CLS 就像是一個訊息總管，根據自己的關注程度去吸收其他詞的資訊，最後變成它的新向量。

因此到這時候就輪到 `BertPooler` 出場了雖然模型裡寫得很簡單：

```
(pooler): BertPooler(
    (dense): Linear(in_features=768, out_features=768, bias=True)
    (activation): Tanh()
)
```

但重點是它只拿 `[CLS]` 的向量來用，也就是在程式裡會看到`hidden_states[:, 0]`意思就是只抓第一個 token，也就是 `[CLS]` 的位置。這個 `[CLS]` 的向量會先丟進一個 Linear 層做轉換，再經過一個 Tanh 函數做激活，讓輸出的值被限制在 -1 到 1 之間，讓後面的模型更好的進行分類或運算。

```
class BertPooler(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.activation = nn.Tanh()

    def forward(self, hidden_states):
        first_token_tensor = hidden_states[:, 0]
        pooled_output = self.dense(first_token_tensor)
        pooled_output = self.activation(pooled_output)
        return pooled_output
```

講到這邊，其實我們已經把 BERT 的整體模型架構講得差不多了。不過要注意這還不是全部！我們現在談的主要是 模型的底層架構，也就是 BERT 怎麼處理文字、怎麼用 Self-Attention、怎麼透過 [CLS] 來代表整句話，但實際上，BERT 在做預訓練時，還會在這個基礎上加上一些額外的分類器。

這些分類器有點像是訓練小助手，專門幫助模型學會更準確地用 [CLS] 去做各種任務，而使用這些訓練技巧，會讓 [CLS] 的表示學得更有意義，也就是我們俗稱的「讓它更懂句子」。不過這部分就留到明天再說吧，今天先消化一下這些架構和機制，不然一次塞太多，頭腦真的會轉不過來。

下集預告
----

今天我們介紹了 [https://huggingface.co/google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased) 這個模型架構，順便用程式碼來幫助你更清楚理解 BERT 的原理。這個架構其實你也可以直接套用我貼的連結裡的預訓練權重，不過這部分我們會留到後面再詳細說明。

明天我們會進一步聊聊**BERT 是怎麼被訓練出來的？它到底學了哪些東西？** 還有在訓練過程中加入的一些小技巧，像是 MLM（Masked Language Modeling）和 NSP（Next Sentence Prediction），這些又是怎麼幫助 BERT 更懂語言的？

---

<a id="day-18"></a>

## Day 18｜【Day 18】一篇文章讓你搞懂BERT預訓練任務與模型實作（MLM + NSP）
- 原文：https://ithelp.ithome.com.tw/articles/10391896

前言
--

在自然語言處理的世界裡BERT 可說是近十年來最具代表性的模型之一，它不僅改寫了多項語言任務的表現標準，更奠定了後續各類 Transformer 模型的核心架構。不過儘管許多開發者早已習慣透過 Hugging Face 等工具輕鬆調用 BERT，我們今天要做的則是往原理層更進一步。

今天將帶你**一步步手動實作 BERT 的預訓練架構**，包括 `MLM（Masked Language Modeling）`與 `NSP（Next Sentence Prediction）`兩大訓練任務，並將昨天的`BertModel`與今天的預訓練head進行官方模型的權重對齊。

BERT 的預訓練階段以 `MLM（Masked Language Model）` 任務為核心，這是一種讓模型透過**遮蔽部分詞彙**來學習語境理解的策略。他在處理輸入文本時大約 **15% 的 token** 會被選中進行遮蔽處理，但這個遮蔽並非單一操作

*   多數會被替換為 `[MASK]` 標記
*   一小部分會被替換成 **隨機的 token**
*   還有些則會 **保留原詞**

這樣的設計表面上看似無意義，但實際上是一種有意識的安排，目的在於讓模型自訓練初期便能接觸多樣且貼近真實使用情境的語境變化，而非僅在理想化的條件下學習。

![Image 1: https://ithelp.ithome.com.tw/upload/images/20241007/20152236gSwoL179O3.png](images/series-7467/day-23/20152236gSwoL179O3-ca2abc56cd76665c.png)

在實際應用中使用者所輸入的語句並不會包含 `[MASK]`這類特殊標記。如果模型過度依賴明確的遮蔽提示來進行預測，則在面對完全不具備此類提示的任務時，其推理能力與表現可能會明顯下降。基於此考量，訓練設計中刻意引入隨機詞替換與保留原詞的機制，目的是使模型逐步習慣於在缺乏遮蔽提示的情況下，依然能夠理解語句的語意邏輯，並在此基礎上自主地進行語境推理。

其實這種 MASK 的設計還有一個蠻大的優點。傳統的語言模型像是那種從左讀到右的 `RNN`，它們只能依靠前面出現的字來預測下一個詞，這樣在理解整個語境的時候就有點受限了。相對來說，BERT 用的是 `Transformer` 結構，裡面有個叫做 `Self-Attention` 的機制，這讓它可以做到**真正的雙向語境理解**。

簡單來說，BERT 在預測被遮起來的字時，不只看前面的詞，連後面的也一起考慮。像我們昨天提到的例子，今天如果輸入這句話：

```
我今天吃了 [MASK]，很好吃。
```

BERT 的處理方式會先加上一個 `[CLS]` 的 token，用來做**整句話的語意摘要**。然後在訓練的時候，它不是只根據「我今天吃了」這段來猜，而是也會看「很好吃」這個線索。這樣一來，它就更有可能猜出像「壽司」或「牛肉麵」這種合理的食物名詞。

NSP任務
-----

除了預測被遮住的token之外BERT 還有另一個蠻關鍵的訓練方法，叫做 `NSP（Next Sentence Prediction）`，意思是「下一句預測」。那這個任務到底是幹嘛用的呢？簡單來說就是要讓模型能夠理解「句子跟句子之間到底有沒有邏輯關係」。這在什麼情況下特別有用？比如說問答系統、文章閱讀理解，甚至是對話生成。這些任務不只是單句理解而已，而是要搞懂句子跟句子之間是不是一脈相承、有沒有邏輯順序。

那 NSP 是怎麼訓練的？在 BERT 的預訓練階段，會拿一對句子給模型判斷，其中有一半是真的連在一起的，比如：

```
A：我今天去了圖書館。
B：我借了一本關於機器學習的書。
```

這樣的句子對是正確連續的，而另一半則是硬湊的、不相干的句子比如：

```
A：我今天去了圖書館。
B：香蕉是黃色的。
```

這種就完全沒關係，屬於隨機組合。

BERT 在訓練時的任務，就是要學會判斷 B 句到底是不是合理地接在 A 句後面。也就是說，它不只是看單句內容，還要考慮整體語意的銜接。更有意思的是**NSP 這個訓練任務也會在某種程度上加強 BERT 做 MLM 的能力**。因為為了判斷兩個句子是不是相關的，模型必須更深入地理解語境，甚至得學會抓出潛在的語意線索，這對整體語言理解是很有幫助的。

程式實現
----

今天的重點，是透過實作方式來更直觀地理解 BERT 模型的架構。不過在動手編碼之前，我們先快速回顧一下昨天寫過的程式碼，並進一步結合 Hugging Face（HF）上的預訓練模型，來做權重的轉移。這樣的操作不只是為了好玩，而是幫助我們確保自己手動實作的模型，能夠與 HF 官方版本在結構與參數上完全對齊。

在Hugging Face 提供的 `bert-base-uncased` 模型中，內建了一個 `config` 檔案，其中包含了模型架構的關鍵設定，例如 Transformer 的層數、每層的神經元維度、hidden size 等。所以等等我們會直接利用這個 `config` 來初始化我們自己的 `BertModel` 類別。

1. 轉移 BERT 模型權重
---------------

在進行權重轉移時，第一步是從 Hugging Face 的 `bert-base-uncased` 模型讀取已訓練好的參數，並將這些參數載入到我們自己撰寫的 `BertModel` 類別中。雖然這樣的模型在使用上與直接呼叫 HF 提供的模型沒有功能差異，但重點在於理解與驗證：我們是否成功複製出與官方版本完全一致的結構。這對於未來想要修改模型（例如自訂 Attention 機制或更動 Pooler 結構）特別有幫助。

相較於直接調整 HF 封裝過的模型，我們自己實作一份會更直觀、自由度也更高。當然如果你已經非常熟悉 PyTorch，也可以透過 `hook` 的方式來動態改變模型的前向傳播邏輯。

```
from transformers import BertModel as HFBertModel

# 從 Hugging Face 載入 BERT encoder
hf_encoder = HFBertModel.from_pretrained("bert-base-uncased")
```

2. 自訂 BertModel 類別
------------------

而我們現在把昨天的組件組合成一個自己定義的 `BertModel` 類別，這裡一定要讓參數名稱與 HF 模型對齊，這是成功載入權重的關鍵。只要有任何一個參數名稱不一致，`.load_state_dict()` 可能就會報錯。

```
class BertModel(nn.Module):
    """
    State dict keys match Hugging Face's `BertModel`.
    Accepts an HF BertConfig directly. No local Config duplication.
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        # names: embeddings, encoder, pooler
        self.embeddings = BertEmbeddings(config)
        self.encoder = BertEncoder(config)
        self.pooler = BertPooler(config)

        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)
        elif isinstance(module, nn.LayerNorm):
            nn.init.ones_(module.weight)
            nn.init.zeros_(module.bias)

    @staticmethod
    def _extend_attention_mask(attention_mask, dtype):
        """
        Input mask: [B, T] with 1 for tokens to keep, 0 for padding
        Output mask: [B, 1, 1, T] with 0 for keep and -inf for mask (same dtype as scores)
        """
        if attention_mask.dim() == 2:
            extended = attention_mask[:, None, None, :]
        elif attention_mask.dim() == 3:
            extended = attention_mask[:, None, :, :]
        else:
            extended = attention_mask
        extended = extended.to(dtype=dtype)
        # 1 -> 0.0, 0 -> -inf
        neg_inf = torch.finfo(dtype).min
        extended = (1.0 - extended) * neg_inf
        return extended

    def forward(
        self,
        input_ids,
        token_type_ids=None,
        attention_mask=None,
        output_hidden_states=False,
        return_dict=False,
    ):
        if token_type_ids is None:
            token_type_ids = torch.zeros_like(input_ids)

        if attention_mask is None:
            attention_mask = (input_ids != self.config.pad_token_id).long()

        embedding_output = self.embeddings(input_ids, token_type_ids)
        # build extended mask in the same dtype as attention scores (float)
        extended_attention_mask = self._extend_attention_mask(attention_mask, embedding_output.dtype)

        sequence_output, all_hidden_states = self.encoder(
            embedding_output, attention_mask=extended_attention_mask, output_hidden_states=output_hidden_states
        )
        pooled_output = self.pooler(sequence_output)

        if return_dict:
            return {
                "last_hidden_state": sequence_output,
                "pooler_output": pooled_output,
                "hidden_states": all_hidden_states,
            }
        return (sequence_output, pooled_output, all_hidden_states if output_hidden_states else None)
```

在這段程式中使用了我們昨日實作的三大模組，並補上了完整的前向傳播邏輯。特別注意我們對 `attention_mask` 和 `token_type_ids` 的處理方式是模仿 Hugging Face 的內部作法，確保計算方式完全一致。

3. 載入 Hugging Face 的權重
----------------------

接下來我們建立模型實例，並載入 HF 模型的 `state_dict`，並檢查有哪些參數對不上。

```
model_encoder_only = BertModel(hf_encoder.config)
sd_encoder = hf_encoder.state_dict()
missing_e, unexpected_e = model_encoder_only.load_state_dict(sd_encoder, strict=False)
print("[Encoder] Missing:", missing_e)
print("[Encoder] Unexpected:", unexpected_e)
```

理想情況下，輸出應該是這樣：

```
[Encoder] Missing: []
[Encoder] Unexpected: []
```

這表示我們的模型與 HF 提供的 Encoder 架構與參數名稱完全一致，沒有遺漏任何參數，也沒有多餘的設定。這個步驟除了驗證模型正確性之外，也為未來的模型微調與客製化打下基礎。

4. 建立BertPreTrainingHeads
-------------------------

接下來我們要談的是 BERT 在預訓練任務（如 MLM 和 NSP）中所使用的架構擴充，具體來說，Hugging Face 中的 BertPreTrainingHeads 模組會在原始的 BertModel 上疊加一層額外結構，這層就是針對預訓練目標所設計的head，而整個 BertPreTrainingHeads 的結構如下：

```
(cls): BertPreTrainingHeads(
    (predictions): BertLMPredictionHead(
      (transform): BertPredictionHeadTransform(
        (dense): Linear(in_features=768, out_features=768, bias=True)
        (transform_act_fn): GELUActivation()
        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      )
      (decoder): Linear(in_features=768, out_features=30522, bias=True)
    )
    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)
  )
```

從結構上來看，這個 head 分為兩個部分：

1.   **`BertLMPredictionHead`**：對應於 MLM 任務。它的邏輯流程很簡單，先經過一個線性變換（Linear），再接 GELU 啟動函數與 LayerNorm 正規化，最後由一個 decoder 層將 hidden state 映射回原始詞彙空間（vocab size = 30522）。
2.   **`seq_relationship`**：則是用來處理 NSP 任務。這部分直接將 `[CLS]` token 的表示輸入一個 linear 層，用來分類兩個句子是否相鄰。

簡單來說就是在 BERT 主體的基礎上，額外疊加兩個分類器，用來同時學習語言模型和句子關聯的預訓練目標。

### 一、BertLMPredictionHead

因此BertLMPredictionHead 本質上其實只是進行一組簡單的線性與非線性轉換，也就是 Linear → GELU → LayerNorm 的運算流程。乍看之下，這部分似乎只是基本的前向傳播組合，沒有特別複雜。不過為了與 Hugging Face 的結構保持一致，我們仍需依照它的定義方式來實作。這不僅是為了能夠順利轉移權重，更能確保我們的自訂模型在功能上完全對齊原始實作。

```
class BertPredictionHeadTransform(nn.Module):
    """
    HF key path: cls.predictions.transform.{dense, LayerNorm}
    """
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        # BERT 使用 GELU
        self.transform_act_fn = nn.GELU()
        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.transform_act_fn(hidden_states)
        hidden_states = self.LayerNorm(hidden_states)
        return hidden_states

class BertLMPredictionHead(nn.Module):
    """
    HF key path:
      cls.predictions.transform.{dense,LayerNorm}
      cls.predictions.decoder.weight  (tied with embeddings.word_embeddings.weight)
      cls.predictions.decoder.bias
    """
    def __init__(self, config):
        super().__init__()
        self.transform = BertPredictionHeadTransform(config)
        # decoder 是 Linear，但 weight 會在外部與 embeddings.word_embeddings.weight 綁定
        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.bias = nn.Parameter(torch.zeros(config.vocab_size))
        # 綁定 bias 名稱以符合 HF
        self.decoder.bias = self.bias

    def forward(self, hidden_states):
        hidden_states = self.transform(hidden_states)
        hidden_states = self.decoder(hidden_states) + self.bias
        return hidden_states
```

### 二、BertPreTrainingHeads

至於 NSP 部分，因為它的任務相對單純只需判斷兩個句子是否相鄰，因此我們只需將 [CLS] token 的表示向量輸入一個 linear 層，即可完成分類任務。也就是說，當我們完成 MLM 預測之後，只要額外接上`seq_relationship` 這個線性分類器，就能同時進行 NSP 的訓練。

```
class BertPreTrainingHeads(nn.Module):
    """
    HF key path base: cls.{predictions, seq_relationship}
    """
    def __init__(self, config):
        super().__init__()
        self.predictions = BertLMPredictionHead(config)
        self.seq_relationship = nn.Linear(config.hidden_size, 2)

    def forward(self, sequence_output, pooled_output):
        prediction_scores = self.predictions(sequence_output)
        seq_relationship_score = self.seq_relationship(pooled_output)
        return prediction_scores, seq_relationship_score
```

因此我們的 `BertForPreTraining` 模型至此也算正式完成了。整體架構其實相當直觀：我們僅需將前面已經建立的 `BertModel` 主體，與`BertPreTrainingHeads`（負責 MLM 和 NSP 任務）結合起來即可。

```
class BertForPreTraining(nn.Module):
    """
    State dict keys match Hugging Face's `BertForPreTraining`.
    Heads under `cls.predictions.*` and `cls.seq_relationship`.
    """
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.bert = BertModel(config)
        self.cls = BertPreTrainingHeads(config)
```

接下來我們來看 `forward` 函式的實作邏輯。在這裡有一個重要的細節需要特別注意，當我們呼叫內部的 `BertModel` 時，它會同時回傳兩個關鍵輸出：

1.   `sequence_output`：這是整段輸入的 contextual representation，會被用於 MLM 任務。
2.   `pooled_output`：這是來自 `pooler` 層的輸出，對應的是 `[CLS]` token 的表示，主要用於 NSP 任務。

```
def forward(
        self,
        input_ids,
        token_type_ids=None,
        attention_mask=None,
        labels=None,                # MLM labels: [B, T], 使用 -100 忽略
        next_sentence_label=None,   # NSP labels: [B]
        output_hidden_states=False,
        return_dict=False,
    ):
        sequence_output, pooled_output, all_hidden_states = self.bert(
            input_ids=input_ids,
            token_type_ids=token_type_ids,
            attention_mask=attention_mask,
            output_hidden_states=output_hidden_states,
            return_dict=False,
        )

        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)

        total_loss = None
        mlm_loss = None
        nsp_loss = None

        if labels is not None:
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            mlm_loss = loss_fct(prediction_scores.view(-1, prediction_scores.size(-1)), labels.view(-1))

        if next_sentence_label is not None:
            nsp_loss_fct = nn.CrossEntropyLoss()
            nsp_loss = nsp_loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))

        if (mlm_loss is not None) and (nsp_loss is not None):
            total_loss = mlm_loss + nsp_loss
        elif mlm_loss is not None:
            total_loss = mlm_loss
        elif nsp_loss is not None:
            total_loss = nsp_loss

        if return_dict:
            return {
                "loss": total_loss,
                "prediction_logits": prediction_scores,
                "seq_relationship_logits": seq_relationship_score,
                "hidden_states": all_hidden_states,
            }
        return (total_loss, prediction_scores, seq_relationship_score, all_hidden_states)

    # 方便與 HF 對齊命名空間
    @property
    def embeddings(self):
        return self.bert.embeddings
```

因此我們可以看到在程式碼中

*   使用 `sequence_output` 作為輸入，傳給 `BertLMPredictionHead` 以預測遮蔽詞（Masked Language Modeling）。
*   使用 `pooled_output` 作為輸入，傳給 `seq_relationship` 分類器以預測句子關聯（Next Sentence Prediction）。

這樣的設計不僅讓兩個任務共享底層的 BERT 編碼器，還能針對各自目標使用專屬的輸出 head，體現了經典的「多任務學習」思維一個骨幹、兩個任務並行訓練。最後我們也同樣驗證自定義模型與 Hugging Face 官方版本的參數是否一致，確保整體架構與權重對齊正確：

```
# 測試與 HF BertForPreTraining 對齊（含 MLM + NSP）
    hf_full = HFBertForPreTraining.from_pretrained("bert-base-uncased")
    model_full = BertForPreTraining(hf_full.config)
    sd_full = hf_full.state_dict()
    missing_f, unexpected_f = model_full.load_state_dict(sd_full, strict=False)
    print("[PreTraining] Missing:", missing_f)
    print("[PreTraining] Unexpected:", unexpected_f)
```

理想情況下的輸出為：

```
[PreTraining] Missing: []
[PreTraining] Unexpected: []
```

這表示我們的 `BertForPreTraining` 模型在架構與參數命名上，已與 Hugging Face 官方版本完全對齊，成功重現了整個預訓練模型的設計與實作。這不僅是技術驗證的一環，也為後續進行 fine-tuning 或客製化模型奠定了穩固基礎。

下集預告
----

我們終於完成了一個經典預訓練模型的完整拆解與實作。回顧整個過程可以發現 BERT 的架構其實並不算特別複雜。真正的挑戰反而在於如何讓我們自行實作的模型精確對齊 Hugging Face 的權重與設計細節。透過這樣的過程，我們不僅深入理解了 Transformer 的核心結構，也更熟悉了 Hugging Face 模型在模組化與命名上的邏輯。這些知識將對你日後進行模型調整、客製化設計，甚至是 debug 問題時發揮極大作用。

而在明天，我將帶你實際使用 Hugging Face 提供的 API 來進行一次 BERT 的 fine-tuning 實作。當你已經掌握了模型底層架構的細節，這時再進行微調操作，你將更清楚地知道這些高階封裝到底在做些什麼。這不只是使用工具，而是真正理解模型的開始。

---

<a id="day-19"></a>

## Day 19｜【Day 19】看起來很簡單？BERT 實作假新聞分類超簡單教學

- 原文：https://ithelp.ithome.com.tw/articles/10392188
- 發佈時間：2025-10-03 09:40:40

前言
==

這幾天從 Day 16 到 Day 18，我們把 Transformer 的數學公式拆得超細，連帶著整個 BERT 的架構也講得蠻透徹了。現在，是時候來點實作了。你可能不會相信，這次的程式碼簡單到讓你懷疑人生。跟之前一樣，我們不會直接拿 Hugging Face 現成的 sequence classification 模型來用，而是要自己從頭搭一個完整的 BERT 分類器，這樣才學得到東西嘛。

BERT 假新聞辨證
==========

在通訊軟體上你應該也常常看到那種標題超聳動的新聞連結吧？第一眼就被吸住忍不住想點進去看看到底發生什麼事。可是一旦你點了這個行為就會被某些追蹤或推薦系統記錄下來，也正因為這樣，假新聞才會一篇接一篇地被擴散出去。

更有趣的是假新聞在文字上常常會有種說不出的怪感，句子時常誇大，語意也常常偏離正常的用法。這時候我們就可以利用像 BERT 這樣的語意分析模型，來幫助我們做分類，所以現在來看看我們要怎麼進行分類吧。

1. BertForSequenceClassification 裝啥呢？
-------------------------------------

而在HF中這個分類器的寫法其實蠻直觀的我們只需要把 BERT 的輸出拿來接一層 Dropout 再接 Linear 就好了。具體來說就是抓出 BERT 輸出的 `pooled output`（也就是那個 [CLS] token 對應的向量），然後丟進 Dropout 做一點 regularization，最後接一層線性分類器，把它變成我們想要的分類結果。

```python
import torch.nn as nn
from transformers import AutoModel, AutoTokenizer
from transformers.modeling_outputs import SequenceClassifierOutput

class CustomBertForSequenceClassification(nn.Module):
    def __init__(self, model_name="bert-base-uncased", num_labels=2):
        super().__init__()
        self.num_labels = num_labels
        self.bert = AutoModel.from_pretrained(model_name)
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)

    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            return_dict=True
        )
        pooled_output = outputs.pooler_output
        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        loss = None
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(logits.view(-1, self.num_labels), labels.view(-1))

        return SequenceClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions
        )

model = CustomBertForSequenceClassification("bert-base-uncased", num_labels=2)
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

而我們整個分類器的輸出最後會包裝成 Hugging Face 官方提供的 `SequenceClassifierOutput` 這個格式。這個輸出物件裡面會包含幾個東西一個是訓練時用的 loss（如果有給 label 的話就會自動計算），再來是預測結果的 logits，另外還有 attention weights 跟 hidden states，說白了就是把輸出結果包裝起來讓我們更好呼叫罷了。

2. 資料準備
-------

而我們一開始當然就是從資料讀取[點我下載](https://github.com/AUSTIN2526/learning-wx-b-in-30-days)開始囉！這邊我們會用 `pandas` 來讀兩份 CSV 檔案一份是假的新聞 `Fake.csv`，另一份是真的新聞 `True.csv`。為了讓模型知道誰真誰假，我們會先各自給它們加上一個欄位 `label`，假新聞標 0，真新聞標 1。接著，再把這兩份資料合併起來，變成我們訓練用的完整 dataset，這樣資料前處理的第一步就完成了。

```python
import pandas as pd
from sklearn.model_selection import train_test_split

df_fake = pd.read_csv('Fake.csv')[['text']].assign(label=0)
df_real = pd.read_csv('True.csv')[['text']].assign(label=1)

df_all = pd.concat([df_fake, df_real], ignore_index=True)

x_train, x_valid, y_train, y_valid = train_test_split(
    df_all['text'].values,
    df_all['label'].values,
    train_size=0.8,
    random_state=46,
    shuffle=True
)
```

資料切分的部分，我們會用比較標準的做法把整體資料按照 8:2 的比例分成訓練集和測試集，這樣的切法可以幫助我們取得比較穩定、可靠的評估結果。

3. Dataset & Dataloader
-----------------------

接下來我們會自定義一個 Dataset 類別，讓 PyTorch 能夠輕鬆讀取我們處理好的資料。這個類別會把每一筆資料的標題或內容和對應的標籤包起來。

```python
import torch
from torch.utils.data import Dataset, DataLoader

class NewsDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __getitem__(self, idx):
        return self.texts[idx], self.labels[idx]

    def __len__(self):
        return len(self.texts)

def news_collate_fn(batch):
    texts, labels = zip(*batch)
    encoded = tokenizer(
        list(texts),
        max_length=512,
        truncation=True,
        padding="longest",
        return_tensors='pt'
    )
    encoded['labels'] = torch.tensor(labels, dtype=torch.long)
    return encoded

trainset = NewsDataset(x_train, y_train)
validset = NewsDataset(x_valid, y_valid)

train_loader = DataLoader(
    trainset,
    batch_size=32,
    shuffle=True,
    num_workers=0,
    pin_memory=True,
    collate_fn=news_collate_fn
)

valid_loader = DataLoader(
    validset,
    batch_size=32,
    shuffle=False,
    num_workers=0,
    pin_memory=True,
    collate_fn=news_collate_fn
)
```

同樣地為了讓 DataLoader 能夠正確處理 batch，我們還會寫一個 `collate_fn` 函式。這個函式會利用事先載好的 tokenizer，把每一筆文字轉成模型可以吃的格式：像是 `input_ids`、`attention_mask` 等等，同時也會進行 padding，確保每個 batch 的長度一致。這樣處理過後，我們的資料就能夠被順利丟進 BERT 裡跑起來了。

4. 開始訓練
-------

同樣地訓練的部分我們就直接沿用前幾天寫好的訓練器，把整個流程接起來就好。而且別忘了BERT 這種大型預訓練模型，其實在微調任務上收斂得非常快大約 1 到 2 個 epoch 就能有不錯的結果了。所以這邊我們會把 `early_stopping` 的值調得比較低，可能設個 1 或 2，讓模型只要稍微沒進步就停止訓練，避免過度擬合、也節省時間。整體來說就是讓訓練過程更有效率，畢竟這套模型本身已經夠聰明了。

這邊有一個地方要特別注意一下如果你是用昨天我們那種整套模型架構都自己搬過來的方式，那麼你其實可以不受 BERT 的輸入長度限制，也就是超過 512 tokens 也 OK，因為你可以自行修改 positional embedding 或其他底層設定。

但像我這邊如果是直接用 Hugging Face 提供的 `BertModel.from_pretrained`，那就得遵守它的輸入長度限制，最多只能接受 512 個 tokens。這是因為 BERT base 的設計就是在這個長度下預訓練的，超過的話就會出錯或自動截斷掉。

```python
from trainer import Trainer
import torch.optim as optim

optimizer = optim.AdamW(model.parameters(), lr=1e-3)
trainer = Trainer(
    epochs=100,
    train_loader=train_loader,
    valid_loader=valid_loader,
    model=model,
    optimizer=optimizer,
    early_stopping=2,
    load_best_model=True,
    grad_clip=1.0,
)

trainer.train(show_loss=True)
```

輸出結果：

```yaml
Using device: cuda
Train Epoch 0: 100%|██████████| 1123/1123 [05:57<00:00,  3.14it/s, loss=0.725]
Valid Epoch 0: 100%|██████████| 281/281 [00:30<00:00,  9.16it/s, loss=0.689]
Saving Model With Loss 0.69998
Train Loss: 0.71012 | Valid Loss: 0.69998 | Best Loss: 0.69998
```

看到這裡有沒有覺得整體流程突然變簡單很多？這也就是為什麼這麼多人喜歡用 Hugging Face 的模型架構因為它真的包裝得很完善，從模型、Tokenizer 到訓練工具，幾乎一條龍搞定，對開發者來說非常友善。

但這種幫你都弄好的包裝也不是沒有代價的。過度依賴的情況下，當模型出了問題，你可能根本搞不清楚是哪裡出錯，也不知道該怎麼下手 debug。這也是為什麼我們前幾天花了那麼多時間，一步步講解 Transformer 和 BERT 的內部運作，還帶你自己動手搭建架構就是希望你不只是「會用」，而是「真的懂」。這樣一來不管你未來想改模型、優化結構，還是針對特定任務調整設計，你都能游刃有餘，不會被框死在現成工具的限制裡。

下集預告
====

OK，Transformer Encoder的部分我們已經打好了穩固的基礎，明天就要進入全新的主題：**Transformer Decoder**。而且接下來幾天，我們也會開始介紹一些 **Decoder-only 的預訓練模型**，像是 GPT 這類的架構。

不過別擔心後面的章節不會再像前面那樣塞滿一堆數學公式了。為什麼？因為你該學的數學基礎，其實我們在講 Transformer Encoder 的時候早就打過一輪了。注意力機制、位置編碼、殘差結構、LayerNorm……那些核心元素你都已經接觸過。

這也是我之前一直強調的：**當你真正理解 Transformer，是 Encoder 也好、Decoder 也罷，甚至 GPT、BART、T5 這些變形體，其實也就差不多懂了。** 後面更多是結構上的變化與任務上的調整，而不是概念上的大轉彎。總之明天我們就正式開始學習 Decoder 吧

*   [留言 1](http://ithelp.ithome.com.tw/articles/10392188#reply)
*   [追蹤](https://ithelp.ithome.com.tw/users/login)
*   [檢舉](https://ithelp.ithome.com.tw/users/login)

[上一篇 【Day 18】一篇文章讓你搞懂BERT預訓練任務與模型實作（MLM + NSP）](https://ithelp.ithome.com.tw/articles/10391896)

[下一篇 【Day 20】Decoder 為何會胡說八道 Transformer 的生成機制與幻覺真相](https://ithelp.ithome.com.tw/articles/10392213)

---

<a id="day-20"></a>

## Day 20｜【Day 20】Decoder 為何會胡說八道 Transformer 的生成機制與幻覺真相
- 原文：https://ithelp.ithome.com.tw/articles/10392213

前言
--

前一章我們拆解了 Transformer Encoder 的結構，從多層的 Self-Attention 到 Feed Forward Network，看到它如何在編碼過程中同時捕捉序列中長短距依賴關係，並且將輸入轉換成上下文相關的語意表示。這樣的設計使得 Encoder 能夠提供一個固定不變的語境基底，而今天我們將要延續這些程式與邏輯繼續介紹Transformer Deocer

很多人第一次看到 Transformer 的 Decoder 都會冒出一個疑問：「欸？這東西不是並行運算嗎？那它怎麼確保模型不會偷看答案啊？」這個問題的答案就是`Masked Multi-Head Attention`。

Masked Multi-Head Attention
---------------------------

![Image 1: https://ithelp.ithome.com.tw/upload/images/20251002/20152236skta7nOPA4.png](images/series-8357/day-20/20152236skta7nOPA4-e7647c45988e3d22.png)

想像你在考試寫作文，規定是一個字一個字往下寫，不能偷看老師在後面偷偷幫你寫好的段落。如果模型沒有限制，它在訓練時就能一次看完整句話，那生成就變成抄答案而不是預測下一步，這樣的話測試時效果肯定會出問題，因此我們做法很簡單，就是在注意力矩陣裡塞一個「下三角遮罩」，而我們可以分常兩個

*   下三角（包含對角線）保留 → 可以看自己和過去。
*   上三角遮起來 → 未來字通通消失。

在 PyTorch 裡，一般的習慣是 True = 要遮，False = 可以算。所以程式碼會長這樣：

```
import torch

def create_causal_mask(seq_len, device=None):
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
    if device is not None:
        mask = mask.to(device)
    return mask

# 測試
mask = create_causal_mask(5)
print(mask.int())
```

輸出結果：

```
tensor([[0, 1, 1, 1, 1],
        [0, 0, 1, 1, 1],
        [0, 0, 0, 1, 1],
        [0, 0, 0, 0, 1],
        [0, 0, 0, 0, 0]], dtype=torch.int32)
```

很直覺吧？0 代表「可以看到」，1 代表「未來要遮起來」。

Cross Attention
---------------

Decoder 裡每一層都有兩個注意力模組。第一個就是 `Masked Multi-Head Attention`，它的作用是讓模型「只能看到自己已經寫出來的東西」。簡單來說就是我們的Encoder模型的Attention作法只不過會多計算一個下三角遮罩罷了。

另一個模組是 `Cross-Attention`，這個比較有趣。它的功能是讓 Decoder 抬頭去看 Encoder 給的資訊。打個比方像你在做英文翻中文的翻譯，Decoder 在寫中文的時候，會不時抬頭瞄一眼原本的英文句子，確認現在該怎麼翻才比較貼切。

```
class DecoderLayer(nn.Module):
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x, memory, tgt_mask: torch.Tensor | None, memory_mask: torch.Tensor | None):
        x = self.norm1(x + self.drop(self.self_attn(x, x, x, tgt_mask)))
        x = self.norm2(x + self.drop(self.cross_attn(x, memory, memory, memory_mask)))
        x = self.norm3(x + self.drop(self.ff(x)))
        return x
```

因此如果 Decoder 沒有 Cross-Attention，它就像是在自己講自己的話。雖然句子可能文法正確，聽起來也很順，但問題是它根本沒在參考原始輸入的內容。加上 Cross-Attention，就像搭了一座橋，讓 Decoder 在每一步生成時，都能回頭看看 Encoder 理解了什麼，這樣才有辦法寫出真正有對應關係的翻譯或回應。

但如果我們根本沒有 Encoder 模型，那當然也就不會用到 Cross-Attention。這也正是現在的語言模型模型產生`幻覺（hallucination`的最大原因之一。因為現在的語言模型大多是Decoder Only，當 Decoder 只用Self-Attention時，**它在生成內容時就是一邊看自己剛剛寫過什麼、一邊繼續編**。整個過程像是它在和自己對話。這樣雖然結果可能語句通順、邏輯也還行，可惜的是，它沒真的在看輸入內容，所以很容易就開始自己想像，寫出來的東西看似合理，其實跟原文沒啥關係這就是我們說的幻覺。

當然Cross-Attention 雖然能降低幻覺風險，但它不是萬靈丹，幻覺出現還可能是其他原因比如：

*   **Encoder 抓錯重點**：一開始 Encoder 就沒理解輸入的意思，那 Decoder 再怎麼看，也只能瞎猜。
*   **訓練資料品質差**：如果模型在訓練時學到的資料本來就錯配、亂寫，那學出來當然也不準。
*   **生成策略設計不佳**：像是用 Beam Search 時設定太貪心，或溫度參數設得太高，這些都可能讓模型變得亂編。

所以 Cross-Attention 的確像是一道安全鎖，但幻覺這件事的核心，還是出在模型自己講自己的話加上訓練過程中的偏差，要真的解決這個問題至今還是很困難的事情，因為這已經是模型的特性了。

Transformer Encoder-Decoder
---------------------------

而接下來讓我們看看標準的 Transformer 架構中，來清楚看到 Encoder 和 Decoder 的分工，而 memory（即 Encoder 最後一層的輸出）在 Decoder 的整個 forward 過程中保持不變。這其實是 Transformer 的一個經典設計Encoder 提供一個固定的語境表示，而 Decoder 則以此為基礎進行條件生成。

```
class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, N, num_heads, d_ff, dropout=0.1, pad_idx=0):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)
        self.pos = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])
        self.drop = nn.Dropout(dropout)
        self.pad_idx = pad_idx
        self.d_model = d_model

    def forward(self, tgt, memory, memory_key_mask):
        # tgt: (B, Lt), memory: (B, Ls, d), memory_key_mask: (B,1,1,Ls) True=遮
        B, Lt = tgt.shape
        device = tgt.device

        # 1) self-attn 的三種遮罩：causal（未來）、key padding（tgt中<pad>當K/V）、query padding（tgt中<pad>當Q）
        causal = make_causal_mask(Lt, device)                     # (1,1,Lt,Lt)
        kpad_t = make_key_pad_mask(tgt, self.pad_idx)             # (B,1,1,Lt)
        qpad_t = make_query_pad_mask(tgt, self.pad_idx)           # (B,1,Lt,1)
        self_mask = causal | kpad_t | qpad_t                      # (B,1,Lt,Lt)

        # 2) cross-attn 遮罩：memory 的 key padding + 當前查詢若是 pad 也一併遮
        cross_mask = memory_key_mask | qpad_t                     # (B,1,Lt,Ls)

        x = self.embed(tgt) * math.sqrt(self.d_model)
        x = self.drop(self.pos(x))
        for layer in self.layers:
            x = layer(x, memory, self_mask, cross_mask)
        return x

class Transformer(nn.Module):
    def __init__(self, src_vocab, tgt_vocab, d_model=512, N=6, num_heads=8, d_ff=2048, dropout=0.1, pad_idx=0):
        super().__init__()
        self.encoder = Encoder(src_vocab, d_model, N, num_heads, d_ff, dropout, pad_idx)
        self.decoder = Decoder(tgt_vocab, d_model, N, num_heads, d_ff, dropout, pad_idx)
        self.generator = nn.Linear(d_model, tgt_vocab)
        self.pad_idx = pad_idx

        # 實務優化：輸出層與輸入嵌入權重綁定（可省參數、常帶來微幅提升）
        self.generator.weight = self.decoder.embed.weight

    def forward(self, src, tgt):
        # encoder 回傳：memory, src_key_mask(B,1,1,Ls) True=遮
        memory, src_key_mask = self.encoder(src)
        dec_out = self.decoder(tgt, memory, src_key_mask)   # (B,Lt,d)
        logits = self.generator(dec_out)                    # (B,Lt,Vt)
        return logits
```

然而這樣的設計也不是完全無懈可擊，這個固定不變的 memory 在一些應用場景中，特別是需要細緻地根據 Decoder 當前狀態調整語境的情況下，可能會成為一種限制。就像我們在討論 Seq2Seq 架構的時候提到的那樣，靜態的編碼表示有時候無法提供足夠的彈性來處理複雜輸出序列的生成。

完整程式碼
-----

不過前面那些 Encoder、Decoder 的內容可能有點久遠了，你大概也忘了 Attention、FFN、Skip connection 這些是怎麼做的。所以這邊我們就直接把完整的 Transformer Wx+b 程式碼貼給你參考。

```
# transformer.py
# Python 3.10+, PyTorch 2.x
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

# ---- Positional Encoding (sinusoidal) ----
class PositionalEncoding(nn.Module):
    def __init__(self, d_model: int, max_len: int = 5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(pos * div)
        pe[:, 1::2] = torch.cos(pos * div)
        self.register_buffer("pe", pe.unsqueeze(0))  # (1, max_len, d_model)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (B, L, D)
        return x + self.pe[:, :x.size(1)]

# ---- Masks ----
def make_subsequent_mask(L: int, device=None) -> torch.Tensor:
    # (L, L), True=可見
    m = torch.tril(torch.ones(L, L, dtype=torch.bool, device=device))
    return m

def make_pad_mask(seq: torch.Tensor, pad_idx: int) -> torch.Tensor:
    # seq: (B, L) -> (B, 1, 1, L), True=非PAD
    return (seq != pad_idx).unsqueeze(1).unsqueeze(2)

# ---- Multi-Head Attention (純線性 Wx+b 投影) ----
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
        super().__init__()
        assert d_model % num_heads == 0
        self.h = num_heads
        self.dk = d_model // num_heads
        self.Wq = nn.Linear(d_model, d_model)  # Wx+b
        self.Wk = nn.Linear(d_model, d_model)
        self.Wv = nn.Linear(d_model, d_model)
        self.Wo = nn.Linear(d_model, d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, q, k, v, mask: torch.Tensor | None = None):
        B = q.size(0)

        def split_heads(x):
            # (B, L, D) -> (B, h, L, dk)
            return x.view(B, -1, self.h, self.dk).transpose(1, 2)

        Q = split_heads(self.Wq(q))
        K = split_heads(self.Wk(k))
        V = split_heads(self.Wv(v))

        scores = Q @ K.transpose(-2, -1) / math.sqrt(self.dk)  # (B, h, Lq, Lk)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float("-inf"))

        attn = torch.softmax(scores, dim=-1)
        attn = self.drop(attn)
        out = attn @ V  # (B, h, Lq, dk)

        out = out.transpose(1, 2).contiguous().view(B, -1, self.h * self.dk)  # (B, Lq, D)
        return self.Wo(out)  # (B, Lq, D)

# ---- Position-wise FeedForward (兩層線性 Wx+b) ----
class FeedForward(nn.Module):
    def __init__(self, d_model: int, d_ff: int = 2048, dropout: float = 0.1):
        super().__init__()
        self.lin1 = nn.Linear(d_model, d_ff)
        self.lin2 = nn.Linear(d_ff, d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x):
        return self.lin2(self.drop(F.relu(self.lin1(x))))

# ---- Encoder/Decoder Layer ----
class EncoderLayer(nn.Module):
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x, src_mask: torch.Tensor | None = None):
        x = self.norm1(x + self.drop(self.self_attn(x, x, x, src_mask)))
        x = self.norm2(x + self.drop(self.ff(x)))
        return x

class DecoderLayer(nn.Module):
    def __init__(self, d_model: int, num_heads: int, d_ff: int, dropout: float = 0.1):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.cross_attn = MultiHeadAttention(d_model, num_heads, dropout)
        self.ff = FeedForward(d_model, d_ff, dropout)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.drop = nn.Dropout(dropout)

    def forward(self, x, memory, tgt_mask: torch.Tensor | None, memory_mask: torch.Tensor | None):
        x = self.norm1(x + self.drop(self.self_attn(x, x, x, tgt_mask)))
        x = self.norm2(x + self.drop(self.cross_attn(x, memory, memory, memory_mask)))
        x = self.norm3(x + self.drop(self.ff(x)))
        return x

# ---- Stacks ----
class Encoder(nn.Module):
    def __init__(self, vocab_size: int, d_model: int, N: int, num_heads: int, d_ff: int,
                 dropout: float = 0.1, pad_idx: int = 0):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)
        self.pos = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])
        self.drop = nn.Dropout(dropout)
        self.pad_idx = pad_idx

    def forward(self, src):
        src_mask = make_pad_mask(src, self.pad_idx)  # (B,1,1,Ls)
        x = self.embed(src) * math.sqrt(self.embed.embedding_dim)
        x = self.drop(self.pos(x))
        for layer in self.layers:
            x = layer(x, src_mask)
        return x, src_mask

class Decoder(nn.Module):
    def __init__(self, vocab_size: int, d_model: int, N: int, num_heads: int, d_ff: int,
                 dropout: float = 0.1, pad_idx: int = 0):
        super().__init__()
        self.embed = nn.Embedding(vocab_size, d_model, padding_idx=pad_idx)
        self.pos = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(N)])
        self.drop = nn.Dropout(dropout)
        self.pad_idx = pad_idx
        self.d_model = d_model

    def forward(self, tgt, memory, memory_mask):
        B, Lt = tgt.shape
        pad = make_pad_mask(tgt, self.pad_idx)               # (B,1,1,Lt)
        causal = make_subsequent_mask(Lt, tgt.device)        # (Lt,Lt)
        tgt_mask = pad & causal.unsqueeze(0).unsqueeze(1)     # (B,1,Lt,Lt)

        x = self.embed(tgt) * math.sqrt(self.d_model)
        x = self.drop(self.pos(x))
        for layer in self.layers:
            x = layer(x, memory, tgt_mask, memory_mask)
        return x

# ---- Transformer ----
class Transformer(nn.Module):
    def __init__(self, src_vocab: int, tgt_vocab: int, d_model: int = 512, N: int = 6,
                 num_heads: int = 8, d_ff: int = 2048, dropout: float = 0.1, pad_idx: int = 0):
        super().__init__()
        self.encoder = Encoder(src_vocab, d_model, N, num_heads, d_ff, dropout, pad_idx)
        self.decoder = Decoder(tgt_vocab, d_model, N, num_heads, d_ff, dropout, pad_idx)
        self.generator = nn.Linear(d_model, tgt_vocab)  # 最終 Wx+b
        self.pad_idx = pad_idx

    def forward(self, src, tgt):
        memory, src_mask = self.encoder(src)
        out = self.decoder(tgt, memory, src_mask)
        logits = self.generator(out)  # (B, Lt, Vt)
        return logits

    @torch.no_grad()
    def greedy_decode(self, src, bos_idx: int, eos_idx: int, max_len: int = 64, device: str = "cpu"):
        self.eval()
        memory, src_mask = self.encoder(src.to(device))
        B = src.size(0)
        ys = torch.full((B, 1), bos_idx, dtype=torch.long, device=device)
        for _ in range(max_len - 1):
            dec = self.decoder(ys, memory, src_mask)
            next_token = self.generator(dec[:, -1:, :]).argmax(-1)  # (B,1)
            ys = torch.cat([ys, next_token], dim=1)
            if (next_token == eos_idx).all():
                break
        return ys
```

下集預告
----

下一章我們要聚焦於 Decoder-only 架構的 GPT-2，與 Encoder-Decoder 不同 GPT-2 完全放棄 Encoder，只依靠多層 Decoder 與 Causal Mask 來進行生成。這樣的設計大幅簡化了結構並提升了可擴展性，但同時也增加了幻覺的風險。因此明天解析 GPT-2 的設計理念、它與 Encoder-Decoder 的差異，以及為何這種簡化的架構能成為現今大型語言模型的主流基礎。

---

<a id="day-21"></a>

## Day 21｜【Day 21】從 Wx+b 到能寫詩的模型GPT-2 的煉成
- 原文：https://ithelp.ithome.com.tw/articles/10392231

前言
--

今天我們來聊聊 GPT 模型的架構，特別是現在很常見、也很實用的「Decoder-only」設計。這類模型其實已經在各種任務上展現出超強的能力，無論是生成長篇文章、聊天對話，甚至是寫程式，都有非常不錯的表現。

所以今天我們就從 GPT-2 的基本設計開始，一步步帶大家拆解這種架構到底怎麼組成、有哪些地方容易踩雷，又有哪些訓練技巧是真的有幫助的。我們不會去講太多花俏的設計，而是回到最小可行架構希望讓大家可以從底層真正搞懂這個模型的原理，也能在實作的時候少走一點冤枉路。

**GPT-2，全名是 Generative Pre-trained Transformer 2，它在自然語言處理（NLP）這個領域裡可以說是一個超重要的里程碑。**

它雖然跟 Google 的 BERT 一樣，都是基於 Transformer 架構打造出來的模型，但它們的設計邏輯其實大不相同。BERT 的重點是「理解語意」，所以它會從前後兩邊同時讀取文字，透過所謂的「雙向編碼」來預測句子中被遮蔽的詞語。簡單說，它是在考你對上下文的理解力。

但 GPT-2 玩的是另一種套路。它的策略是「自回歸生成」，也就是從左到右一個詞一個詞慢慢生出來。這樣的方式，就像人類在寫東西時，一邊想、一邊打字的邏輯流動。因為它是按順序產出語句，所以在生成像小說、聊天對話、甚至程式碼時，它的自然度跟創造力都表現得非常強。

> GPT 的目標不是理解文字，而是要創作，而作這件事本來就不是先知道全部再倒推，而是像人類一樣，一步步寫下去。

GPT 的預訓練任務
----------

GPT-2 訓練的核心任務叫做 **自回歸語言建模（Causal Language Modeling）**，意思是它要學會預測下一個詞會是什麼，舉例來說，給它一串文字 `[x₁, x₂, …, xₙ]`，它的工作是學會在每個時間點預測下一個 token 的機率。這用數學式子表示如下：

```
P(x₁, x₂, …, xₙ) = ∏ P(xᵢ | x₁, x₂, …, xᵢ₋₁)
```

這句話翻成白話就是每個詞的出現只能根據它前面那些詞來判斷，不能偷看後面還沒出現的內容。這種規則也就是自回歸的本質。而這樣才貼近人類書寫時的真實狀況。當我們在打字時，是不知道未來幾個字會怎樣寫的，我們只能根據現在的語境去決定下一步。

GPT 的模型結構
---------

雖然 GPT 和 BERT 都是建立在 Transformer 這個架構之上，但其實它們對這個原始設計並沒有大刀闊斧地改造，基本骨架幾乎一模一樣。大多數的變化，其實只是一些細節上的調整。以 GPT 為例最主要的幾個修改包括：使用了`可學習的位置編碼（learnable position embeddings）`，還有把 LayerNorm 的位置做了調整。

我們先來看 Transformer 原始的設計。在 2017 年的那篇經典論文中，每一層的處理流程大概是這樣的：

```
x = x + Sublayer(x)  
x = LayerNorm(x)
```

這叫做 **Post-LN 架構**，意思是模組處理完後，再加上原來的輸入最後做 Layer Normalization。這樣做的好處是**訓練初期穩定**，不容易一開始就亂掉。但隨著模型越來越深，比如 GPT-2 有幾十層那種深度，就發現這種設計會在訓練後期出現 **梯度消失** 的問題模型變得學不動了。

所以 GPT-2 改用了一種叫做 **Pre-LN 架構** 的設計，它把 LayerNorm 移到一開始，流程變成這樣：

```
y = x + Sublayer(LayerNorm(x))
```

這個改動讓模型在非常深的情況下還能保持穩定，也比較容易訓練得好。這也是為什麼 GPT-2 能做出像 1.5 億個參數、甚至超過 40 層深度的大型模型，還能有效運作。

> 你可能會想為什麼 LayerNorm 的位置會影響這麼大？因為 LayerNorm 是在調整訊號的穩定性。如果等模組跑完才正規化，深層模型可能會累積太多訊號雜訊，最後訓練失效。反過來，一開始就做正規化，會讓整個訊號流程更穩。

Wx + b 就能造出 GPT-2？
------------------

如果你對 Transformer 架構有點概念，那 GPT-2 的設計應該不會太陌生。這邊就不從頭細講整個架構了，我們挑幾個比較核心的部分來聊一下。先來看看 HuggingFace 的 `transformers` 套件裡 GPT-2 的模型結構，大致上是這樣：

```
GPT2LMHeadModel(
  (transformer): GPT2Model(
    (wte): Embedding(50257, 768)
    (wpe): Embedding(1024, 768)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-11): 12 x GPT2Block(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): GPT2Attention(
          (c_attn): Conv1D()
          (c_proj): Conv1D()
          (attn_dropout): Dropout(p=0.1, inplace=False)
          (resid_dropout): Dropout(p=0.1, inplace=False)
        )
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): GPT2MLP(
          (c_fc): Conv1D()
          (c_proj): Conv1D()
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=768, out_features=50257, bias=False)
)
```

### 1. Attention

在 GPT 的 Attention 設計裡，**其實跟原本 Transformer Decoder 的做法是一樣的**，使用的是 **causal attention**。這種設計的關鍵在於模型在預測下一個詞的時候，只能看到它前面的詞，不能偷看後面的內容，這樣才能符合語言生成的因果順序。而在 GPT 的實作裡，Attention 中的 Q、K、V 是透過一個叫 `c_attn` 的模組來計算的，輸出結果則是透過 `c_proj` 來處理。這兩個部分，其實本質上都是用一個叫 `Conv1D` 的模組來實作的。

不過這裡的 `Conv1D` 有點容易讓人誤會。它名字裡雖然有`Conv`，但其實跟我們在 CNN 裡學到的那種一維卷積完全不一樣。這裡的 `Conv1D` 其實就是一個線性層，本質上就是做一個矩陣乘法加上偏置，所以它不是真的做卷積，而是把輸入的向量轉成我們需要的維度。

```
class Conv1D(nn.Module):
    def __init__(self, nf, nx):
        super().__init__()
        self.nf = nf
        self.weight = nn.Parameter(torch.empty(nx, nf))
        self.bias = nn.Parameter(torch.zeros(nf))
        nn.init.normal_(self.weight, mean=0.0, std=0.02)

    def forward(self, x):
        # x: [..., nx]
        size_out = x.size()[:-1] + (self.nf,)
        x = torch.addmm(self.bias, x.view(-1, x.size(-1)), self.weight)
        x = x.view(size_out)
        return x
```

而GPT-2 把 QKV 和 O 分開處理（用 `c_attn` 處理 QKV，用 `c_proj` 處理 O）其實還有個很實用的好處，就是做 `hook` 或分析模型的時候方便很多。如果你只是想抓出來看看模型在跑的時候產生的 Q、K、V 值，那只要 hook 一下 `attn.c_attn` 這個模組就好，不但寫起來簡單、程式碼也比較乾淨好維護。反之如果你只關心最後注意力的輸出（也就是 O），那就可以直接 `hook attn.c_proj`。

```
class GPT2Attention(nn.Module):
    def __init__(self, config):
        super().__init__()
        nx = config.n_embd
        n_head = config.n_head
        if nx % n_head != 0:
            raise ValueError("n_embd must be divisible by n_head")
        self.n_head = n_head
        self.head_dim = nx // n_head
        self.scale_attn = 1.0 / math.sqrt(self.head_dim)

        # c_attn projects to q, k, v concatenated
        self.c_attn = Conv1D(3 * nx, nx)
        self.c_proj = Conv1D(nx, nx)

        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        self.resid_dropout = nn.Dropout(config.resid_pdrop)

        # Register a causal mask buffer up to max positions
        max_pos = config.n_positions
        mask = torch.tril(torch.ones((max_pos, max_pos), dtype=torch.bool))
        self.register_buffer("causal_mask", mask[None, None, :, :], persistent=False)  # [1,1,T,T]

    def _split_heads(self, x):
        # x: [B, T, n_embd] -> [B, n_head, T, head_dim]
        B, T, C = x.size()
        x = x.view(B, T, self.n_head, self.head_dim).permute(0, 2, 1, 3)
        return x

    def _merge_heads(self, x):
        # x: [B, n_head, T, head_dim] -> [B, T, n_embd]
        x = x.permute(0, 2, 1, 3).contiguous()
        B, T, _, _ = x.size()
        return x.view(B, T, self.n_head * self.head_dim)

    def forward(self, x, attention_mask=None):
        B, T, _ = x.size()

        qkv = self.c_attn(x)  # [B, T, 3*n_embd]
        q, k, v = qkv.split(qkv.size(-1) // 3, dim=2)

        q = self._split_heads(q)  # [B, h, T, hd]
        k = self._split_heads(k)  # [B, h, T, hd]
        v = self._split_heads(v)  # [B, h, T, hd]

        attn_scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale_attn  # [B,h,T,T]

        # Causal mask
        attn_scores = attn_scores.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float("-inf"))

        # Additive attention mask [B,1,1,T], if provided
        if attention_mask is not None:
            attn_scores = attn_scores + attention_mask  # broadcast on last dim

        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.attn_dropout(attn_probs)

        context = torch.matmul(attn_probs, v)  # [B,h,T,hd]
        context = self._merge_heads(context)   # [B,T,n_embd]
        out = self.c_proj(context)
        out = self.resid_dropout(out)
        return out
```

### 2. FFN

而 FFN 在 GPT-2 裡是透過 GPT2MLP 這個類別來實作的。不過要特別注意GPT-2 採用的是 Pre-LN 架構，也就是說，在進入 FFN（以及 Self-Attention）之前，會先做 LayerNorm，而不是像某些其他模型那樣把 LayerNorm 放在最後。

```
class GPT2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        nx = config.n_embd
        # HF uses intermediate size = 4 * n_embd by default (config.n_inner may override)
        n_inner = getattr(config, "n_inner", None) or 4 * nx
        self.c_fc = Conv1D(n_inner, nx)
        self.c_proj = Conv1D(nx, n_inner)
        self.act = nn.GELU()
        self.dropout = nn.Dropout(config.resid_pdrop)

    def forward(self, x):
        x = self.c_fc(x)
        x = self.act(x)
        x = self.c_proj(x)
        x = self.dropout(x)
        return x
```

### 3. GPT2Block

GPT2Block 所採用的其實正是 Pre-LN 架構，其實現方式並不複雜，基本延續了我們先前所構建的處理流程，只是在每個子模組執行前引入 LayerNorm 而已。

```
class GPT2Block(nn.Module):
    def __init__(self, config):
        super().__init__()
        eps = getattr(config, "layer_norm_epsilon", 1e-5)  # HF key
        self.ln_1 = nn.LayerNorm(config.n_embd, eps=eps)
        self.attn = GPT2Attention(config)
        self.ln_2 = nn.LayerNorm(config.n_embd, eps=eps)
        self.mlp = GPT2MLP(config)

    def forward(self, x, attention_mask=None):
        x = x + self.attn(self.ln_1(x), attention_mask=attention_mask)
        x = x + self.mlp(self.ln_2(x))
        return x
```

### 4. GPT2Model

在最終階段我們構建了 `GPT2Model` 架構，開頭部分的 `wte`（word token embedding）負責將輸入的 token 映射到向量空間。這裡的 `50257` 是 GPT-2 的詞彙表大小，表示模型能識別的 token 總數，而 `768` 則代表每個 token 的向量維度。

接下來是 `wpe`（position embedding），它負責加入位置資訊。與原始 Transformer 採用固定的正弦位置編碼不同，GPT-2 選擇了 **可訓練的嵌入向量**，也就是說每個位置都有一個參數化的向量，能夠在訓練過程中學習序列中位置的語義特徵。預設最大長度為 1024，表示這個模型最多能處理 1024 個 token 的輸入長度。這兩者加總後會經過 dropout 層，再傳入一連串的 `GPT2Block`。

```
class GPT2Model(nn.Module):
    """
    Matches HF GPT2Model module/param names:
      - wte, wpe, h.{i}.attn.{c_attn,c_proj}, h.{i}.ln_1, h.{i}.mlp.{c_fc,c_proj}, ln_f
    """
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.wte = nn.Embedding(config.vocab_size, config.n_embd)
        self.wpe = nn.Embedding(config.n_positions, config.n_embd)
        self.drop = nn.Dropout(config.embd_pdrop)
        self.h = nn.ModuleList([GPT2Block(config) for _ in range(config.n_layer)])
        eps = getattr(config, "layer_norm_epsilon", 1e-5)
        self.ln_f = nn.LayerNorm(config.n_embd, eps=eps)
        
    def forward(self, input_ids, attention_mask=None, output_hidden_states=False, return_dict=False):
        B, T = input_ids.size()
        if T > self.config.n_positions:
            raise ValueError(f"Sequence length {T} exceeds n_positions {self.config.n_positions}")

        # Positions
        pos = torch.arange(T, device=input_ids.device, dtype=torch.long).unsqueeze(0).expand(B, T)

        # Embeddings
        x = self.wte(input_ids) + self.wpe(pos)
        x = self.drop(x)

        # Attention mask -> additive [B,1,1,T]
        ext_mask = _make_extended_attn_mask(attention_mask, x.dtype) if attention_mask is not None else None

        all_hidden_states = [] if output_hidden_states else None
        for block in self.h:
            if output_hidden_states:
                all_hidden_states.append(x)
            x = block(x, attention_mask=ext_mask)
        x = self.ln_f(x)
        if output_hidden_states:
            all_hidden_states.append(x)

        if return_dict:
            return {"last_hidden_state": x, "hidden_states": all_hidden_states}
        return (x, all_hidden_states)
```

到目前為止我們已經完成了模型的主體結構，不過很明顯還少了一個關鍵部分，輸出層現在模型僅產生了 hidden states，也就是最後一層 Decoder 的隱表示。但這些向量本身還不能直接對應到語言輸出。為了讓模型能夠預測下一個詞或 token，還需要一個額外的線性層，將 hidden states 投影回詞彙表的大小，從而生成 logits 分佈。

換句話說我們還少了一個 詞彙投影層（language modeling head），它負責將隱藏狀態轉換為每個 token 的機率分佈，這才是實際生成文字的關鍵步驟。

### 5.`GPT2LMHeadModel`

有個滿關鍵的細節是，lm_head 的權重其實是跟 wte（也就是輸入的詞嵌入層）共用的。這種做法叫做 `weight tying`，簡單來說就是把輸入跟輸出用同一組權重。這樣不只可以大幅減少模型的參數量，也能讓學習過程更穩定。而 `lm_head` 這一層，正是模型用來產生最終文字的那個head

```
class GPT2LMHeadModel(nn.Module):
    """
    Matches HF GPT2LMHeadModel heads and weight tying:
      - transformer (GPT2Model)
      - lm_head.weight tied to transformer.wte.weight
    """
    def __init__(self, config):
        super().__init__()
        self.transformer = GPT2Model(config)
        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)
        # tie weights
        self.lm_head.weight = self.transformer.wte.weight

    # HF API helpers
    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings
```

接著在 forward 函數裡，模型會先算出最後一層的 hidden states，然後通過 lm_head 把它轉換成 logits，也就是每個 token 對應所有詞彙的預測分數。通常我們在訓練的時候，會對 logits 和 labels 做個「右移」對齊，這樣模型才能學會預測「下一個」字。

```
def forward(self, input_ids, attention_mask=None, labels=None, output_hidden_states=False, return_dict=False):
        outputs = self.transformer(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=output_hidden_states,
            return_dict=True,
        )
        hidden_states = outputs["last_hidden_state"]
        logits = self.lm_head(hidden_states)

        loss = None
        if labels is not None:
            # Shift for next-token prediction
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        if return_dict:
            return {"loss": loss, "logits": logits, "hidden_states": outputs["hidden_states"]}
        return (loss, logits, outputs["hidden_states"])
```

最後如果我們要讓模型產生文字，最終輸出的 logits 是個三維的張量，形狀是 `[batch_size, sequence_length, vocab_size]`，每個位置都表示那個時間點上，每個詞出現的機率。然後就可以用像是取最大值或是抽樣的方法，從這些機率裡選出最有可能的下一個字，完成一整段的生成。

下集預告
----

今天我們從 GPT-2 的基礎設計一路拆解到整個模型的實作細節，應該可以感受到Decoder-only架構雖然看起來簡單，背後其實藏了不少設計巧思。那明天我們要來換個口味，實際動手做一個簡單但實用的應用場景：語言翻譯任務。這個任務看似老派但正因為夠直觀，也能夠比對與seq2seq的差異，那麼我們明天再見！

---

<a id="day-22"></a>

## Day 22｜【Day 22】不靠 Encoder？用 GPT-2 試試翻譯的可能性
- 原文：https://ithelp.ithome.com.tw/articles/10393194

前言
--

在進行中文翻英文的任務時，我們這次使用 GPT-2 進行訓練，並延續先前提到過的資料集與概念。回顧一下之前我們提過像是 [CLS] 和 [SEP] 這類特殊標籤在 BERT 類模型中的作用，但在 GPT-2 這類僅由 Decoder 組成的模型架構中，它的運作邏輯是不同的。

GPT-2 模型主要是依賴因果語言建模（Causal Language Modeling）來預測序列中的下一個詞，而不是整體句子的分類或雙句任務。因此在這種語言生成的場景中，我們會直接餵入原始的中文句子，讓模型去生成對應的英文翻譯。

1.準備資料集
-------

我們首先同樣的透過 pandas 讀取 CSV 檔案並且使用 sklearn 的 train_test_split 將整體資料集以 8:2 的比例劃分為訓練與驗證集。

```
import pandas as pd
from sklearn.model_selection import train_test_split

df = pd.read_csv('translate.csv')
input_texts = df['chinese'].values
target_texts = df['english'].values

x_train, x_val, y_train, y_val = train_test_split(input_texts, target_texts, train_size=0.8, random_state=46)
```

2. 讀取模型權重
---------

接著到了模型載入的階段第一種較為直接我們可以透過 Hugging Face 的 transformers 套件，直接調用官方訓練好的 GPT-2 模型與對應的 tokenizer。這裡特別要注意的一點是，GPT-2 原生並未設定 padding token，因此我們手動將 pad_token 設定為 eos_token，這樣在批次處理時才不會出錯。

```
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("gpt2")
model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # 避免 padding 出錯
```

另一種方式則是利用自己自定義的 GPT-2 模型結構來承接 Hugging Face 所提供的模型權重。這裡我們透過 GPT2LMHeadModel 載入 Hugging Face 的 GPT-2 權重，並取得其中的 config 配置資訊，再根據這份設定來初始化我們自己的 GPT-2 模型架構。

```
from transformers import GPT2LMHeadModel as HFGPT2

    # Load HF model and config
    hf = HFGPT2.from_pretrained("gpt2")
    config = hf.config  # GPT2Config with fields: n_embd, n_head, n_layer, n_positions, layer_norm_epsilon, etc.

    # Build our model with the same config and load weights
    model = GPT2LMHeadModel(config)
    sd = hf.state_dict()
    missing, unexpected = model.load_state_dict(sd, strict=False)
    print("Missing keys:", missing)
    print("Unexpected keys:", unexpected)

    # Quick forward
    B, T = 2, 16
    input_ids = torch.randint(0, config.vocab_size, (B, T))
    labels = input_ids.clone()
    attn_mask = torch.ones(B, T, dtype=torch.long)  # keep all tokens

    out = model(
        input_ids=input_ids,
        attention_mask=attn_mask,
        labels=labels,
        output_hidden_states=True,
        return_dict=True,
    )
    print("Loss:", float(out["loss"]))
    print("Logits shape:", tuple(out["logits"].shape))  # [B, T, vocab]
```

最後我們也可以使用 state dict 讀入我們的模型中，並找出有遺漏或不匹配的鍵值名稱，以確認權重是否正確套用。同時為了保險起見，我們也跑了一次 forward pass，隨機產生兩筆長度為 16 的序列資料，讓模型輸出 loss 與 logits，藉此驗證模型的基本功能是否正常。

3.建立DataLoader
--------------

在GPT-2這類的預訓練模型中，通常會使用prompt進行訓練，因此我們可以在DataLoader抓取資料時自動套用一個特定的 prompt 模板，例如 "Translate Chinese to English: 你好 => Hello"。這種方式其實有點像是做「少量提示學習（few-shot prompting）」，利用 prompt 結構告訴模型它目前的任務是翻譯中文成英文。

接下來這個類別裡最核心的其實是 collate_fn 這個方法，我們把每一筆資料按照指定格式拼接成一段文字，然後一口氣送進 tokenizer。這裡做了一個重要的處理我們在每一段訓練輸入後面都加上了 tokenizer.eos_token，這是讓 GPT-2 知道句子結束的信號。

```
from torch.utils.data import Dataset, DataLoader

# ---------- 自訂 Dataset 類別 ----------
class GPT2TranslateDataset(Dataset):
    def __init__(self, sources, targets, tokenizer, prompt="Translate Chinese to English: {} =>"):
        self.sources = sources
        self.targets = targets
        self.tokenizer = tokenizer
        self.prompt = prompt

    def __len__(self):
        return len(self.sources)

    def __getitem__(self, idx):
        return self.sources[idx], self.targets[idx]

    def collate_fn(self, batch):
        sources, targets = zip(*batch)
        texts = [self.prompt.format(src) + tgt + self.tokenizer.eos_token for src, tgt in zip(sources, targets)]

        tokenized = self.tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors='pt')

        input_ids = tokenized['input_ids']
        attention_mask = tokenized['attention_mask']

        labels = input_ids.clone()
        labels[attention_mask == 0] = -100

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }
    
prompt_template="Translate Chinese to English: {} =>"
trainset = GPT2TranslateDataset(x_train, y_train, tokenizer, prompt_template)
validset = GPT2TranslateDataset(x_val, y_val, tokenizer, prompt_template)

train_loader = DataLoader(
    trainset,
    batch_size = 16,
    shuffle = True,
    num_workers = 0,
    pin_memory = True,
    collate_fn = trainset.collate_fn
)

valid_loader = DataLoader(
    validset,
    batch_size = 16,
    shuffle = False,
    num_workers = 0,
    pin_memory = True,
    collate_fn = validset.collate_fn
)
```

而在這裡我們將padding 的部分在 labels 裡標記為 -100，這樣在計算 loss 的時候就會自動忽略這些 token，避免干擾模型的訓練(Pytorch的損失函數預設-100是不被計算的)

4.訓練模型
------

接下來訓練的時候我們會加上一個叫 `get_cosine_schedule_with_warmup` 的方法，簡單來說就是一種把學習率先拉高再慢慢降下來的策略。它一開始會用 warmup 的方式讓學習率慢慢升高，接著再按照餘弦曲線慢慢往下調，這樣可以幫助模型在訓練初期更穩定，不會一開始就學太快、搞得很不穩。

這邊我們把 warmup 的步數設成整個訓練步數的 20%，也就是說前 20% 的時間學習率會漸漸升上去，之後再緩緩下降。

```
from trainer import Trainer
import torch.optim as optim
from transformers import get_cosine_schedule_with_warmup

# 總步數 = epoch 數 * 每個 epoch 的 batch 數
num_training_steps = len(train_loader) * 100  # 100 是總 epoch 數
num_warmup_steps = int(0.2 * len(train_loader))  # 可調整 warmup 比例

optimizer = optim.AdamW(model.parameters(), lr=1e-3)
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps,
)
trainer = Trainer(
    epochs=100,
    train_loader=train_loader,
    valid_loader=valid_loader,
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    early_stopping=2,
    load_best_model=True,
    grad_clip=1.0,
)

trainer.train(show_loss=True)
```

輸出結果：

```
Train Epoch 1: 100%|██████████| 1496/1496 [01:23<00:00, 17.83it/s, loss=1.221]
Valid Epoch 1: 100%|██████████| 374/374 [00:07<00:00, 48.00it/s, loss=1.512]
Saving Model With Loss 1.42683
Train Loss: 1.39996 | Valid Loss: 1.42683 | Best Loss: 1.42683
```

其實這種模型不用花太多時間訓練，因為我們大多只會調最後那層 head 的權重，讓它更貼近我們要解決的問題。

5.模型評估
------

當模型需要把輸入補到一樣長的時候，通常會選擇把 padding token 加在「左邊」，這在做生成任務的時候特別重要，尤其是像 GPT 這種自回歸模型。因為這類模型是從左到右一個字一個字慢慢生成的。如果 padding 加在右邊，而又沒給 attention mask，那模型一開始就會看到一堆沒用的 padding，結果可能會亂生成。

而且左側 padding 還有個實務上的好處——它讓計算更有效率。舉例來說，batch 處理時模型會用 attention mask 去跳過 padding 的部分。如果所有 padding 都在左邊，那有效的文字內容就會整齊地對齊在右邊，這樣在做矩陣運算的時候資料會比較緊湊，對像 GPU 這種硬體來說也比較好發揮，速度會更快。

```
import torch
import sacrebleu

def translate_and_eval(model, tokenizer, loader):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device).eval()
    hyps, refs = [], []
    with torch.no_grad():
        for batch in loader:
            batch = {k:v.to(device) for k,v in batch.items()}
            out = model.generate(**{k: v for k,v in batch.items() if k != 'labels'})
            
            # decode hypotheses
            hyps += tokenizer.batch_decode(out, skip_special_tokens=True)

            # handle -100 in labels
            labels = batch['labels'].clone()
            labels[labels == -100] = tokenizer.pad_token_id
            refs += tokenizer.batch_decode(labels, skip_special_tokens=True)

    bleu = sacrebleu.corpus_bleu(hyps, [refs], lowercase=True)
    print(f"Corpus BLEU: {bleu.score:.2f}")

# 呼叫
tokenizer.padding_side = "left"
tokenizer.pad_token = tokenizer.eos_token
translate_and_eval(model, tokenizer, valid_loader)
```

輸出結果：

```
Corpus BLEU: 28.46%
```

可以看到我們最終的結果，雖然跟傳統的 seq2seq 模型差不多，但在訓練速度和評估效率上還是有蠻明顯的差別。至於為什麼效能沒有差太多，主要是因為 Decoder 模型不像 Encoder 那樣擅長「理解」語言的結構和語意，所以兩者在效果上不會差太遠。

下集預告
----

現在我們已經學過了 Encoder 和 Decoder，那接下來就來看看把這兩個結合起來的 Encoder-Decoder 架構吧！順帶一提，明天我們也會進入一個新主題，第一次接觸語音模型，不過你已經理解的Transformer所以我相信你很快就能知道這些模型在幹嘛了。

---

<a id="day-23"></a>

## Day 23｜【Day 23】語音模型原來長這樣？Wx+b拆給你看Whisper 架構！
- 原文：https://ithelp.ithome.com.tw/articles/10394402

前言
--

訓練一個語音模型其實比你想的還難，因為你需要大量的語音資料、逐字的轉錄、還有很強的硬體資源。所以大家常見的做法就是先拿一個已經學會很多語音跟語言規則的現成模型，然後換自己的資料來做微調。而用得最廣的選擇之一就是 OpenAI 推出的 Whisper。這次我們會一步步拆解 Whisper 的架構、它是怎麼被訓練的、怎麼微調，最後還會給你一個 PyTorch + Hugging Face 的實作範例。

簡單來說，Whisper 是一個 Encoder–Decoder 的 Transformer 架構，它前面多了一段卷積層處理聲音輸入，並用了大量的半監督學習資料來訓練。

輸入資料會先變成一張 log-Mel 頻譜圖（就是聲音的視覺化表示），然後先經過兩層 1D 卷積，讓時間軸資料變成原本的四分之一，再丟進 Encoder 做特徵抽象。接下來Decoder 就會從文字 token開始產出輸出，利用 cross-attention 把聲音資訊對齊，逐步生成文字或其他任務的結果。

![Image 1: https://images.ctfassets.net/kftzwdyauwt9/d9c13138-366f-49d3-a1a563abddc1/8acfb590df46923b021026207ff1a438/asr-summary-of-model-architecture-desktop.svg?w=1920&amp;q=90](images/series-8357/day-23/asr-summary-of-model-architecture-de-90bc7b7e2cace889.svg)

> 圖片來源:[OpenAi](https://www.google.com/url?sa=i&url=https%3A%2F%2Fopenai.com%2Fzh-Hant%2Findex%2Fwhisper%2F&psig=AOvVaw0aWrhtPtNP7zYeLnmcuHd1&ust=1759883110783000&source=images&cd=vfe&opi=89978449&ved=0CBUQjRxqFwoTCNjbuc_pkJADFQAAAAAdAAAAABAE)

Whisper 最大的優勢是，它不只會做語音轉文字，它一開始訓練時就同時學會了語音辨識、語言辨識、翻譯、時間戳記標註等等任務。所以你只要選擇你要做的任務，丟一些資料，它就能幫你做微調訓練，非常方便。

Whisper 模型架構介紹
--------------

我們來看一下 Hugging Face 上實作 Whisper 的程式碼結構長什麼樣子，裡面有 Encoder、Decoder、Attention、FFN 等組件：

```
WhisperForConditionalGeneration(
  (model): WhisperModel(
    (encoder): WhisperEncoder(
      (conv1): Conv1d(80, 384, kernel_size=(3,), stride=(1,), padding=(1,))
      (conv2): Conv1d(384, 384, kernel_size=(3,), stride=(2,), padding=(1,))
      (embed_positions): Embedding(1500, 384)
      (layers): ModuleList(
        (0-3): 4 x WhisperEncoderLayer(
          (self_attn): WhisperAttention(
            (k_proj): Linear(in_features=384, out_features=384, bias=False)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (out_proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (activation_fn): GELUActivation()
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): WhisperDecoder(
      (embed_tokens): Embedding(51865, 384, padding_idx=50257)
      (embed_positions): WhisperPositionalEmbedding(448, 384)
      (layers): ModuleList(
        (0-3): 4 x WhisperDecoderLayer(
          (self_attn): WhisperAttention(
            (k_proj): Linear(in_features=384, out_features=384, bias=False)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (out_proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (activation_fn): GELUActivation()
          (self_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): WhisperAttention(
            (k_proj): Linear(in_features=384, out_features=384, bias=False)
            (v_proj): Linear(in_features=384, out_features=384, bias=True)
            (q_proj): Linear(in_features=384, out_features=384, bias=True)
            (out_proj): Linear(in_features=384, out_features=384, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (final_layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
    )
  )
  (proj_out): Linear(in_features=384, out_features=51865, bias=False)
)
```

1. 一些舊的組件簡單回顧
-------------

Whisper 的架構其實很多部分你應該都不陌生，如果你對 Transformer 有基本認識的話。這邊快速回顧一下幾個熟面孔：

*   `Attention(self_attn)`：就是自注意力機制。
*   `Pre-LN residual`：也就是在 LayerNorm 前先加殘差連接（像是 `encoder_attn_layer_norm`、`final_layer_norm`）。
*   `FFN(fc1、fc2)`：前饋神經網路，包含兩層線性變換。

### 一、WhisperAttention

講到 Attention，我們來仔細看一下 Whisper 的注意力模組是怎麼實作的。整體邏輯其實跟一般 Transformer 差不多，都是多頭注意力的結構，比較特別的一點是，Whisper 的線性投影層沒有加 bias，也就是說在 `W*x + b` 裡面，這邊把 `b` 拿掉了，這樣做可能會讓模型更簡潔，或是訓練更穩定一些。

```
class WhisperAttention(nn.Module):
    # 與 HF 對齊：q/k/v/out 使用 bias=False
    def __init__(self, embed_dim, num_heads, attn_dropout=0.0, resid_dropout=0.0):
        super().__init__()
        if embed_dim % num_heads != 0:
            raise ValueError("embed_dim must be divisible by num_heads")
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scale_attn = 1.0 / math.sqrt(self.head_dim)

        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False)

        self.attn_dropout = nn.Dropout(attn_dropout)
        self.resid_dropout = nn.Dropout(resid_dropout)

    def _shape(self, x, bsz, tgt_len):
        return x.view(bsz, tgt_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)

    def forward(self, hidden_states, key_value_states=None, attention_mask=None, causal_mask=None):
        bsz, tgt_len, _ = hidden_states.size()
        kv = hidden_states if key_value_states is None else key_value_states

        q = self.q_proj(hidden_states)
        k = self.k_proj(kv)
        v = self.v_proj(kv)

        q = self._shape(q, bsz, tgt_len)
        k = self._shape(k, bsz, kv.size(1))
        v = self._shape(v, bsz, kv.size(1))

        attn_scores = torch.matmul(q, k.transpose(-1, -2)) * self.scale_attn
        if causal_mask is not None:
            attn_scores = attn_scores.masked_fill(causal_mask == 0, float("-inf"))
        if attention_mask is not None:
            attn_scores = attn_scores + attention_mask

        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.attn_dropout(attn_probs)

        context = torch.matmul(attn_probs, v)
        context = context.transpose(1, 2).contiguous().view(bsz, tgt_len, self.embed_dim)

        out = self.out_proj(context)
        out = self.resid_dropout(out)
        return out
```

### 二、LayerNorm 與 FFN

接著來談 LayerNorm 和 FFN。Transformer 的每一層通常都會包著 Attention 跟 FFN，而這些模塊的前後都會套上一層 LayerNorm。這樣的設計目的是讓模型的輸出分佈比較穩定，避免梯度爆炸或消失。在 Whisper 裡面用的是所謂 Pre-LN 架構，這是目前很多強化版 Transformer 模型常用的做法。

```
self.self_attn_layer_norm = nn.LayerNorm(embed_dim, eps=eps)
self.encoder_attn_layer_norm = nn.LayerNorm(embed_dim, eps=eps)
```

FFN 的結構就比較簡單了，基本上就是先把輸入維度放大（用一個線性層），再通過一個激活函數（通常是 GELU），最後再投影回原本的維度

```
self.fc1 = nn.Linear(embed_dim, config.encoder_ffn_dim, bias=True)
self.fc2 = nn.Linear(config.encoder_ffn_dim, embed_dim, bias=True)
```

這邊後續我們會看到實際的前項傳播，這裡先告訴你們該怎麼宣告。

### 三、WhisperEncoderLayer

在 Whisper 中一層 Encoder 主要包含了 self-attention 和 FFN 這兩大塊。這一層會先對輸入做 LayerNorm，然後進行自注意力計算，再把注意力的輸出加回原始輸入，形成第一個殘差連接。接著它會再做一次 LayerNorm，把資料丟進 FFN 裡做特徵轉換，轉換完的結果也會再加回前面的輸出，形成第二個殘差。

```
class WhisperEncoderLayer(nn.Module):
    # 命名對齊 HF：self_attn/self_attn_layer_norm、fc1/fc2、final_layer_norm
    def __init__(self, config):
        super().__init__()
        embed_dim = config.d_model
        n_head = config.encoder_attention_heads
        eps = getattr(config, "layer_norm_eps", 1e-5)

        self.self_attn = WhisperAttention(embed_dim, n_head, attn_dropout=config.attention_dropout, resid_dropout=config.dropout)
        self.self_attn_layer_norm = nn.LayerNorm(embed_dim, eps=eps)

        self.fc1 = nn.Linear(embed_dim, config.encoder_ffn_dim, bias=True)
        self.fc2 = nn.Linear(config.encoder_ffn_dim, embed_dim, bias=True)
        self.activation_fn = _get_act(getattr(config, "activation_function", "gelu"))
        self.dropout = nn.Dropout(config.dropout)

        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=eps)

    def forward(self, x, attention_mask=None):
        x = x + self.self_attn(self.self_attn_layer_norm(x), attention_mask=attention_mask, causal_mask=None)
        y = self.final_layer_norm(x)
        y = self.fc2(self.activation_fn(self.fc1(y)))
        y = self.dropout(y)
        x = x + y
        return x
```

簡單來說這整個架構基本上就是一個標準的Attention + FFN + Pre-LN 設計流程。

2. encoder
----------

OpenAI 原版的 Encoder 採用的是**固定的正弦位置嵌入（sinusoidal positional embedding）**，也就是說，這部分的權重是根據公式算出來的，而且在訓練過程中不會被更新，也不需要學習。

相反地Hugging Face 的版本雖然一開始也是用正弦方式初始化這些位置嵌入，但它是透過 `nn.Embedding` 來實作的，**而這層預設是可訓練的**，當然你可以選擇把這層 Embedding 凍結（也就是不讓它更新權重），讓它維持原本的正弦初始化狀態，不過這麼做其實會失去使用 nn.Embedding 的彈性優勢。

如果你都要把它凍結不動，那倒不如直接使用固定的正弦位置編碼，反而更省記憶體、也不需要額外的參數更新。換句話說，如果不打算讓位置嵌入參與訓練，選擇 nn.Embedding 就有點多此一舉。

```
class WhisperEncoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        d_model = config.d_model
        num_mel = config.num_mel_bins
        eps = getattr(config, "layer_norm_eps", 1e-5)

        self.conv1 = nn.Conv1d(num_mel, d_model, kernel_size=3, stride=2, padding=1, bias=True)
        self.conv2 = nn.Conv1d(d_model, d_model, kernel_size=3, stride=2, padding=1, bias=True)

        self.embed_positions = nn.Embedding(config.max_source_positions, d_model)
        self.dropout = nn.Dropout(config.dropout)

        self.layers = nn.ModuleList([WhisperEncoderLayer(config) for _ in range(config.encoder_layers)])
        self.layer_norm = nn.LayerNorm(d_model, eps=eps)
```

而在模型的前向傳播過程中，一開始輸入的聲音資料（經過轉換後的 log-Mel 頻譜圖）會先通過兩層一維卷積（1D convolution）。這兩層卷積的設計其實滿直觀的第一層保持時間解析度不變，主要是做特徵提取，第二層則使用了 stride=2，把時間軸壓縮，也就是讓每一步代表的時間範圍變寬，進一步減少後面 Transformer 模組要處理的序列長度。

```
def forward(self, input_features, attention_mask=None, output_hidden_states=False):
        x = input_features.transpose(1, 2)
        x = F.gelu(self.conv1(x))
        x = F.gelu(self.conv2(x))
        x = x.transpose(1, 2)

        B, T_enc, _ = x.size()
        if T_enc > self.config.max_source_positions:
            raise ValueError(f"Encoder sequence length {T_enc} exceeds max_source_positions {self.config.max_source_positions}")

        pos = torch.arange(T_enc, device=x.device, dtype=torch.long).unsqueeze(0).expand(B, T_enc)
        x = x + self.embed_positions(pos)
        x = self.dropout(x)

        if attention_mask is not None:
            attention_mask = _downsample_mask(attention_mask, times=2, stride=2)
        ext_mask = _make_extended_attn_mask(attention_mask, x.dtype) if attention_mask is not None else None

        all_hidden = [] if output_hidden_states else None
        for layer in self.layers:
            if output_hidden_states:
                all_hidden.append(x)
            x = layer(x, attention_mask=ext_mask)

        x = self.layer_norm(x)
        if output_hidden_states:
            all_hidden.append(x)
        return x, all_hidden
```

這樣做的好處是，前面這段卷積不只幫忙做了特徵抽象，還順便降低了計算負擔，讓模型可以用比較少的資源處理長語音。簡單來說，就是先用卷積把聲音濃縮一下，再交給 Transformer 去處理比較高層的語言邏輯。

3. Decoder
----------

Whisper 的 Decoder大致上就是一個從文字 token 開始，一步一步地產生輸出的過程。每一層在做事情時，會同時考慮兩個方向的資訊：一邊是它自己目前已經生成的文字（這部分是透過 self-attention 完成的），另一邊則是來自 Encoder（編碼器）那邊的語音特徵（用 cross-attention 處理）。這樣設計的目的是要讓模型能夠把語音訊號正確對應到文字上。

在self-attention這一塊，模型會去理解目前已經產生的文字序列上下文。不過因為這是一個生成任務，所以會加上一種叫做 causal mask 的機制。模型在生成某個 token 時，只能參考它之前看到的文字，而不能看未來還沒產生的內容。接著是 cross-attention，也就是去參考從 Encoder 傳過來的聲音資訊，最後它會經過一個 FFN做向量轉換，讓輸出更有意義。

```
class WhisperDecoderLayer(nn.Module):
    # 命名對齊 HF：self_attn/encoder_attn + fc1/fc2 + final_layer_norm
    def __init__(self, config, max_positions):
        super().__init__()
        embed_dim = config.d_model
        n_head = config.decoder_attention_heads
        eps = getattr(config, "layer_norm_eps", 1e-5)

        self.self_attn = WhisperAttention(embed_dim, n_head, attn_dropout=config.attention_dropout, resid_dropout=config.dropout)
        self.encoder_attn = WhisperAttention(embed_dim, n_head, attn_dropout=config.attention_dropout, resid_dropout=config.dropout)

        self.self_attn_layer_norm = nn.LayerNorm(embed_dim, eps=eps)
        self.encoder_attn_layer_norm = nn.LayerNorm(embed_dim, eps=eps)

        self.fc1 = nn.Linear(embed_dim, config.decoder_ffn_dim, bias=True)
        self.fc2 = nn.Linear(config.decoder_ffn_dim, embed_dim, bias=True)
        self.activation_fn = _get_act(getattr(config, "activation_function", "gelu"))
        self.dropout = nn.Dropout(config.dropout)

        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=eps)

        mask = torch.tril(torch.ones((max_positions, max_positions), dtype=torch.bool))
        self.register_buffer("causal_mask", mask[None, None, :, :], persistent=False)

    def forward(self, x, encoder_hidden_states, self_attn_mask=None, cross_attn_mask=None):
        B, T, _ = x.size()
        causal = self.causal_mask[:, :, :T, :T]
        x = x + self.self_attn(self.self_attn_layer_norm(x), attention_mask=self_attn_mask, causal_mask=causal)
        x = x + self.encoder_attn(
            self.encoder_attn_layer_norm(x),
            key_value_states=encoder_hidden_states,
            attention_mask=cross_attn_mask,
            causal_mask=None,
        )
        y = self.final_layer_norm(x)
        y = self.fc2(self.activation_fn(self.fc1(y)))
        y = self.dropout(y)
        x = x + y
        return x
```

這樣一層一層疊上去，其實整體設計跟現在很多語言模型滿像的。唯一的差別就是加了 cross-attention，這讓 Decoder 不只是靠前面文字來猜接下來的內容，還能根據語音資訊來決定怎麼產生正確的文字。也正因為這樣，Whisper 的 Decoder 本質上就是一個可以學各種語言輸出的系統。你可以把它想成是一個文字產生器，但靈感來源是你的聲音，而不是一段文字。

這也是為什麼 Whisper 可以同時處理語音轉文字、語音翻譯，甚至處理多國語言因為它的 Decoder 很靈活，能夠根據語音特徵產出各種語言的文字內容。

```
class WhisperDecoder(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        d_model = config.d_model
        eps = getattr(config, "layer_norm_eps", 1e-5)

        self.embed_tokens = nn.Embedding(config.vocab_size, d_model)
        self.embed_positions = nn.Embedding(config.max_target_positions, d_model)
        self.dropout = nn.Dropout(config.dropout)

        self.layers = nn.ModuleList([WhisperDecoderLayer(config, max_positions=config.max_target_positions) for _ in range(config.decoder_layers)])
        self.layer_norm = nn.LayerNorm(d_model, eps=eps)

    def forward(self, input_ids, encoder_hidden_states, decoder_attention_mask=None, encoder_attention_mask=None, output_hidden_states=False):
        B, T_dec = input_ids.size()
        if T_dec > self.config.max_target_positions:
            raise ValueError(f"Decoder seq len {T_dec} exceeds max_target_positions {self.config.max_target_positions}")

        pos = torch.arange(T_dec, device=input_ids.device, dtype=torch.long).unsqueeze(0).expand(B, T_dec)
        x = self.embed_tokens(input_ids) + self.embed_positions(pos)
        x = self.dropout(x)

        ext_dec_mask = _make_extended_attn_mask(decoder_attention_mask, x.dtype) if decoder_attention_mask is not None else None
        ext_enc_mask = _make_extended_attn_mask(encoder_attention_mask, x.dtype) if encoder_attention_mask is not None else None

        all_hidden = [] if output_hidden_states else None
        for layer in self.layers:
            if output_hidden_states:
                all_hidden.append(x)
            x = layer(x, encoder_hidden_states, self_attn_mask=ext_dec_mask, cross_attn_mask=ext_enc_mask)

        x = self.layer_norm(x)
        if output_hidden_states:
            all_hidden.append(x)
        return x, all_hidden
```

看到這邊應該開始有點開竅的感覺了吧？其實一開始看 Attention 架構可能會覺得有點複雜，但當你一層一層拆開來看，會發現它們的組成就那幾個固定的套路，這類模型的架構大致上就繞不開幾個核心元件：

*   **Embedding**：把原始輸入（不管是文字還是其他形式的資料）轉成模型看得懂的向量。
*   **Encoder / Decoder**：這兩者的角色不同，但內部結構都逃不出 Attention 和 FFN 的循環。 
    *   裡面會有 **Self-Attention** 處理序列內的關聯
    *   **Cross-Attention**（只有在 Decoder 中才有）用來連結 Encoder 的輸出
    *   再加上 **Feed Forward Network** 做非線性轉換
    *   最後加上 **LayerNorm** 做穩定處理（有的架構放前面叫 pre-LN，有的放後面叫 post-LN）。

說白了這些大型模型雖然名字多、功能強，但核心就是這幾塊在組合變形。越看越多你就會開始發現：欸？這不就是 Transformer 套路的某個變形嗎？

下集預告
----

隨著我們一路介紹到現在，可以發現模型的架構其實越講越大，但也越來越清楚它們是怎麼運作的。理解這些基礎後，明天我們會進一步討論一個很實用的主題，怎麼透過比較另類的微調方式，來加速模型的訓練流程。也就是說不用從頭訓練一個龐大的模型，我們也能有效調整它，讓訓練成本更低、效率更高。因此明天我會帶你一步一步看，該怎麼實際訓練出一個中文語音模型。

---

<a id="day-24"></a>

## Day 24｜【Day 24】LoRA 是什麼？一篇文章教你 Whisper 中文微調全流程！
- 原文：https://ithelp.ithome.com.tw/articles/10394982

前言
--

今天我們要來聊聊 LLM 的微調技巧。因為 Whisper 是一個參數量非常大的模型，所以我們會簡單介紹一下什麼是 QLoRA，還有怎麼在程式裡面進行量化，並轉換成 QLoRA 的格式。那就讓我們一起來看看，要怎麼微調一個中文的 ASR Whisper 模型吧。

QLoRA簡介
-------

`QLoRA（Quantized Low-Rank Adaptation）`是一種專為大型語言模型設計的高效微調技術，旨在顯著降低訓練過程中的參數量與計算成本。它巧妙地結合了`量化（Quantization）`與`低秩適應（Low-Rank Adaptation）`兩種方法，實現了資源節省與模型表現之間的平衡。

![Image 1: https://ithelp.ithome.com.tw/upload/images/20241013/20152236d4DgEc5Iaw.png](images/series-7467/day-29/20152236d4DgEc5Iaw-608086d8ce3f7cb6.png)

QLoRA 的核心作法是在將原始神經網路進行量化並凍結其參數後，額外加上一個外掛模組 `Adapter（適配器）`。這樣的設計背後有其必要性，由於模型權重經過量化，從高精度格式（如 float64）轉為較低精度（如 float32 甚至更低），雖然顯著降低了記憶體消耗，卻也可能犧牲部分精度。為了補償這種潛在損失，Adapter 被用來模擬參數更新的能力，使模型在保持輕量的同時，仍能維持良好的學習與泛化效果。這種策略不僅提升了微調效率，也大幅擴展了大型模型在資源受限環境中的應用潛力。

1. 設定量化參數
---------

我們首先使用 `BitsAndBytesConfig` 來設定量化相關的參數。這次選擇的是 4-bit 量化，也就是將模型中的部分浮點數權重轉換成更小的格式，以此降低記憶體的使用量。

```
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 啟用 4-bit 量化
    bnb_4bit_compute_dtype=torch.float16,  # 運算時用 float16，速度與精度兼顧
    bnb_4bit_use_double_quant=True,  # 啟用雙層量化，進一步壓縮
    bnb_4bit_quant_type='nf4'  # 使用 nf4（Normalized Float 4）作為量化方式
)
```

接下來我們載入 `whisper-large-v3-turbo` 模型，並將前面設定好的量化參數套用到模型中。這個過程非常簡單，只需要將設定好的參數作為引數傳入即可。

```
base_model = AutoModelForSpeechSeq2Seq.from_pretrained(
    'openai/whisper-large-v3-turbo',
    quantization_config=quant_config,
    torch_dtype=torch.float16,
    use_cache=False
)
```

如果我們 print 出模型結構，可以發現許多原本的 Linear 層已經被替換成了 Linear4bit。這代表這些層的權重如今都已經轉換成 4-bit 格式起來會像這樣：

```
WhisperForConditionalGeneration(
  (model): WhisperModel(
    (encoder): WhisperEncoder(
      (conv1): Conv1d(128, 1280, kernel_size=(3,), stride=(1,), padding=(1,))
      (conv2): Conv1d(1280, 1280, kernel_size=(3,), stride=(2,), padding=(1,))
      (embed_positions): Embedding(1500, 1280)
      (layers): ModuleList(
        (0-31): 32 x WhisperEncoderLayer(
          (self_attn): WhisperSdpaAttention(
            (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)
            (v_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
            (out_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (activation_fn): GELUActivation()
          (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)
          (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    )
    (decoder): WhisperDecoder(
      (embed_tokens): Embedding(51866, 1280, padding_idx=50257)
      (embed_positions): WhisperPositionalEmbedding(448, 1280)
      (layers): ModuleList(
        (0-3): 4 x WhisperDecoderLayer(
          (self_attn): WhisperSdpaAttention(
            (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)
            (v_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
            (out_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
          )
          (activation_fn): GELUActivation()
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (encoder_attn): WhisperSdpaAttention(
            (k_proj): Linear4bit(in_features=1280, out_features=1280, bias=False)
            (v_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
            (out_proj): Linear4bit(in_features=1280, out_features=1280, bias=True)
          )
          (encoder_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1): Linear4bit(in_features=1280, out_features=5120, bias=True)
          (fc2): Linear4bit(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
    )
  )
  (proj_out): Linear(in_features=1280, out_features=51866, bias=False)
```

實際上他們的轉換方式大致就是透過類似下面這樣的步驟，逐一將模型中的每個 nn.Linear 層替換成對應的 Linear4bit 模組。這裡的 Linear4bit 跟 PyTorch 裡常用的 nn.Linear 最大的差別就是它的權重格式不同。nn.Linear 使用的是 float32 或 float16，而 Linear4bit 則是使用更壓縮的 4-bit 格式，就是你把原本用 float32 的大胖模型換成了壓縮過的瘦身模型，跑起來比較快、佔的記憶體也少，對部署來說非常實用。

2. 載入QLoRA Adapter
------------------

我們現在要做的事情，是讓一個大型語言模型準備好進行`低位元量化訓練（k-bit training）`。這個做法可以大幅節省記憶體、提升訓練效率，特別是在 GPU 資源有限的情況下非常實用。而這裡的關鍵步驟就是使用 `prepare_model_for_kbit_training` 這個方法。這個函式會幫我們做幾件很重要的事情，來讓模型進入可訓練、可量化的狀態：

啟用 gradient checkpointing 時，系統預設會啟用以下幾項優化策略，首先模型在前向傳播階段通常會保留每一層的`中間值（activations）`，以便於後續的反向傳播。不過當啟用 gradient checkpointing 後，僅會儲存關鍵節點的中間值，其餘部分則在反向傳播時再動態重新計算，以節省記憶體。

其次為了避免不必要的運算資源浪費，系統會自動凍結那些不需要參與訓練的參數，例如 LayerNorm 或 embedding 層中原本就不打算更新的部分。最後還會根據執行環境，自動設定資料型別（`dtype`）與運算設備（`device`），進一步提升執行效率與穩定性。

```
# 準備模型支援量化訓練
base_model = prepare_model_for_kbit_training(base_model, use_gradient_checkpointing=False)
```

接著我們建立一個 LoRA 訓練的設定檔，這個設定檔會指定：

*   r: 低秩矩陣的維度（越大代表模型容量越大，你可以把它當成參數量的概念）
*   lora_alpha: 控制訓練過程中權重的縮放程度
*   lora_dropout: LoRA 層的 dropout 比例
*   bias: 是否也訓練 bias（這裡我們設定為 'none'，代表不訓練）
*   target_modules: 指定哪些模組要加上 LoRA 層，例如 q_proj, v_proj 是 transformer attention 裡的查詢和鍵值投影層。

```
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
)

# 建立 LoRA 配置
lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.05,
    bias='none',
    target_modules=['q_proj', 'v_proj']
)
```

當我們用 `get_peft_model` 這個函式時，其實就是在模型上加上像 LoRA 或其他 PEFT 的層，讓它可以被訓練。它會根據你給的 `peft_config` 插入像 LoRA 這種 adapter 模組，然後把這些 adapter 設定成可訓練的，其他原本的模型參數就會被凍結起來不動。這樣一來只訓練 adapter 的部分，就可以達到快速又節省資源的微調效果。

```
# 加入 LoRA 模型
model = get_peft_model(base_model, lora_config)
```

3. 載入資料
-------

接下來我們需要從 [GitHub](https://github.com/AUSTIN2526/learning-wx-b-in-30-days) 下載這次要用的資料集，下載完之後我們會用 `librosa` 這個音訊處理套件來讀取音檔。

在讀取的時候，有兩個重要的設定要注意：

1.   **`sr=16000`**：這表示我們把音檔的取樣率（sampling rate）設成 16kHz。這個設定在語音處理領域非常常見，尤其是像 Whisper 這樣的語音辨識模型，**它要求輸入的音訊一定得是 16kHz**，否則模型無法正確處理。透過這個參數，我們可以在讀取時直接把音檔轉換成 16kHz，確保後續處理流程順利進行。
2.   **`mono=True`**：這表示不管原始音檔是單聲道還是立體聲，我們都會把它轉成`單聲道（mono）`，這樣處理起來比較一致也省記憶體。

簡單來說就是用 `librosa` 幫我們讀音檔，確保格式是統一的、適合後面模型使用。

```
import os
import pandas as pd
import librosa
from tqdm import tqdm

def load_dataset(audio_dir, transcript_file='ASR_CN.csv', target_sr=16000):
    df = pd.read_csv(transcript_file, encoding='utf-8-sig')
    df['path'] = df['ID'].apply(lambda x: os.path.join(audio_dir, x))

    print(f'>>> 共有 {len(df)} 筆紀錄，開始載入音訊...')

    audio_list = []
    sentence_list = []

    for _, row in tqdm(df.iterrows(), total=len(df), desc="載入音訊中", unit="記錄"):
        wav_path = row['path']
        sentence = row.get('sentence', '').strip()
        audio, sr = librosa.load(wav_path, sr=target_sr, mono=True)

        audio_list.append(audio)
        sentence_list.append(sentence)

    return audio_list, sentence_list

audios, sentences = load_dataset('audio', 'ASR_CN.csv', target_sr=16000)
```

4. 載入特徵抽取器
----------

在 Whisper 模型中，`WhisperProcessor` 是專門設計來處理語音資料的工具，它融合了`特徵擷取（feature extraction）`和 tokenizer 的功能。在 Whisper 中不直接處理`聲音波形（raw waveform）`，而是透過 `WhisperFeatureExtractor` 將聲音轉換成對應的`對數梅爾頻譜圖（log-Mel spectrogram）`。這個格式能保留語音的音高與語調特徵，是模型理解聲音的基礎。

在Whisper 的 tokenizer 會在輸入序列中加上特定的控制 token，這些 token 讓模型知道它要做什麼任務、處理哪種語言、是否需要加入標點或時間戳。例如：

*   `<|en|>`：代表這段語音是英文。
*   `<|transcribe|>`：代表這是語音轉文字（ASR）任務。
*   `<|notimestamps|>`：指示模型不要在輸出中加入時間戳。
*   `<|startoftranscript|>`：代表轉錄的開始。
*   `<|endoftext|>`：代表文本結束。

像是如果我們要告訴模型：「我要開始轉錄英文語音，任務是語音轉文字，請不要加時間戳。」可以這樣撰寫

```
input_tokens = ["<|startoftranscript|>", "<|en|>", "<|transcribe|>", "<|notimestamps|>"]
```

而模型對應的輸出則會像是

```
output_tokens = ["▁Hello", "▁world", "!", "<|endoftext|>"]
```

其中 `▁` 是空格的標記（代表 subword tokenization），而 `<|endoftext|>` 告訴系統這是文本結尾。而這些Token我們可以直接在一開始就使用`AutoProcessor`進行設定。

```
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained(
    'openai/whisper-large-v3-turbo',
    predict_timestamps=False,
    task="transcribe",
    language='zh'
)
```

5. 建立Pytorch DataLoader
-----------------------

Whisper 模型在輸入的時候，需要三樣東西：

*   input_features：語音特徵（像是 Mel spectrograms）
*   attention_mask：注意力遮罩，用來標示哪些部分是有效輸入
*   labels：實際的文字標註（也就是我們要模型學會產生的輸出）

這些資料不會一開始就剛好符合 PyTorch DataLoader 的格式，所以我們得自訂一個類別來整理這些資料。

```
from torch.utils.data import Dataset, DataLoader

class SpeechSeq2SeqDataset(Dataset):

    def __init__(self, input_features, attention_masks, sentences, processor):
        assert len(input_features) == len(attention_masks) == len(sentences)
        self.input_features = input_features
        self.attention_masks = attention_masks
        self.sentences = sentences
        self.processor = processor

    def __len__(self):
        return len(self.input_features)

    def __getitem__(self, idx):
        return {
            "input_features": self.input_features[idx],
            "attention_mask": self.attention_masks[idx],
            "sentence": self.sentences[idx],
        }

    def collate_fn(self, batch):
        input_feats = torch.stack([item['input_features'] for item in batch])
        attention_masks = torch.stack([item['attention_mask'] for item in batch])
        sentences = [item['sentence'] for item in batch]

        # 處理 target：tokenizer 編碼句子
        tok = self.processor.tokenizer(
            sentences,
            padding=True,
            return_tensors='pt',
            return_attention_mask=True
        )

        # 對非 padding 的部分保留，其他設為 -100 以供 loss 使用
        labels = tok['input_ids'].masked_fill(tok['attention_mask'].ne(1), -100)

        return {
            'input_features': input_feats,
            'attention_mask': attention_masks,
            'labels': labels
        }

from sklearn.model_selection import train_test_split

# 拆分資料
feat_train, feat_valid, attn_train, attn_valid, sent_train, sent_valid = train_test_split(
    input_features, attention_mask, sentences, train_size=0.8, random_state=2526, shuffle=True
)

# 建立 Dataset
train_dataset = SpeechSeq2SeqDataset(feat_train, attn_train, sent_train, processor)
valid_dataset = SpeechSeq2SeqDataset(feat_valid, attn_valid, sent_valid, processor)

# DataLoader
train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=train_dataset.collate_fn)
valid_loader = DataLoader(valid_dataset, batch_size=4, shuffle=False, collate_fn=valid_dataset.collate_fn)
```

在寫程式的邏輯上其實跟我們用 GPT 的時候滿像的。先在 `collate_fn` 裡把語音的特徵跟 attention mask 疊成一個 batch，然後把文字丟給 tokenizer 把它轉成數字。為了讓模型專心學有意義的部分，我們會把 padding 的地方設成 -100，這樣在算 loss 的時候就會自動跳過那些沒內容的地方。

6. 訓練模型
-------

同樣的這次整合了 AdamW 優化器與 cosine warmup 學習率調整策略。但要記得 `is_lora=True` 時，表示模型啟用了 LoRA 格式。在此設定下若使用舊有的模型儲存方式，將會導致錯誤，因此必須使用與 LoRA 相容的保存與載入方式。

```
from trainer import Trainer
import torch.optim as optim
from transformers import get_cosine_schedule_with_warmup

# 總步數 = epoch 數 * 每個 epoch 的 batch 數
num_training_steps = len(train_loader) * 100  # 100 是總 epoch 數
num_warmup_steps = int(0.2 * len(train_loader))  # 可調整 warmup 比例

optimizer = optim.AdamW(model.parameters(), lr=1e-4)
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=num_warmup_steps,
    num_training_steps=num_training_steps,
)
trainer = Trainer(
    epochs=100,
    train_loader=train_loader,
    valid_loader=valid_loader,
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    early_stopping=5,
    load_best_model=True,
    grad_clip=1.0,
    is_lora=True
)

trainer.train(show_loss=True)
```

輸出結果：

```
Train Epoch 5: 100%|██████████| 200/200 [02:35<00:00,  1.28it/s, loss=0.123]
Valid Epoch 5: 100%|██████████| 50/50 [00:15<00:00,  3.13it/s, loss=0.611]
Train Loss: 0.24830 | Valid Loss: 0.54996 | Best Loss: 0.51839

Train Epoch 6: 100%|██████████| 200/200 [02:36<00:00,  1.28it/s, loss=0.061]
Valid Epoch 6: 100%|██████████| 50/50 [00:15<00:00,  3.13it/s, loss=0.600]
Train Loss: 0.21451 | Valid Loss: 0.56744 | Best Loss: 0.51839

Train Epoch 7: 100%|██████████| 200/200 [02:36<00:00,  1.28it/s, loss=0.048]
Valid Epoch 7: 100%|██████████| 50/50 [00:15<00:00,  3.14it/s, loss=0.601]
Train Loss: 0.18471 | Valid Loss: 0.58050 | Best Loss: 0.51839
```

![Image 2: https://ithelp.ithome.com.tw/upload/images/20251008/20152236kaSSyzxGvD.png](images/series-8357/day-24/20152236kaSSyzxGvD-a8542e23e13d2ddd.png)

在訓練過程中模型的訓練損失初期較高，約為 1.0，但隨著訓練推進損失穩定下降。到了第 7 個 epoch，訓練損失已降至約 0.2，顯示模型對訓練資料的擬合能力明顯提升。但驗證損失的變化趨勢則有所不同：儘管初期從約 0.55 降至 0.5 左右，之後卻出現逐步上升，特別是在第 4 到第 7 個 epoch 間，上升趨勢更加明顯。這現象暗示模型在驗證資料上的泛化能力開始退化，代表出現過度擬合。這種情況在預訓練模型中相當常見，反映出這類模型即便在小資料集上也能迅速學習特定模式，卻也因此更容易過度擬合。

下集預告
----

好啦今天的 Whisper 介紹就先告一段落啦，也代表你現在已經掌握 Transformer 的架構，還有預訓練模型的基本概念了。不簡單欸。今天我們也小小地踏進了 LLM 的微調世界，學了一種滿基礎但超實用的方法，叫做 LoRA。

那明天呢，我們來輕鬆一點，聊聊什麼是 prompt。不同的 prompt 類型又有什麼差別？我會慢慢帶你看，從最早的 prompt 到現在這些花招百出的技巧，它們是怎麼一步一步演進而來的。

---

<a id="day-25"></a>

## Day 25｜【Day 25】語言模型的認知轉向，GPT 系列中的提示學習與指令學習解析

- 原文：https://ithelp.ithome.com.tw/articles/10395520
- 發佈時間：2025-10-09 23:56:21

前言
==

自從 GPT-2 問世以來，OpenAI 很快就推出了功能更強大的 GPT-3這個版本不只是模型參數暴增，連訓練資料的規模也大幅提升。但真正令人注意的是，從 GPT-3 開始，透過提示來引導模型的做法突然爆紅，幾乎成為與大型語言模型互動的主要方式。某種程度上，這也象徵著語言模型的發展邁入了一個全新的階段。

Few-shot 與 Zero-shot 是什麼
========================

其實在 GPT-2 時期，模型就已經展現出一點多功能的潛力，雖然沒經過特別訓練卻能處理不同的任務。不過真正讓 `Zero-shot Learning` 成為焦點並證實其可行性的是 GPT-3。簡單來說這種學習方式的驚人之處在於，即使模型從未接觸過特定任務，它仍能依靠語言知識進行推論與判斷。

打個比方就像聽人說斑馬長著黑白條紋，生活在非洲草原雖然沒親眼看過，大概也能想像牠的模樣。GPT-3 的推論方式正是如此，透過語意的拼湊補足知識的缺口。例如若問 GPT-3：「一封申請大學的動機信應該包含哪些內容？」即使它從沒接受過這類任務的訓練，也能根據語言知識與常識推理出如個人背景、學術目標、以及對該學校的了解等要素。

相比之下 `Few-shot Learning` 的作法則是在提示中提供幾個範例，讓模型看過幾題後就能上手，不需額外訓練。這種方式又稱為 `In-Context Learning`，也就是模型透過當下的語境做出即時判斷。例如：若給 GPT-3 三組「問答配對」的例子，再問它第四題，它就能依照前面範例的格式與邏輯，自行產出正確答案。

模型越大 Few-shot 效果越明顯？
====================

研究發現一個有趣的現象，模型越大 Few-shot 的表現通常越出色。大型模型在語言理解與模式識別上具備優勢，即使只提供一兩個例子，它在翻譯或問答等任務中也能明顯提升表現。

這一點在實務上非常關鍵，尤其在資料稀少、註記成本高的情況下，Few-shot 能夠有效減少對資料的依賴，卻依然能產出有水準的結果。

在 GPT-3 的使用中，`Prompt Learning` 幾乎成了整套技術的核心。簡單來說提示的設計會影響模型的表現。只要在輸入前加入明確的任務描述，例如要求翻譯時寫上請將以下句子從中文翻成英文，模型便能立刻切換模式，反應自然也更精準。實驗結果顯示，這樣的提示設計對模型的理解力與輸出品質有明顯提升，尤其在處理語言任務時更是如此。有時候，一句關鍵的提示語就足以讓結果高下立判。從這個角度看，提示不只是開啟模型的指令，更是我們與其溝通的橋梁。

Prompt Learning 和 Instruction Learning 有何不同
===========================================

GPT-3 的問世在自然語言處理領域引發極大關注，因為它在許多語言任務中的表現已經與人類相當接近。然而，它並非毫無缺陷，像是偶爾產出具攻擊性或涉及隱私的內容，這些風險必須正視。

為了解決這些問題，OpenAI 推出了 InstructGPT，也就是針對 GPT-3 再進一步調整的版本。它引入了一種名為 `Instruction Learning` 的方法，目的是讓模型更準確理解人類需求。這種做法與早期依賴提示語操作的方式有所不同。

GPT-3 的使用方式大致是透過提示語提供範例引導模型，例如給出背景資訊、問題與答案的格式，讓模型依樣畫葫蘆。雖然這種方式有效，但效果很依賴提示設計的精細度，也較不具彈性。

而 InstructGPT 採用更清楚的指令說明，像是直接告訴模型根據以下內容回答問題，甚至可以加入限制條件，例如請避免使用冒犯或涉及隱私的詞語。這樣的設計不僅有助於模型掌握語境，也能有效降低產出不當內容的風險。

從訓練角度來看，Prompt Learning 比較像提供許多範例讓模型學著模仿；而 Instruction Learning 則進一步告訴它任務的本質與原則。這不僅是格式上的轉變，更是模型從模仿語言走向理解意圖的過程。

總的來說從 GPT-3 的 Prompt Learning 到 InstructGPT 的 Instruction Learning，這條發展路徑不只是操作上的演進，更是大型語言模型對人類意圖理解能力的一大飛躍。過去我們透過反覆舉例引導模型猜測任務，如今則是以清楚的任務說明直接對話，這不只提升效率，也有助於避免潛在的誤解與風險。未來如何設計與語言模型互動的方式，不論是提示語還是任務指令，或許將成為一種嶄新的溝通技藝。

下集預告
====

既然對於 Prompt 的運作方式已經有了完整的認識，那麼下一步便是進入大型語言模型的具體實踐層面。而其中一個不可不提的重要代表，就是由 Meta 所推出的 LLaMA 系列。這套模型的誕生不只是技術上的突破，更某種程度上代表了開源力量在語言模型領域的一次集體反擊。

那麼LLaMA 究竟在技術上做了哪些關鍵調整？又為什麼它能在不到 GPT-3 規模的情況下，展現媲美甚至超越的表現？這一點就是我們明天要深入探討的重點。

---

<a id="day-26"></a>

## Day 26｜【Day 26】GPT 落伍了嗎？來看看 LLaMA 怎麼反向壓制參數怪獸
- 原文：https://ithelp.ithome.com.tw/articles/10395690

前言
--

自從 GPT 系列爆紅之後大家一提到大型語言模型，腦中浮現的幾乎都是那幾個熟悉的縮寫 GPT-2、GPT-3、GPT-4⋯⋯ 但有趣的是這幾年另一條技術支線正在快速崛起，並以更少的參數、更快的推理效率，打出了媲美甚至超越 GPT 的性能。這條支線的主角之一正是 Meta 所推出的 LLaMA 模型系列。

LLaMA 的設計理念幾乎反其道而行不是一昧堆疊參數，而是透過精巧的架構優化、數學設計與訓練策略，達到小而強的模型效果。你可能會好奇它怎麼做到的？又為什麼越來越多研究者和開發者開始轉向 LLaMA 生態系？

今天這篇文章就帶你一探究竟，從 RMSNorm、SwiGLU、RoPE 到 GQA，一步步拆解 LLaMA 的底層設計，看它如何在不走傳統套路的情況下，重塑大型語言模型的技術格局。

從 2023 年 2 月 LLaMA 問世以來，Meta 已推出多個版本的改進型模型，每一代皆具備獨特的架構創新與設計理念，以下為各版本的概覽：

*   **LLaMA 1（2023.02）**：採用 Decoder-only Transformer 架構，核心技術包括 RMSNorm、SwiGLU 以及 RoPE，支援原生 2K 上下文長度。常見的模型參數規模有 7B、13B、33B 與 65B，奠定了 LLaMA 架構的基礎。

*   **LLaMA 2（2023.07）**：在訓練資料規模與品質上有所提升，並同步推出針對對話應用與商用授權的版本。延續前代核心技術，並引入 **Grouped-Query Attention（GQA）**，有效減少 KV cache 占用。原生支援 4K 上下文，模型尺寸涵蓋 7B、13B 與 70B。

*   **LLaMA 3（2024.04）**：初期推出 8B 與 70B 版本，原生支援 8K 上下文。這一代強調使用大規模、乾淨的語料，並透過更加嚴謹的後訓練流程（如 SFT 與 DPO）來提升模型表現。

*   **LLaMA 3.1（2024.07）**：新增 405B 的超大模型版本，將上下文長度擴展至 128K，並全面採用 GQA。此版本著重於強化長上下文推理能力與工具使用的整合性，展現出更強大的實用性與泛化能力。

而架構中的 `RMSNorm`、`SwiGLU`、`RoPE` 和 `GQA` 這些技術，現在幾乎已經成了 LLM 的基本配備，幾乎每個新模型都少不了它們，因此我們現在來特別了解一下這些架構是基於何種的理由進行改動，並他與原始究竟差異在哪裡。

1.RMSNorm
---------

RMSNorm 是一種用來取代 LayerNorm 的正規化技術，主要目的是提升模型的運算效率與梯度穩定性。與 LayerNorm 不同的是，**RMSNorm 不會計算輸入的平均值**，而是專注於根據輸入的`均方根（RMS）`進行縮放。

簡單來說它的運作方式如下。首先將每個輸入值平方，接著計算這些平方值的平均，再開根號得到 RMS 值。

![Image 1: https://ithelp.ithome.com.tw/upload/images/20251010/20152236Ndwjr4SGNF.png](images/series-8357/day-26/20152236Ndwjr4SGNF-1e4b353d24e36f23.png)

然後將原始輸入除以該 RMS，達到穩定整體數值的效果，最後再乘上一組可學習的縮放參數，使模型能根據任務需求自動調整輸出尺度。

![Image 2: https://ithelp.ithome.com.tw/upload/images/20251010/20152236IxBItL3HXi.png](images/series-8357/day-26/20152236IxBItL3HXi-5a3a9aa61c95572e.png)

這組參數為逐維度的縮放向量，不僅簡化了計算流程，省去均值與方差的處理，也有助於保持反向傳播時的梯度穩定。在 LLaMA 架構中，則採用了 Pre-Norm 策略，將 RMSNorm 放置於注意力層與前饋神經網路，進一步提升訓練的穩定性。而在程式中我們可以如此表示。

```
import torch
import torch.nn as nn

class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        # x: (B, T, C)
        rms = x.pow(2).mean(dim=-1, keepdim=True).add(self.eps).sqrt()
        x_hat = x / rms
        return self.weight * x_hat
```

2.SwiGLU 前饋層
------------

當我們在講 Transformer 架構裡的 FFN 傳統的做法就是兩層線性轉換，中間夾一個像 ReLU 或 GELU 這樣的激勵函數。這種設計其實蠻直覺的但它有個問題，激勵函數是直接套用在整個中間層輸出上，沒辦法幫我們選擇哪些資訊比較重要，導致模型在處理複雜表達時比較不靈活。

用比較簡單的方式來看，整個 FFN 就像這樣運作先做一次線性轉換，套個激勵函數，再做一次線性轉換，因此公式可以寫成樣：

![Image 3: https://ithelp.ithome.com.tw/upload/images/20251010/201522361RLIofRF0o.png](images/series-8357/day-26/201522361RLIofRF0o-f4a607d62ec3c4f6.png)

但這樣的設計它不會告訴你：「欸，這個資訊有用，那個沒用」。在深度學習裡當我們想要讓模型自己決定哪些特徵該留下，通常的做法就是用 `Wx + b`，讓它透過參數自己學。所以這時就出現了 SwiGLU 這個比較新穎的前饋層設計。

![Image 4: https://ithelp.ithome.com.tw/upload/images/20251010/20152236fVb66yq2Oz.png](images/series-8357/day-26/20152236fVb66yq2Oz-048cd1edc18a726d.png)

這個方法的核心在於引入一種閘控機制，概念上與我們學過的 LSTM 中使用 `sigmoid(Wx + b)` 的結構相似，用來判斷哪些訊號該被保留、哪些該被抑制。SwiGLU 的實作方式，是將中間層的輸出切成兩半其中一半直接作為主要訊號保留，另一半則經過 Swish 函數處理後，作為閘門訊號使用。

這個閘門負責調整哪些訊號應該被強化，哪些應該被抑制，等於替模型增加了一層訊息過濾的能力，使其在表達複雜關係時更具彈性，也更容易聚焦於關鍵特徵。對應的程式碼實作如下，展示了如何用 PyTorch 實現 SwiGLU 的前饋計算邏輯：

```
import torch
import torch.nn as nn
import torch.nn.functional as F

class SwiGLU(nn.Module):
    def __init__(self, d_model, d_ff):
        super().__init__()
        self.linear1 = nn.Linear(d_model, d_ff)  # 主訊號
        self.linear2 = nn.Linear(d_model, d_ff)  # 閘門訊號
        self.output_proj = nn.Linear(d_ff, d_model)  # 最終輸出維度投影

    def swish(self, x):
        return x * torch.sigmoid(x)

    def forward(self, x):
        gate = self.swish(self.linear2(x))       # 閘門經 Swish
        signal = self.linear1(x)                 # 主訊號
        fused = signal * gate                    # 逐元素相乘
        return self.output_proj(fused)           # 投影回輸出維度
```

3.RoPE
------

`RoPE（Rotary Positional Embedding）`是一種相當有趣且逐漸成為主流的位置信息編碼方式，它徹底改變了我們過去處理位置的方法。傳統的 Positional Encoding 通常是將一組 sin/cos 值加進詞向量中，等於是替每個詞貼上位置標籤。但 RoPE 採取的是完全不同的策略它不加而是轉進向量中。

更具體地說RoPE 將位置資訊以旋轉的方式直接作用在注意力機制中的 Query 和 Key 向量上。可以想像一下原始的向量是一根箭頭，而位置資訊就像是在多維空間中給這根箭頭旋轉一個角度，讓每個位置的向量指向不同的方向。這種旋轉式融合會讓模型更自然地感知相對位置，特別在處理長距離依賴的文本時，效果相當顯著。

![Image 5: https://ithelp.ithome.com.tw/upload/images/20241012/20152236N5jEGqYRwK.png](images/series-7467/day-28/20152236N5jEGqYRwK-ce17f726003af9cd.png)

當我們看圖中上半部的時候，其實它就是在講 RoPE 是怎麼動手處理這些向量的，假設現在我們只看一小部分的向量，也就是 Query 或 Key 裡面的一對維度首先 θ₁ 是根據位置算出來的一個角度，就有點像以前 Positional Encoding 用 sin/cos 去搞出的那些週期性訊號。然後紅色的 m 就是你目前這個詞的位置（比如說是第 1 個字、第 2 個字…這樣）。

接著你原本那個向量是`(x₁, x₂)`，RoPE 就是拿那個位置算出來的角度`mθ₁`，然後把這個向量整個旋轉一下。想像一下你在平面上拿著一根箭頭，把它轉個角度，方向就變了，但它還是同樣的長度。而這個轉過角度的新向量 `(x′₁, x′₂)`，就是經過 RoPE 編碼後的版本。這種方式不只加上了位置感，而且還讓每個位置的向量指向不一樣的方向，這對模型來說非常有幫助，因為它就能更靈活地理解 **誰跟誰的距離感** 這種語言特性。

![Image 6: https://ithelp.ithome.com.tw/upload/images/20251010/201522366n97Vz1vKF.png](images/series-8357/day-26/201522366n97Vz1vKF-b666a1a693089494.png)

在數學實作上，RoPE 結合了 sin 與 cos 函數所構成的旋轉基底（這點與傳統方法相似），圖像上你可以把它想成，對於每一維的特徵，RoPE 都是在複數平面上轉了一圈而這個角度由位置決定。值得注意的是，它所使用的參數 θ 和傳統 Positional Encoding 中的 `10000^(2i/d)`結構其實很接近，只是 RoPE 沒有把它當成加法項處理，而是作為旋轉角度使用。這也意味著 RoPE 可以自然保留向量之間的相對位置信息，並在注意力內積的過程中持續發揮作用，而這樣子的好處是，**因此注意力可天然編碼相對距離 m−n** 使其知道序列之間的距離，而原始的RoPE我們可以如此撰寫

```
import torch

class RoPEOriginal:
    def __init__(self, seq_len, dim, base=10000.0, device="cuda"):
        # 確保維度是偶數，因為 RoPE 會將維度分成兩半處理
        assert dim % 2 == 0
        half = dim // 2

        # 建立索引，用於計算不同頻率的旋轉角度
        idx = torch.arange(half, device=device)

        # 計算每個維度對應的旋轉頻率比例 θ
        # theta = base^(-2i/dim)
        theta = base ** (-2 * idx / dim)

        # 建立序列位置 pos，形狀為 [seq_len, 1]
        pos = torch.arange(seq_len, device=device).float().unsqueeze(1)

        # 計算位置與頻率的乘積角度矩陣 [seq_len, dim/2]
        angles = pos * theta.unsqueeze(0)

        # 預先儲存 cosine 與 sine 值，供後續旋轉使用
        self.cos = angles.cos()  # [seq_len, dim/2]
        self.sin = angles.sin()  # [seq_len, dim/2]

    def apply(self, x):
        # x 的形狀為 [batch, seq_len, dim]
        # 將最後一個維度拆分成偶數與奇數索引兩部分
        x1, x2 = x[..., ::2], x[..., 1::2]

        # 將 cos 和 sin 的形狀擴展以便與 x 對齊
        # 形狀變為 [1, seq_len, 1, dim/2]
        cos = self.cos.unsqueeze(0).unsqueeze(2)
        sin = self.sin.unsqueeze(0).unsqueeze(2)

        # 套用旋轉位置編碼公式
        # x1p = x1 * cos - x2 * sin
        # x2p = x1 * sin + x2 * cos
        x1p = x1 * cos - x2 * sin
        x2p = x1 * sin + x2 * cos

        # 將旋轉後的結果重新拼接回 [batch, seq_len, dim]
        return torch.stack([x1p, x2p], dim=-1).flatten(-2)
```

其實你仔細看程式碼就會發現，原本的 RoPE 是先把整個 cos/sin 的表格都算好，這種做法在序列長度不長、模型又比較小的時候還算 OK。但一旦進入長序列訓練或推理這樣的表格就會使用超多記憶體。LLaMA 2 為了解決這問題，就改成「動態計算」cos 跟 sin，不再預先建立整張表。還有一點原本的頻率縮放是用 `10000^(2i/d)`，但 LLaMA 2 把它換成了 `base^(−d/2i)`，其實兩種寫法在數學上是等價的，只是後者看起來更簡潔，還直接表達出每半個維度頻率會降低的這個特性。

```
import torch
import torch.nn.functional as F

class RoPELlama2:
    def __init__(self, dim, base=10000.0, device="cuda"):
        assert dim % 2 == 0, "dim 必須為偶數"
        half = dim // 2

        # 頻率比例 (inv_freq)，根據維度遞減
        # θ_i = base^(-2i/dim)
        self.inv_freq = base ** (-torch.arange(0, half, device=device).float() / half)
        self.device = device

    def get_cos_sin(self, seq_len):
        # 建立位置索引 [seq_len]
        pos = torch.arange(seq_len, device=self.device).float()

        # 計算每個位置的角度 pos * inv_freq -> [seq_len, dim/2]
        angles = torch.einsum('i,j->ij', pos, self.inv_freq)

        # cos, sin 形狀 [seq_len, dim/2]
        cos = angles.cos()
        sin = angles.sin()
        return cos, sin

    def apply_rotary(self, x, cos, sin):
        # 將維度拆成兩半 (even, odd)
        x1 = x[..., ::2]
        x2 = x[..., 1::2]

        # 擴展 cos, sin 尺寸匹配
        cos = cos.unsqueeze(0).unsqueeze(2)  # [1, seq_len, 1, head_dim/2]
        sin = sin.unsqueeze(0).unsqueeze(2)

        # 套用旋轉公式
        x_rotated_even = x1 * cos - x2 * sin
        x_rotated_odd = x1 * sin + x2 * cos

        # 合併回原始形狀
        return torch.stack([x_rotated_even, x_rotated_odd], dim=-1).flatten(-2)
```

在 LLaMA 3 中，為了支援極長上下文 RoPE 的設計再最關鍵的變化是將旋轉頻率的 base 值從 LLaMA 2 的 10000 提升至 500000，這樣的調整使得角度變化的頻率下降得更慢，進而讓模型在面對長距離的 token 時仍能保持穩定且可區分的相對位置信息。

> 由於 sin 與 cos 本質上是週期函數，當序列長度變得非常長時，若 base 選得過小，會出現位置編碼繞回來的現象，使得序列尾端的位置信息與開頭產生混淆。而提升 base 的設定，正是為了拉長這樣的週期，避免長序列尾端出現與序列開頭環環相扣的錯位對齊問題，從而確保模型能穩定地捕捉遠距依賴關係。

```
import torch

class RoPELLama3:
    def __init__(self, head_dim, max_seq_len=4096, base=500000.0, device="cuda", dtype=torch.float32):
        # head_dim 必須為偶數
        assert head_dim % 2 == 0
        self.dim = head_dim
        self.device = device
        self.dtype = dtype

        # LLaMA 3 採用較大的 base（500000）以支援長上下文
        half = head_dim // 2
        idx = torch.arange(half, device=device, dtype=dtype)
        inv_freq = 1.0 / (base ** (idx / half))  # 頻率倒數，用於控制角度變化速度

        # 建立位置張量 [seq_len, 1]
        pos = torch.arange(max_seq_len, device=device, dtype=dtype).unsqueeze(1)

        # 角度矩陣 [seq_len, dim/2]
        angles = pos * inv_freq.unsqueeze(0)

        # 儲存 cosine/sine 值供後續使用
        self.register_buffers(angles)

    def register_buffers(self, angles):
        self.cos_cached = angles.cos()  # [seq_len, dim/2]
        self.sin_cached = angles.sin()  # [seq_len, dim/2]

    def apply_rotary_emb(self, x, seq_len=None):
        if seq_len is None:
            seq_len = x.shape[1]

        # 取對應長度的 cos/sin
        cos = self.cos_cached[:seq_len].unsqueeze(0).unsqueeze(2)  # [1, seq_len, 1, dim/2]
        sin = self.sin_cached[:seq_len].unsqueeze(0).unsqueeze(2)

        # 拆分偶數與奇數索引
        x1, x2 = x[..., ::2], x[..., 1::2]

        # 旋轉操作
        x1p = x1 * cos - x2 * sin
        x2p = x1 * sin + x2 * cos

        # 合併回 [batch, seq_len, num_heads, head_dim]
        return torch.stack([x1p, x2p], dim=-1).flatten(-2)
```

並且你可以看到 LLaMA 3 不再在每次 forward pass 中動態生成 sin 與 cos 表格，而是在初始化時就根據預設的最大序列長度（如 4096 或更長）預先計算好整張角度矩陣並緩存起來。這種方式在推理時只需從快取中擷取對應長度的部分，兼顧了執行效率與記憶體使用。

4.Grouped-Query Attention
-------------------------

`Grouped-Query Attention（GQA）` 的核心想法是讓多個查詢（Query）頭共用較少數量的鍵（Key）和值（Value）頭。假設有 `H` 個 attention 頭，我們可以將它們分成 `G` 組，讓每組共用同一組 Key 和 Value。這麼做的好處包括：

*   **計算成本下降**：只需要為 G 組計算 K 和 V，而非 H 組。
*   **記憶體使用量減少**：降低了儲存 Key 和 Value 的需求。

在 LLaMA 系列模型中ROPE 是套用在 `q` 和 `k` 上，因此在此實作中我們也使用該作法。

```
class GQAAttention(nn.Module):
    def __init__(self, dim, num_heads, num_kv_heads, rope_base=500000.0, max_seq_len=4096):
        super().__init__()
        assert num_heads % num_kv_heads == 0  # 確保 Query 頭數能被 KV 頭數整除（好做分組）
        
        self.dim = dim  # 輸入特徵維度
        self.h = num_heads  # Query 總頭數
        self.kvh = num_kv_heads  # KV 頭數
        self.head_dim = dim // num_heads  # 每個 attention 頭的維度

        # 線性轉換層：生成 Q, K, V
        self.wq = nn.Linear(dim, dim, bias=False)  # 為所有 Q 頭產生 Q
        self.wk = nn.Linear(dim, self.kvh * self.head_dim, bias=False)  # 為 G 組產生 K
        self.wv = nn.Linear(dim, self.kvh * self.head_dim, bias=False)  # 為 G 組產生 V
        self.wo = nn.Linear(dim, dim, bias=False)  # 輸出映射層

        # Rotary Positional Embedding：LLaMA 風格的位置編碼
        self.rope = RoPELLama3(self.head_dim, max_seq_len=max_seq_len, base=rope_base)

    def forward(self, x, mask=None):
        B, T, C = x.shape  # B: batch size, T: sequence length, C: hidden dim

        # 產生 Q, K, V，並 reshape 成多頭格式
        q = self.wq(x).view(B, T, self.h, self.head_dim)
        k = self.wk(x).view(B, T, self.kvh, self.head_dim)
        v = self.wv(x).view(B, T, self.kvh, self.head_dim)

        # 套用 Rotary Positional Embedding 到 Q 和 K 上
        q = self.rope.apply_rotary_emb(q, seq_len=T)
        k = self.rope.apply_rotary_emb(k, seq_len=T)

        # 將較少的 KV 頭複製，使其能與所有 Q 頭對應
        group_size = self.h // self.kvh  # 每組共享多少 Q 頭
        k = k.repeat_interleave(group_size, dim=2)
        v = v.repeat_interleave(group_size, dim=2)

        # 計算注意力分數
        attn_scores = torch.einsum("bthd,bThd->bhtT", q, k) / math.sqrt(self.head_dim)
        
        # 如果有 mask，將無效位置設為 -inf 以避免注意力聚焦
        if mask is not None:
            attn_scores = attn_scores.masked_fill(mask == 0, float("-inf"))
        
        # 計算 softmax 注意力權重
        attn = torch.softmax(attn_scores, dim=-1)
        
        # 使用注意力權重加權 V 並輸出
        out = torch.einsum("bhtT,bThd->bthd", attn, v).contiguous().view(B, T, C)
        return self.wo(out)
```

到這裡為止我們可以清楚地看到，目前的大型語言模型在設計上已進行了多項技術層面的革新，而這些改良往往不只是單純的效能優化，更是針對原有方法進行深度的重構。像是 RoPE 的演進過程便是一個鮮明的例子從原始版本中的預算旋轉表格，到 LLaMA 2 採用的動態計算策略，再到 LLaMA 3 透過提升 base 值來穩定長距離表徵，這些變化雖然在實作上大幅度偏離了傳統 Positional Encoding 的架構，但其核心概念仍舊保留在其中。

這些創新不是完全拋棄舊技術，而是在其原理的基礎上，針對現代模型的需求進行了極具針對性的強化與轉化。這種保留骨幹、重構細節的策略，幾乎成為了所有AI模型的演化方式。

下集預告
----

今天我們已經把 LLaMA 這個語言模型的架構拆解完畢，讓大家對它的內部運作有了初步了解。那明天呢我們會進一步教你們怎麼從零開始建立一整個 LLaMA 模型，還會帶你操作怎麼登入 Hugging Face、取得權限，還有其他實用功能。

接下來我們也會陸續介紹 base 版本跟 chat 版本的建構方式，以及怎麼優化推理速度、提升效能等重要資訊。這些通通都會在之後的內容中告訴你們，就敬請期待囉！

我們明天見～

---

<a id="day-27"></a>

## Day 27｜【Day 27】RoPE(x) = cosθx + sinθ(-x)？LLaMA 3 的 Wx + b 的完整拆解
- 原文：https://ithelp.ithome.com.tw/articles/10396096

前言
--

今天這篇文章我們就要從 HuggingFace 的 LLaMA 3 實作出發，帶大家完整解析其內部架構與運作邏輯。特別聚焦在 Transformer 模型裡最常見也最重要的推論加速技巧 KV cache 的運作方式。我們會一步步拆開 RoPE 的位置編碼設計、GQA 如何降低計算成本、KV 快取如何避免重複運算，同時實際帶你看看它們的 PyTorch 程式碼長什麼樣子。

先來談談 LLaMA 3 的整體架構它的參數規模非常的高。在8B參數量的模型中，它支援最多 **128256 個輸入 token**，而像是 embedding、FFN、Attention 模組等部分的參數也都比前代大幅提升。再加上整整 **32 層 Decoder**，不難看出這是一個相當重型的模型。從 HuggingFace 的模型結構來看，`LlamaForCausalLM` 類別裡包含了主要的模型與語言模型頭（`lm_head`），而主體架構可大致拆解如下：

```
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
```

在這個結構中我們是看不到位子設計的，過去許多模型都會在 `embedding` 階段就把位置資訊加進去，例如使用 sinusoidal 或 learned positional embedding。但 LLaMA 3 採取的路線是embedding 階段只專注於詞彙本身的向量表示，位置資訊則完全交由 **RoPE 搭配 GQA** 在 Attention 計算階段動態處理。這種做法雖然設計上更複雜，但好處是彈性高且更符合實際語境中的 token 排列邏輯。

不過今天的主角不只有這兩個設計，還有一個跟推論效率息息相關的元件 **KV cache**。該方式簡單來說當模型用於聊天或文本生成時，它每次推論只會產生一個新 token。若每次都重新計算所有的 Query、Key、Value，那效率會大打折扣。KV cache 的做法是把已經算好的 K 和 V 快取起來，下一次生成時就可以直接使用，省下重複計算的時間和資源。

所以在今天我們將會一步步拆解如何為一個大型語言模型設計一套高效、可擴展的 KV cache 機制。

1. RoPE
-------

在這裡 RoPE 的實作我們昨天已經知道該如何進行了，也就是透過餘弦與正弦角度生成方式，建立可快取的旋轉張量，再將這些角度作用到向量上，實現位置嵌入：

```
class RotaryEmbedding(nn.Module):
    """
    RoPE cache in cos/sin。與 HF 相同的 rope_theta。
    """
    def __init__(self, dim, max_position_embeddings=8192, base=10000.0, device=None):
        super().__init__()
        self.dim = dim
        self.base = base
        self.max_position = max_position_embeddings
        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, device=device).float() / dim))
        self.register_buffer("inv_freq", inv_freq, persistent=False)
        self._set_cos_sin_cache(max_position_embeddings, device)

    def _set_cos_sin_cache(self, seq_len, device):
        t = torch.arange(seq_len, device=device, dtype=self.inv_freq.dtype)  # [T]
        freqs = torch.einsum("i,j->ij", t, self.inv_freq)  # [T, dim/2]
        emb = torch.cat([freqs, freqs], dim=-1)  # [T, dim]
        self.register_buffer("cos_cached", emb.cos()[None, None, :, :], persistent=False)  # [1,1,T,dim]
        self.register_buffer("sin_cached", emb.sin()[None, None, :, :], persistent=False)  # [1,1,T,dim]

    def forward(self, seq_len_needed):
        """
        回傳 cos/sin cache（直到需要的 seq_len）。
        只負責提供索引所需長度，實際 slice 在 attention 內完成。
        """
        if (self.cos_cached is None) or (seq_len_needed > self.cos_cached.size(2)):
            new_len = max(seq_len_needed, (self.max_position * 2 if self.max_position else 16384))
            self._set_cos_sin_cache(new_len, device=self.inv_freq.device)
        return self.cos_cached[:, :, :seq_len_needed, :], self.sin_cached[:, :, :seq_len_needed, :]
```

同樣地 LLaMA 3 使用 `inv_freq` 這個變數來為不同維度建立頻率變化，這是實現 RoPE 的第一步。接下來它會根據這些頻率，事先為每一個可能的位置準備好對應的 **cos/sin 向量**。這些向量會被**快取（cache）**起來，避免在每次推論時重複運算。

這樣一來每當模型需要嵌入位置資訊時，就能直接從快取中取出對應的旋轉角度，快速完成計算。同時這套設計也支援 **動態長度擴展**，能靈活應對不同長度的輸入序列。不過真正把這些位置資訊應用到 token 向量上的關鍵，其實藏在下面這個函式中：

```
def apply_rotary_pos_emb(x, cos, sin):
    x1, x2 = x[..., : x.size(-1) // 2], x[..., x.size(-1) // 2 :]
    x_rot = (x * cos) + (torch.cat([-x2, x1], dim=-1) * sin)
    return x_rot
```

這樣一來就算模型本身沒用絕對位置編碼，它依然能夠根據這些相對位置信息，理解整個序列的順序。

當然可以，這段說明我幫你口語化整理如下：

2. GQA + KV cache
-----------------

複習一下傳統的多頭注意力機制，基本上一般的做法是 Q、K、V這三個的頭數都是一樣的，也就是說如果你有 H 個注意力頭，那 Q、K、V 都會各有 H 個對應的頭。但 GQA 保留了 Q 有 H 個頭不變，但是把 K 跟 V 的頭數減少了，可能只保留原來的四分之一或八分之一的數量。那這樣少了怎麼辦？很簡單，**就是把這些比較少的 K/V 頭「重複使用」，讓每個 Q 頭都還是能跟它們互動。**

這樣做有幾個明顯的好處：

1.   **省記憶體**：因為 K/V 的矩陣小了很多。
2.   **跑得快**：資料量變少，速度自然就上來了。
3.   **表現依然不錯**：即便簡化了，在 decoder-only 的模型裡效果也還是很穩定。

再來看看程式碼的部分：

```
def _repeat_kv(self, x):
    if self.num_kv_heads == self.num_heads:
        return x
    repeat = self.num_heads // self.num_kv_heads
    return x.repeat_interleave(repeat, dim=1)
```

這段程式碼的意思是如果 K/V 的頭數和 Q 一樣多，那就直接回傳原本的資料。否則就用 `repeat_interleave` 把 K/V 重複幾次，湊到跟 Q 一樣多的頭數。這樣輸出的形狀會變成 `(batch_size, num_heads, ...)`，方便接下來做 dot product。

接著談到 **kv cache** 在Transformer的處理，如果模型有使用 cache（像 Hugging Face 的 `use_cache=True`），它就會先檢查之前有沒有存過的 Key/Value。如果有就拿出來；然後再把這次新來的 token 加上之前的，變成一整段更長的序列。

```
if past_key_value is not None:
    past_k, past_v = past_key_value
    T_past = past_k.size(2)
else:
    past_k = past_v = None
    T_past = 0
T_total = T_past + t
```

然後它就會把舊的和新的 K/V 合併起來（`concat`），存到 `present` 變數裡下次還可以接著用。

```
if past_k is not None:
    k_cat = torch.cat([past_k, k_new], dim=2)
    v_cat = torch.cat([past_v, v], dim=2)
else:
    k_cat, v_cat = k_new, v
present = (k_cat, v_cat) if use_cache else None
```

這樣一來不管是延續上下文還是加快推論速度，因此整個GQA我們可以如此撰寫程式碼。

```
class LlamaAttention(nn.Module):
    def __init__(self, config):
        super().__init__()
        hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.num_kv_heads = getattr(config, "num_key_value_heads", self.num_heads)
        if self.num_heads % self.num_kv_heads != 0:
            raise ValueError("num_attention_heads must be divisible by num_key_value_heads for GQA.")
        self.head_dim = hidden_size // self.num_heads
        self.scale = 1.0 / math.sqrt(self.head_dim)

        # Projections: 形狀匹配 HF
        self.q_proj = nn.Linear(hidden_size, self.num_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(hidden_size, self.num_kv_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(hidden_size, self.num_kv_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, hidden_size, bias=False)

        attn_pdrop = getattr(config, "attention_dropout", 0.0)
        resid_pdrop = getattr(config, "hidden_dropout", 0.0)
        self.attn_dropout = nn.Dropout(attn_pdrop)
        self.resid_dropout = nn.Dropout(resid_pdrop)

        # RoPE
        rope_theta = getattr(config, "rope_theta", 10000.0)
        max_pos = getattr(config, "max_position_embeddings", 8192)
        self.rotary_emb = RotaryEmbedding(self.head_dim, max_position_embeddings=max_pos, base=rope_theta)

        # 預建上三角 causal mask；必要時動態擴張
        mask = torch.triu(torch.ones((max_pos, max_pos), dtype=torch.bool), diagonal=1)
        self.register_buffer("causal_mask", mask[None, None, :, :], persistent=False)  # [1,1,T,T]

    def _repeat_kv(self, x):
        # x: [B, kv_heads, T, D] -> 重複到 [B, heads, T, D]
        if self.num_kv_heads == self.num_heads:
            return x
        repeat = self.num_heads // self.num_kv_heads
        return x.repeat_interleave(repeat, dim=1)

    def _grow_causal_mask(self, tgt_len, device):
        if self.causal_mask.size(-1) < tgt_len:
            new_max = max(tgt_len, self.causal_mask.size(-1) * 2)
            mask = torch.triu(torch.ones((new_max, new_max), dtype=torch.bool, device=device), diagonal=1)
            self.causal_mask = mask[None, None, :, :]

    def forward(self, x, attention_mask=None, past_key_value=None, use_cache=False):
        B, t, _ = x.size()
        device = x.device

        # 新片段投影
        q = self.q_proj(x)  # [B, t, H*D]
        k = self.k_proj(x)  # [B, t, KV*D]
        v = self.v_proj(x)  # [B, t, KV*D]

        q = q.view(B, t, self.num_heads, self.head_dim).permute(0, 2, 1, 3)      # [B, H, t, D]
        k = k.view(B, t, self.num_kv_heads, self.head_dim).permute(0, 2, 1, 3)  # [B, KV, t, D]
        v = v.view(B, t, self.num_kv_heads, self.head_dim).permute(0, 2, 1, 3)  # [B, KV, t, D]

        # === KV Cache：取出 past，並計算總長度 ===
        if past_key_value is not None:
            past_k, past_v = past_key_value  # [B, KV, T_past, D]
            T_past = past_k.size(2)
        else:
            past_k = past_v = None
            T_past = 0
        T_total = T_past + t  # K/V 的最終長度（包含 past + 新片段）

        # RoPE：取得到 T_total 的 cos/sin，並切出「新片段的 t 行」
        self._grow_causal_mask(T_total, device=device)
        cos_full, sin_full = self.rotary_emb(T_total)  # [1,1,T_total,D]
        cos = cos_full[:, :, T_total - t : T_total, : q.size(-1)]  # [1,1,t,D]
        sin = sin_full[:, :, T_total - t : T_total, : q.size(-1)]  # [1,1,t,D]

        # 套用 RoPE（僅對新片段）
        q = apply_rotary_pos_emb(q, cos, sin)          # [B, H, t, D]
        k_new = apply_rotary_pos_emb(k, cos, sin)      # [B, KV, t, D]

        # === KV Cache：拼接 past_k/past_v 與新片段 ===
        if past_k is not None:
            k_cat = torch.cat([past_k, k_new], dim=2)  # [B, KV, T_total, D]
            v_cat = torch.cat([past_v, v],     dim=2)  # [B, KV, T_total, D]
        else:
            k_cat, v_cat = k_new, v

        # 需要回傳 present 以便下次快取
        present = (k_cat, v_cat) if use_cache else None

        # GQA：將 KV 重複到與 H 相同的 head 數
        k_rep = self._repeat_kv(k_cat)  # [B, H, T_total, D]
        v_rep = self._repeat_kv(v_cat)  # [B, H, T_total, D]

        # 注意力計算：Q @ K^T -> [B, H, t, T_total]
        attn_scores = torch.matmul(q, k_rep.transpose(-1, -2)) * self.scale  # [B,H,t,T_total]

        # Causal mask：僅取對應「最後 t 列 x T_total 欄」的區塊，等價於行索引 [T_past: T_total]
        causal_slice = self.causal_mask[:, :, T_total - t : T_total, :T_total]  # [1,1,t,T_total]
        attn_scores = attn_scores.masked_fill(causal_slice, float("-inf"))

        # Padding additive mask（若提供，形狀 [B,1,1,T_total]，可廣播到 [B,H,t,T_total]）
        if attention_mask is not None:
            attn_scores = attn_scores + attention_mask

        attn_probs = F.softmax(attn_scores, dim=-1)
        attn_probs = self.attn_dropout(attn_probs)

        context = torch.matmul(attn_probs, v_rep)  # [B,H,t,D]
        context = context.transpose(1, 2).contiguous().view(B, t, self.num_heads * self.head_dim)  # [B,t,C]
        out = self.o_proj(context)
        out = self.resid_dropout(out)
        return out, present  # === 回傳 present（KV Cache）===
```

3. FFN
------

複習一下在傳統的 Transformer 裡 FFN 通常就是兩層線性層中間加個非線性激活函數形式大概是這樣：

```
FFN(x) = Linear2(activation(Linear1(x)))
```

而在昨天數學公式中我們其實需要**三個線性層** ，在這裡我們先看看程式碼。

```
class LlamaMLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        hidden_size = config.hidden_size
        intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)
        self.dropout = nn.Dropout(getattr(config, "hidden_dropout", 0.0))

    def forward(self, x):
        x_g = F.silu(self.gate_proj(x))
        x_u = self.up_proj(x)
        x = x_g * x_u
        x = self.down_proj(x)
        x = self.dropout(x)
        return x
```

一開始的兩個步驟是這樣的 `gate_proj` 會先把輸入資料拉到一個比較大的維度，然後丟進一個叫 SiLU 的激活函數裡，這樣就產生一個gating 向量，有點像是學出來的一組開關。

接著 `up_proj` 這條線也會把原本的輸入資料投影到一樣大的維度，但它本身不做任何非線性處理。然後這兩條路線的輸出會進行 element-wise 相乘，也就是一個位置對應一個位置來做乘法。這樣一來，`gate_proj` 的輸出就變成了選通器，控制 `up_proj` 的訊號要不要通過。

但因為這樣一放大，維度也會跟著變大，所以我們還得用第三個線性層 `down_proj` 把資料縮回原來的維度，這樣才不會影響到後面的結構或計算量。

4. LLaMA Decoder
----------------

在看 Decoder Layer 的時候，其實我們只需要搞清楚一件事：原本論文是用 pre-normalization 還是 post-normalization 的方法。像在 LLaMA 這個架構裡，Decoder 的設計有個很關鍵的點，就是它採用的是 **pre-normalization**。意思就是，每個子層在運算前，先做正規化。這樣的設計對模型來說有幾個好處，像是更穩定，也比較容易收斂，訓練起來效果會比較好。

```
class RMSNorm(nn.Module):
    # 與 HF LlamaRMSNorm 相同語意
    def __init__(self, hidden_size, eps=1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.eps = eps

    def forward(self, x):
        # x: [..., hidden_size]
        norm = x.pow(2).mean(dim=-1, keepdim=True)
        x = x * torch.rsqrt(norm + self.eps)
        return self.weight * x
```

在 KV cache 裡我們需要設定一個 `past_key_value`，也就是把之前存下來的 key 跟 value 拿來做 attention 計算。然後系統會回傳一個 `present`（也就是這一層在當前時間步算出來的 key/value），之後推論時就可以直接拿來用。這個機制在做 autoregressive decoding（像是逐字產生文字）時特別有用，因為它可以省下重複算前面那些 token 的 attention 的時間。其他部分的設計其實就跟 Decoder 的原則一樣。

```
class LlamaDecoderLayer(nn.Module):
    def __init__(self, config):
        super().__init__()
        eps = getattr(config, "rms_norm_eps", 1e-6)
        self.input_layernorm = RMSNorm(config.hidden_size, eps=eps)
        self.self_attn = LlamaAttention(config)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=eps)
        self.mlp = LlamaMLP(config)

    def forward(self, x, attention_mask=None, past_key_value=None, use_cache=False):
        """
        past_key_value:  # === KV Cache ===
            該層的 (past_k, past_v) 或 None
        use_cache:
            True -> 回傳 present_key_value 供下次使用
        """
        attn_out, present = self.self_attn(
            self.input_layernorm(x),
            attention_mask=attention_mask,
            past_key_value=past_key_value,
            use_cache=use_cache,
        )
        x = x + attn_out
        x = x + self.mlp(self.post_attention_layernorm(x))
        if use_cache:
            return x, present  # === 回傳 present（KV Cache）===
        return x
```

5. LLaMA主架構
-----------

模型的開頭會先經過一個詞嵌入層，這層的作用就是把每個 token 的索引值轉換成向量的形式，讓後面模型能理解這些詞的語意。接著模型會堆疊好幾層 Transformer Decoder，每一層負責進一步處理與理解輸入的上下文資訊。

比較核心的運算邏輯是寫在 `forward` 方法裡，這部分設計得滿彈性的，支援各種不同的輸入輸出選項。特別值得一提的是，在做推論的時候會用到 `past_key_values`，這東西是用來記錄前面步驟的注意力資訊。

```
def _make_additive_attn_mask(attention_mask, dtype):
    """
    將 [B, T_total] mask (1=keep, 0=pad) 轉成加法遮罩 [B, 1, 1, T_total]
    其中 keep=0，masked=-inf，以供 softmax 前相加。
    """
    if attention_mask is None:
        return None
    if attention_mask.dim() != 2:
        raise ValueError("attention_mask must be [batch, seq_len]")
    extended = attention_mask[:, None, None, :]  # [B,1,1,T_total]
    extended = extended.to(dtype=dtype)
    neg_inf = torch.finfo(dtype).min
    return (1.0 - extended) * neg_inf
    
class LlamaModel(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])
        self.norm = RMSNorm(config.hidden_size, eps=getattr(config, "rms_norm_eps", 1e-6))
        self.dropout = nn.Dropout(getattr(config, "hidden_dropout", 0.0))

        self.apply(self._init_weights)

    def _init_weights(self, module):
        # Llama 初始化：normal(0, 0.02)
        if isinstance(module, nn.Linear):
            nn.init.normal_(module.weight, mean=0.0, std=getattr(self.config, "initializer_range", 0.02))
            if module.bias is not None:
                nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            nn.init.normal_(module.weight, mean=0.0, std=getattr(self.config, "initializer_range", 0.02))

    def forward(
        self,
        input_ids,
        attention_mask=None,          # [B, T_total]；含 pad=0 的位置
        past_key_values=None,         # === KV Cache：list(tuple(k,v))，每層一組 ===
        use_cache=False,              # === KV Cache：是否回傳 present ===
        output_hidden_states=False,
        return_dict=False,
    ):
        B, t = input_ids.size()
        x = self.embed_tokens(input_ids)  # [B, t, C]
        x = self.dropout(x)

        # 建立 additive mask（對齊 T_total），若 None 則不加
        ext_mask = _make_additive_attn_mask(attention_mask, x.dtype) if attention_mask is not None else None

        all_hidden_states = [] if output_hidden_states else None
        presents = [] if use_cache else None

        # past_key_values：長度應等於層數；若 None，視為每層皆無 past
        if past_key_values is None:
            past_key_values = [None] * len(self.layers)

        for i, blk in enumerate(self.layers):
            if output_hidden_states:
                all_hidden_states.append(x)

            layer_past = past_key_values[i]  # 該層 past 或 None
            if use_cache:
                x, present = blk(
                    x,
                    attention_mask=ext_mask,
                    past_key_value=layer_past,
                    use_cache=True,
                )
                presents.append(present)  # === 收集 present（KV Cache）===
            else:
                x = blk(
                    x,
                    attention_mask=ext_mask,
                    past_key_value=layer_past,
                    use_cache=False,
                )

        x = self.norm(x)
        if output_hidden_states:
            all_hidden_states.append(x)

        if return_dict:
            return {
                "last_hidden_state": x,
                "hidden_states": all_hidden_states,
                "past_key_values": presents,  # === KV Cache 回傳 ===
            }
        return (x, all_hidden_states, presents)
```

再來談到模型初始化這塊你會注意到它有定義一個 `_init_weights` 的方法，這個方法會自動套用到模型裡所有的線性層和嵌入層上，並用高斯分布（通常是平均為 0、標準差為 0.02）來初始化權重。這種是HF最常見的初始化方式能幫助模型在訓練一開始就比較穩定。

5. LM Head
----------

在這種`因果語言模型（causal language model）`裡，最後通常會接一個叫做LM head的東西。它的工作就是把模型輸出的那些隱藏向量，轉成一個機率分布，簡單來說，就是幫你預測下一個最有可能出現的字是什麼。像這邊提到的 `LlamaForCausalLM`，其實就是把核心的 `LlamaModel` 包起來，再接上一個 `lm_head` 層。這個 `lm_head` 就是一層線性變換，它的輸出大小會對應整個詞彙表，也就是說，模型每預測一個字，就會算出所有詞的分數（logits）。而且通常這個 `lm_head` 的權重，會直接綁定到詞嵌入（`embed_tokens`）那邊的權重，這個技巧在 GPT-2 就有用了，其實在 Transformer 架構裡也滿常見的。

```
class LlamaForCausalLM(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.model = LlamaModel(config)
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        # 權重綁定
        self.lm_head.weight = self.model.embed_tokens.weight

    # HF API helpers
    def get_output_embeddings(self):
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        self.lm_head = new_embeddings

    def forward(
        self,
        input_ids,
        attention_mask=None,
        labels=None,
        past_key_values=None,   # === KV Cache：輸入 past ===
        use_cache=False,        # === KV Cache：是否輸出 present ===
        output_hidden_states=False,
        return_dict=False,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,  # === KV Cache 傳入 ===
            use_cache=use_cache,              # === KV Cache 啟用 ===
            output_hidden_states=output_hidden_states,
            return_dict=True,
        )
        hidden_states = outputs["last_hidden_state"]  # [B, t, C]
        logits = self.lm_head(hidden_states)          # [B, t, vocab]

        loss = None
        if labels is not None:
            # 只對齊自回歸訓練格式
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        if return_dict:
            return {
                "loss": loss,
                "logits": logits,
                "hidden_states": outputs["hidden_states"],
                "past_key_values": outputs["past_key_values"],  # === KV Cache 回傳 present ===
            }
        return (loss, logits, outputs["hidden_states"], outputs["past_key_values"])
```

而模型第一次開始生成文字時，`past_key_values` 是空的（設為 None），所以模型得從頭開始計算整個序列的 Query、Key 跟 Value。但在接下來繼續生成的過程中，只會輸入最新的一個 token，然後把前一次算好的 `past_key_values` 傳進去。這時如果這時有開啟 `use_cache=True`，模型還會把新的 Key/Value（也就是 `present_key_values`）回傳回來，這樣下一步可以繼續接著用，不用每次都從頭來過來增加推理速度。

下集預告
----

明天我會跟大家分享怎麼訓練出屬於自己的聊天機器人，也會帶你們了解現在這些大型語言模型從訓練到實際應用之間，整個流程是怎麼走的。你可以把明天的內容想像成 ChatGPT 從模型設計、訓練，到最後變成一個網站可以用的那個完整過程。而且我也會講一下，從 GPT-3.5 演進到現在的 GPT，公開資料中到底透露了哪些技術和方法。

---

<a id="day-28"></a>

## Day 28｜【Day 28】弱智吧 is all you need？教AI聽懂亂流語言的奇幻旅程
- 原文：https://ithelp.ithome.com.tw/articles/10396537

前言
--

今天我們要來談談一個很好玩的資料集"弱智吧"，沒錯你沒看錯就是那個在網路上以瘋言瘋語、奇思妙想著稱的討論區。乍看之下這種地方的對話充滿跳針、無厘頭，甚至讓人懷疑發言者是不是認真在講話。但也正因如此這類資料特別適合拿來做語言模型的微調訓練，因為它具備了高度非結構化、多樣語境轉換與語言風格突變的特性——這些恰恰是測試與強化模型對話理解能力的絕佳素材。

而在今天我們先告訴你一個LLM怎麼做的，然後再一步步實作如何用"弱智吧"資料，搭配 Chat 模型格式並加入NEFtune這項技術，打造一個不只能理解亂流對話，還能用哲學角度回應你的人文風格聊天機器人。

Instruction 與 RLHF
------------------

我們先前使用的 GPT‑2 是典型的 base 模型。它在文字生成上表現不俗，能接續段落、創作詩句、撰寫簡短故事，但若給它請總結下列文章或進行多輪對話這類指令型任務，它常常跑題，無法準確執行任務或維持對話連貫性。這是因為 base 模型雖具備語言能力，但缺乏任務導向與上下文理解的機制。

到了 GPT‑3，模型參數量大幅提升，使其在翻譯、摘要、問答、寫程式等多種自然語言處理任務上展現更高水準。不過，原始的 GPT‑3 模型仍然只是被動地接續輸入文字，對於明確執行人類指令這件事並不擅長，回應品質也時常忽高忽低、不穩定。

為了改善這個問題，OpenAI 開發了 **InstructGPT**。它是在 GPT‑3 的基礎上加入了一套關鍵訓練流程`指令微調（instruction tuning）`搭配 `RLHF（Reinforcement Learning from Human Feedback）`。RLHF 的主要目的是讓模型學會產出人類認為好的回答

而實際做法是會讓 base 模型針對同一個 prompt 生成多個回答，並請人類標註者對這些回答依品質進行排序。這些排序資料會用來訓練一個 **獎勵模型（Reward Model）**，這個模型能學會模擬人類偏好，對語言模型輸出的文字進行評分。

接著語言模型會利用這個獎勵模型的評分進行**強化學習**，常用的方法是 **PPO（Proximal Policy Optimization）**。透過 PPO，語言模型的生成策略會逐步朝向人類偏好靠攏，例如更清晰、有條理、符合語境或更具禮貌。這樣的調校大幅提升了語言模型的任務執行力與對話品質。從 InstructGPT 開始，模型能更準確地依據指令回應，理解使用者意圖，並避免不當或偏差的內容。這也為 ChatGPT 的誕生奠定基礎。

**ChatGPT（GPT‑3.5）與 GPT‑4**這種透過 RLHF 調教出來的對話能力更加成熟。這些模型不僅能進行上下文連貫的多輪對話，還能維持語氣一致、合理拒絕敏感請求，甚至在對話中穿插幽默或進行自我澄清。

> 簡單來說一個 LLM 的出現過程大致如下，首先從大量語料訓練出一個預訓練的 base 模型，接著透過 instruction tuning 讓模型具備基本的任務理解與指令回應能力，轉化為 Chat 形式。之後，開發者會收集人類標註者針對模型回答的偏好排序，用來訓練一個獎勵模型，使其能對回應進行自動評分。最後模型根據這些評分結果，透過 PPO進行強化學習，逐步學會產出更符合人類期待的回應。

在中文互聯網文化中弱智吧是一個非常獨特的存在。它原本是百度貼吧中的一個子版塊，內容充滿了看似荒謬、邏輯混亂甚至無厘頭的貼文。這些發言的共同特點是：語言誇張、思路跳脫、常見諧音與反諷。一句話形容的話，「越是正常的說法，越不合這個貼吧的胃口」。

乍看之下這樣的內容似乎沒什麼價值，甚至顯得低俗或反智。但有趣的是近年來這類語言風格反而在人工智慧領域引起關注，特別是在中文語言模型的訓練與微調階段。研究者發現與其使用過度乾淨、正規的語料，不如加入這類語義扭曲、邏輯不穩定的文本，讓模型學會如何處理更複雜的語言變體。例如弱智吧中的問題經常帶有雙關、多義、反問或模稜兩可的用詞，這些正好可以訓練模型面對語言的灰色地帶，而今天我們將要用這個資料集進行模型的訓練。

1. 讀取資料
-------

今天我們用的資料一樣是從 m-a-p/COIG-CQIA 這個資料集中提取的弱智吧內容，大家可以直接到我的 [GitHub](https://github.com/AUSTIN2526/learning-wx-b-in-30-days) 頁面下載資料。至於今天要用的模型，是 Chat 版本的，不像之前我們用的 GPT-2 base model 那樣。這個 Chat 模型是透過微調原本的 base model 來實現的，所以它可以更好地理解並判斷多人對話的情境，例如在LLaMA 3上他的特定輸入格式是：

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

這是系統指令<|eot_id|><|start_header_id|>user<|end_header_id|>

這是用戶的輸入<|eot_id|><|start_header_id|>assistant<|end_header_id|>

這是模型回復<|eot_id|>
```

這格式看起來滿複雜的對吧？早期在 HuggingFace上用這類模型的時候，我們還得自己手動加這些 token，真的蠻麻煩的。而且不同的 Chat 模型格式還都不一樣，導致我們寫程式的時候很難統一處理。不過現在比較方便了，只要你用的是 Chat 版本的 tokenizer，它通常都會自帶一個叫做 `apply_chat_template` 的方法，直接就可以把對話格式套進去。這個方法的用法跟現在 ChatGPT API 裡的輸入格式很像，就是一個由多個角色（像 system、user、assistant）組成的訊息列表（messages）。其中 system 就是我們拿來放指令的地方，所以你可以這樣寫程式碼：

```
import pandas as pd

def transform_format(instructions, outputs, system="你是一個繁體中文聊天機器人"):
    data = []
    for q, a in zip(instructions, outputs):
        data.append([
            {"role": "system", "content": system},
            {"role": "user", "content": q},
            {"role": "assistant", "content": a}
        ])
    return data
    
df = pd.read_csv("ruozhiba_trad.csv", encoding="utf-8")
df = df.dropna(subset=["instruction", "output"])
formatted = transform_format(df["instruction"], df["output"])
```

我們先來理解一下關於大型語言模型（LLM）的一個基本概念所謂的 Chat 版本，其實是從 base 版本的 LLM 開始，透過 `SFT（Supervised Fine-Tuning）`這種微調的方式訓練出來的。這個過程同時也會搭配我們在第 25 天講過的 Instruction Learning 技術，讓模型能聽得懂任務的指令，並且學會哪些回答該給、哪些不能亂講。

2. 讀取模型並量化
----------

當我們用 `prepare_model_for_kbit_training` 這個函數來處理模型，其實就是在幫模型做好低位元訓練的準備工作。這一步的主要目的是讓模型在只用少量精度的情況下，還能穩定地訓練，不會因為精度損失導致梯度亂跳、效果變差。順便複習一下這個函數會做幾件事，它會把模型原本的大部分參數凍結起來，這樣可以省下很多資源，然後也會啟用 gradient checkpointing，來進一步節省記憶體用量。

做好這些準備後，我們就可以把 LoRA 模組加進來了，具體來說我們會針對 `k`、`q`、`v`、`o` 這些部分進行訓練，這樣就完成了模型量化跟 LoRA 組件的整合。

```
from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import torch

def load_llama_model(model_name='meta-llama/Meta-Llama-3-8B-Instruct'):
    quantization_params = {
        'load_in_4bit': True,
        'bnb_4bit_quant_type': "nf4",
        'bnb_4bit_use_double_quant': True,
        'bnb_4bit_compute_dtype': torch.bfloat16
    }
    bnb_config = BitsAndBytesConfig(**quantization_params)

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        use_cache=False,
    )

    peft_params = {
        'r': 32,
        'target_modules': ["q_proj", "k_proj", "v_proj", "o_proj"],
        'lora_dropout': 0.1,
        'task_type': "CAUSAL_LM",
    }
    peft_config = LoraConfig(**peft_params)

    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
    model = get_peft_model(model, peft_config)

    return model, tokenizer

model = load_llama_model()
```

3. 加入NEFtune
------------

我們要把 **NEFTune** 技術加進模型裡。簡單來說NEFTune 是一種在訓練期間，針對輸入的嵌入加上一點隨機噪音的小技巧。雖然看起來只是加點 noise，但這其實對訓練很有幫助。為什麼要這麼做？因為在低精度訓練的情況下，模型對輸入變化的敏感度會變得比較高，這就容易讓訓練不穩定。NEFTune 就像是在這種不穩的情況下，給模型多一點彈性，讓它能更穩定地收斂。

根據 [原始論文](https://arxiv.org/abs/2310.05914)，這個方法實際在某些資料集上，甚至可以讓 LLaMA 模型的效能提升接近兩倍，效果非常驚人。這也是為什麼現在越來越多人在進行微調時會主動加上 NEFTune，而在程式上我們可以如此撰寫

```
from transformers.modeling_utils import unwrap_model

def activate_neftune(model, neftune_noise_alpha = 5):
        unwrapped_model = unwrap_model(model)
        embeddings = unwrapped_model.base_model.model.get_input_embeddings()
        embeddings.neftune_noise_alpha = neftune_noise_alpha
        # hook embedding layer
        hook_handle = embeddings.register_forward_hook(neftune_post_forward_hook)
        
        return model
        
def neftune_post_forward_hook(module, input, output):
    # 公式來源:https://github.com/neelsjain/NEFTune
    # 論文網址:https://arxiv.org/abs/2310.05914
    if module.training:
        dims = torch.tensor(output.size(1) * output.size(2))
        mag_norm = module.neftune_noise_alpha / torch.sqrt(dims)
        output = output + torch.zeros_like(output).uniform_(-mag_norm, mag_norm)
            
    return output
model = activate_neftune(model)
```

而在這裡的 `activate_neftune` 函數，主要是負責把 NEFTune 整合進我們的模型。而透過 `unwrap_model` 這個工具，把模型外層的包裝拆掉，取得最底層、也就是實際運作的模型架構。接著我們會定位到模型的輸入嵌入層，這是模型接收文字資料的第一個處理環節。

接下來我們會設定一個參數叫 `neftune_noise_alpha`，這個值決定了噪音的強度。設定好之後我們會在嵌入層上註冊一個 `forward hook`。這個 hook 的功能是在每次模型做前向傳遞時，自動在輸出嵌入上加上一點隨機噪音。

4. 建立Pytorch DataLoader
-----------------------

當我們在使用 PyTorch 的 DataLoader 這塊時，整體流程其實和之前差不多。因為我們前面已經把格式處理好了，所以這邊只要直接套用 `apply_chat_template` 就能完成轉換。而且一樣要記得一件事處理 labels 的時候，要把那些 padding 的地方遮蔽起來，這點我們前面講過好幾次了。因為在訓練像這種 causal language model 時，這個步驟是絕對不能少的。

```
from torch.utils.data import Dataset, DataLoader

# 定義自定義 Dataset
class PTTDataset(Dataset):
    def __init__(self, formatted_context, tokenizer):
        self.formatted_context = formatted_context
        self.tokenizer = tokenizer

    def __getitem__(self, index):
        return self.formatted_context[index]
       
    def __len__(self):
        return len(self.formatted_context)

    def collate_fn(self, batch):
        formatted_contexts = self.tokenizer.apply_chat_template(batch, padding=True, return_dict=True, max_length=8192, return_tensors='pt', truncation=True)
        attention_mask = formatted_contexts['attention_mask']
        labels = formatted_contexts['input_ids'].clone()
        labels[attention_mask == 0] = -100
        formatted_contexts['labels'] = labels
        return formatted_contexts

# 建立資料集
trainset = PTTDataset(formatted, tokenizer)
validset = PTTDataset(formatted, tokenizer)

# 創建 DataLoader
train_loader = DataLoader(trainset, batch_size=4, shuffle=True, collate_fn=trainset.collate_fn)
valid_loader = DataLoader(validset, batch_size=4, shuffle=True, collate_fn=validset.collate_fn)
```

5. 開始訓練模型
---------

為什麼會這樣呢？這是因為 LoRA 的設計本身是輕量化的，只是在模型的一小部分（像是 attention weights）中插入少量可訓練的參數並沒有去動整個模型的主要權重，如果你學習率設得太高，那些少數參數很容易就會發散，讓模型的訓練變得不穩定。

```
import torch.optim as optim
from transformers import get_cosine_with_hard_restarts_schedule_with_warmup
from trainer import Trainer

optimizer = optim.AdamW(model.parameters(), lr=5e-5)
scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=len(train_loader) * 0.2, 
        num_training_steps=len(train_loader) * 10, 
        num_cycles=1, 
)

trainer = Trainer(
    epochs=10, 
    train_loader=train_loader, 
    valid_loader=valid_loader,
    model=model, 
    optimizer=optimizer,
    scheduler=scheduler,
    early_stopping=3,
    is_lora=True
)
trainer.train()
```

輸出結果：

```
rain Epoch 0: 100%|██████████| 60/60 [00:25<00:00,  2.38it/s, loss=1.820]
Valid Epoch 0: 100%|██████████| 60/60 [00:08<00:00,  7.02it/s, loss=1.703]
Saving Model With Loss 1.91710
Train Loss: 1.96181 | Valid Loss: 1.91710 | Best Loss: 1.91710

Train Epoch 1: 100%|██████████| 60/60 [00:25<00:00,  2.39it/s, loss=1.847]
Valid Epoch 1: 100%|██████████| 60/60 [00:08<00:00,  7.02it/s, loss=1.957]
Saving Model With Loss 1.88124
Train Loss: 1.92786 | Valid Loss: 1.88124 | Best Loss: 1.88124
```

5. 比對生成結果
---------

我們用資料集的第一筆資料來做生成和比對。

```
device = "cuda" if torch.cuda.is_available() else "cpu"
chat_input = formatted[0][:-1] 
inputs = tokenizer.apply_chat_template(
    chat_input,
    return_tensors="pt",
    padding=True,
    truncation=True
).to(device)

outputs = model.generate(inputs, max_new_tokens=100)
texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)
reply = texts[0].split(chat_input[-1]["content"])[-1].strip()
print(reply)
```

訓練前的模型回覆長這樣：

```
我們相聚的意義在於創造一個舒適的環境，讓我們一起享用一些美好的時候，例如聚會、旅行、學習、交朋友等。
```

訓練後的模型則是這樣說的：

```
意義在於經驗與成長。相聚是人與人之間交流、理解、影響的過程，其價值不在永恆，而在於曾經共同存在的時間留下了改變。
如莊子所言：「相濡以沫，不如相忘於江湖。」別離不消解意義，只是使它完成。
```

從這兩段可以很明顯看出，模型訓練前後在生成風格上有不小的差異。沒訓練過的模型說話比較直接，像是在描述一個很普通的情境，例如「創造一個舒適的環境」或「一起享用一些美好的時候」，語句通順但比較平淡，內容也偏表面。

而訓練過的模型，講話就不一樣了。不只是用上了「經驗」、「成長」這些抽象概念，還引用了莊子的話，把人際關係拉到哲學的層次去談，整體語氣更有深度，也更有人文氣息。這代表訓練之後的模型，不只是講話變得更有表達力，連理解語境、呈現價值觀的能力也都提升了不少。

下集預告
----

今天我們聊的是一個模型是怎麼從 base 模型變成 chat 模型的第一步，也就是先經過 Instruction 微調，讓它能比較好地理解指令、做出像樣的回答。但其實這還只是開始而已。要讓一個 Chat 模型真正好用，還需要經過後續更複雜的調校流程——也就是我們稍早提到的 RLHF。不過 RLHF 這塊說實在不簡單，因為它牽涉到 "強化學習" 這類比較進階的概念。這次的系列文章不會深入講解這一部分，如果你真的很有興趣，推薦你去看這篇整理得很清楚的說明文：[這篇文章](https://zhuanlan.zhihu.com/p/677607581)。

而明天的內容會更延伸一下LLM的內容除了繼續帶你了解 Chat 版本模型，我也會教你怎麼用 base 模型來處理像是 Encoder 類型的任務。

---

<a id="day-29"></a>

## Day 29｜【Day 29】Decoder-only 模型也能搞定 NER？用 LLaMA3 找出個資
- 原文：https://ithelp.ithome.com.tw/articles/10396698

前言
--

為什麼今天特別想聊聊 base model 呢？因為跟那些早就被綁定特定任務的成品模型比起來，base model 靈活多了、可塑性也更高。我們可以根據需求把它變成聊天機器人、分類器、單輪對話模型，甚至是用來做資訊擷取都沒問題。這種彈性雖然帶來很多設計空間，但也代表你在微調策略、資料處理流程，甚至頭部設計上都得花點心思。

因此今天主要來告訴你，怎麼對資料集進行去識別化前處理、怎麼訓練，然後把之前提到的一些技巧整合起來，像是模型疊加、權重共享、QLoRA 量化等等。你可以把今天的內容當成是一篇總整理，來加深你對整個模型的實作進接近巧。

認識 B-I-O 標註方式
-------------

接下來我們要說到 `B-I-O（Begin-Inside-Outside）`這個東西，它是一種在自然語言處理中很常見的序列標註格式。主要是用來標記一句話裡哪些詞是屬於某個實體，像是人名、地名這類的東西。它的邏輯就是每個詞會有個標籤，告訴你它在實體裡的位置。

像是如果一個詞是某個實體的開頭，那就會標 B（Begin），像是 B-PER 表示是「人名」的開始；如果是在實體中但不是開頭，就標 I（Inside），像是 I-PER；至於不屬於任何實體的，就標 O，代表 Outside。舉個例子：

```
小明  去了 台北  101   。  
B-PER O   B-LOC I-LOC O
```

而在今天我們也會使用這種標註方是對模型進行訓練與評估

## 把 Decoder 當 Encoder？
-----------------------

做去識別化（De-ID）任務，常見會遇到兩件事：

第一要能判斷「這是不是敏感資訊？」

第二得準確標出它的**起訖位置**，有時甚至還得**生成替代的文字**來取代原本的內容。

現在的大型語言模型大多已經是多語言預訓練的，所以做跨語言的任務通常會比較好，這也讓 **Decoder-only** 架構在需要生成或彈性推理的 De-ID 場景中變得特別好用。尤其當你碰到那種 **比較少見的標籤** 時，模型往往可以靠它內建的語言知識把空缺補起來。

當然它也不是沒有缺點。Decoder-only 本質上是 **causal LM**，預測時只能看前面的上下文，沒辦法像雙向模型一樣，同時用到前後資訊。如果你的任務是**單語言、標註很明確、又不需要生成替代文字**，那傳統的 **Encoder-only** 架構其實會更省資源、更有效率。

但今天我們還是要用 Decoder-only 架構實作一次，這主要是讓你知道該怎麼樣設計模型head，還有怎麼做訓練與評估。

1. 讀取模型
-------

這次我們用的模型是 `meta-llama/Meta-Llama-3-8B`，也就是 Llama 3 的 base 版本。接下來的訓練就會以它為主角。

跟之前一樣，我們會對 Q、K、V、O 這幾個部分加上 LoRA，再進行量化處理。這邊的流程其實跟昨天寫的 `load_llama_model` 差不多，所以等等就會直接接著那段程式碼繼續往下寫。

```
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training
)

def load_llama_model(model_name='meta-llama/Meta-Llama-3-8B'):
    quantization_params = {
        'load_in_4bit': True,
        'bnb_4bit_quant_type': "nf4",
        'bnb_4bit_use_double_quant': True,
        'bnb_4bit_compute_dtype': torch.bfloat16
    }
    bnb_config = BitsAndBytesConfig(**quantization_params)

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        use_cache=False,
    )

    peft_params = {
        'r': 32,
        'target_modules': ["q_proj", "k_proj", "v_proj", "o_proj"],
        'lora_dropout': 0.1,
        'task_type': "CAUSAL_LM",
    }
    peft_config = LoraConfig(**peft_params)

    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
    model = get_peft_model(model, peft_config)

    return model, tokenizer

base_model, tokenizer = load_llama_model()
```

2. 讀取資料集
--------

我們此次使用的是來自 `ai4privacy/open-pii-masking-500k-ai4privacy` 的英文資料，並進行 NER 所需的 **BIO 標註格式** 前處理，同樣的整理後的資料集也已備份於我的 GitHub，方便你快速下載，我將每個資料整理成以下格式。

```
{
  "input": "ID de visitante: TJ6QSLSJ8J. Ciudad de residencia: Coyuca de Benítez",
  "spans": [
    {
      "start": 17,
      "end": 26,
      "type": "IDCARDNUM"
    },
    {
      "start": 51,
      "end": 67,
      "type": "CITY"
    }
  ],
  "language": "en"
}
```

資料中的 `input` 欄位是一段純文字，而 `spans` 則標註了該文字中含有個人識別資訊的區段，並標明其對應的類型（例如身份證號或城市名稱）。每個區段的位置是透過 `start` 和 `end` 兩個欄位定義，這些位置是以字元為單位來計算的，並非模型分詞後的 token 索引。因此在進行 BIO 編碼前，我們必須先使用 tokenizer 將文字轉換為 token，同時透過其 `offset_mapping` 功能將字元位置正確對應到 token 索引，如此才能將每個 token 標記為適當的 BIO 標籤，簡單來說就是以下的流程

1.   **取得所有實體類型，建立對應的 BIO 標籤清單**
2.   **對每筆資料進行 tokenizer 編碼，並取得 offset_mapping**
3.   **根據 offset_mapping 將字元級的 span 對應到 token 索引**
4.   **依照 BIO 標準為每個 token 標註對應的實體類型**
5.   **儲存標註後的 `input_ids`、`attention_mask`、`token_labels`、`start_positions`、`end_positions` 回原資料中**

也就是假設某個 `span` 指出位置 17 到 26 是一組 `IDCARDNUM`，我們透過 tokenizer 取得每個 token 對應的文字範圍（offsets），找出落在 17～26 的 token 索引範圍。

*   第一個落在範圍內的 token → 標註為 `B-IDCARDNUM`
*   後續落在範圍內的 tokens → 標註為 `I-IDCARDNUM`
*   未落在任何 span 中的 tokens → 標註為 `O`

最後我們產生的`start_positions` 與 `end_positions` 是額外提供的二進位序列，用於後續的線性分類器計算索引值，整體程式碼看起來就像下面這樣子

```
import json
from tqdm import tqdm
from sklearn.model_selection import train_test_split

def create_bio_labels(types):
    """建立 BIO 標籤系統"""
    bio_labels = ['O']
    for entity_type in sorted(types):
        bio_labels.append(f'B-{entity_type}')
        bio_labels.append(f'I-{entity_type}')
    return bio_labels

def preprocess_data_with_bio(data, tokenizer):
    """使用 BIO 編碼進行前處理"""
    types = sorted({span["type"] for d in data for span in d.get("spans", [])})
    bio_labels = create_bio_labels(types)
    bio2id = {label: i for i, label in enumerate(bio_labels)}
    id2bio = {i: label for label, i in bio2id.items()}

    for sample in tqdm(data, desc="BIO前處理"):
        text = sample["input"]
        encoding = tokenizer(text, return_offsets_mapping=True)
        offsets = encoding["offset_mapping"]
        input_ids = encoding["input_ids"]
        attention_mask = encoding["attention_mask"]
        seq_len = len(input_ids)

        token_labels = [bio2id['O']] * seq_len
        start_positions = [0.0] * seq_len
        end_positions = [0.0] * seq_len

        for span in sample.get("spans", []):
            span_type = span["type"]
            start_char, end_char = span["start"], span["end"]

            token_start, token_end = None, None
            for i, (s, e) in enumerate(offsets):
                if s <= start_char < e:
                    token_start = i
                    break
            for i, (s, e) in enumerate(offsets):
                if s < end_char <= e:
                    token_end = i
                    break
            if token_end is None:
                for i, (s, e) in enumerate(offsets):
                    if s >= end_char:
                        token_end = i - 1
                        break
            if token_end is None:
                token_end = len(offsets) - 1

            span["token_start"] = token_start
            span["token_end"] = token_end

            if token_start is not None and token_end is not None:
                for j in range(token_start, token_end + 1):
                    if j < seq_len:
                        if j == token_start:
                            token_labels[j] = bio2id[f'B-{span_type}']
                        else:
                            token_labels[j] = bio2id[f'I-{span_type}']
                start_positions[token_start] = 1.0
                end_positions[token_end] = 1.0

        sample.update({
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "token_labels": token_labels,
            "start_positions": start_positions,
            "end_positions": end_positions
        })

    return data, bio2id, id2bio

def load_limited_json(path, limit=None):
    """限制輸入 JSON 的最大筆數"""
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if limit is not None and len(data) > limit:
        data = data[:limit]
    return data
```

程式碼中的三個主要函式，各自負責資料前處理流程中的關鍵任務。`create_bio_labels(types)` 會根據資料中出現的所有實體類型，動態建立對應的 BIO 標籤清單，其中每個實體類型都會生成一組 B-（開頭）與 I-（內部）標籤，並加上通用的 O（非實體）標籤。

`preprocess_data_with_bio(data, tokenizer)` 則是整體資料處理的核心函式，負責將每筆文字資料透過 tokenizer 編碼，同時根據 offset_mapping 將字元級的實體區段位置轉換為 token 索引，並套用 BIO 標註規則，同時建立包含 `input_ids`、`attention_mask`、`token_labels`、`start_positions` 和 `end_positions` 等訓練所需欄位。而`load_limited_json(path, limit)` 則是一個簡易的資料載入函式，支援讀取 JSON 格式檔案，並可依需要限制讀入筆數，方便開發與測試階段快速驗證處理流程。

3. 建立線性層
--------

在這個階段我們設計並實作了一個名為 `DeIDModelBIO` 的自定義模型，專門用來處理個資辨識任務。這個模型的核心在於結合兩種關鍵任務BIO 序列標註與Span 起訖位置預測，讓模型能更全面地學習如何定位並標示具有敏感資訊的文字片段，因此先讓我們看看模型架構

```
import torch
import torch.nn as nn

class DeIDModelBIO(nn.Module):
    def __init__(self, base_model, num_bio_labels):
        super().__init__()
        self.num_labels = num_bio_labels
        self.model = base_model
        hidden_size = self.model.config.hidden_size

        # 共用中介層
        self.shared_proj = nn.Sequential(
            nn.Linear(hidden_size, hidden_size),
            nn.GELU(),
            nn.Dropout(0.1)
        )

        # BIO 標籤分類器
        self.token_classifier = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_size // 2, self.num_labels)
        )

        # Span 起訖點偵測
        self.start_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.GELU(),
            nn.Linear(hidden_size // 4, 1)
        )

        self.end_head = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 4),
            nn.GELU(),
            nn.Linear(hidden_size // 4, 1)
        )

        # 移動裝置
        self.main_device = next(self.model.parameters()).device
        self.shared_proj = self.shared_proj.to(self.main_device)
        self.token_classifier = self.token_classifier.to(self.main_device)
        self.start_head = self.start_head.to(self.main_device)
        self.end_head = self.end_head.to(self.main_device)

        # 損失函數
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100)
        self.bce_loss = nn.BCEWithLogitsLoss()

    def forward(self, input_ids, attention_mask,
                token_labels=None, start_positions=None, end_positions=None):

        outputs = self.model.base_model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            return_dict=True,
        )

        last_hidden = outputs.hidden_states[-1]
        shared_hidden = self.shared_proj(last_hidden)  # 共用中介層

        # BIO 標籤分類
        token_logits = self.token_classifier(shared_hidden)

        # Span 起訖點預測
        start_logits = self.start_head(shared_hidden).squeeze(-1)
        end_logits = self.end_head(shared_hidden).squeeze(-1)

        losses = {}
        total_loss = 0

        if token_labels is not None:
            token_loss = self.ce_loss(
                token_logits.view(-1, self.num_labels),
                token_labels.view(-1)
            )
            losses['token_loss'] = token_loss
            total_loss += token_loss

        if start_positions is not None and end_positions is not None:
            start_loss = self.bce_loss(start_logits, start_positions.float())
            end_loss = self.bce_loss(end_logits, end_positions.float())
            span_loss = (start_loss + end_loss) / 2
            losses['span_loss'] = span_loss
            total_loss += span_loss * 0.5

        losses['total_loss'] = total_loss

        return (
            losses.get('total_loss', None),
            losses.get('token_loss', None),
            losses.get('span_loss', None),
            token_logits,
            start_logits,
            end_logits,
        )

# 用法範例
model = DeIDModelBIO(base_model, len(bio2id))
```

我們的模型主體是 LLaMA，訓練時會先從 backbone 模型抽出最後一層的 hidden states。這些 hidden states 接著會先通過一層叫做 `shared_proj` 的中介層，做個基本的特徵轉換。這層設計成共用的，是為了讓後面兩個不同任務的 head 可以共享一部分參數，避免各做各的、浪費學習資源。

模型在訓練時會同時處理兩個任務，因此會算兩種損失來一起學習。

*   第一種是 **BIO 標籤分類的損失（token_loss）**，這邊是用 `CrossEntropyLoss` 做多類別分類。對於像是 padding 或沒有標註的 token（通常 index 是 -100），我們會把它們忽略掉，不讓它們影響學習。

*   第二種是 **Span 預測的損失（span_loss）**。這部分是針對每個 token 去預測它是不是某個實體的起點或終點，所以是個二元分類問題，用 `BCEWithLogitsLoss` 來處理。最終會把起點跟終點的 loss 平均，當作整體的 span loss。

這兩個 loss 加起來就是我們的總損失，這樣模型就能同時學到「這是什麼類別的實體」以及「實體的範圍在哪」。

5. 建立DataLoader
---------------

在建立 DataLoader 的時候，因為我們在前處理階段就已經把那些麻煩的 token 轉換處理好了，所以這邊其實只需要加上 padding、再把資料轉成 tensor 就可以用了，沒什麼額外複雜的步驟。

```
import json
import torch
from torch.utils.data import Dataset, DataLoader

# =====================
# Dataset 定義
# =====================
class DeIDDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data[idx]
        return {
            "input_ids": torch.tensor(sample["input_ids"], dtype=torch.long),
            "attention_mask": torch.tensor(sample["attention_mask"], dtype=torch.long),
            "token_labels": torch.tensor(sample["token_labels"], dtype=torch.long),
            "start_positions": torch.tensor(sample["start_positions"], dtype=torch.float),
            "end_positions": torch.tensor(sample["end_positions"], dtype=torch.float),
        }

def collate_fn(batch):
    batch_size = len(batch)
    max_len = max(len(b["input_ids"]) for b in batch)

    def pad_tensor(seq_list, pad_value=0):
        out = torch.full((batch_size, max_len), pad_value, dtype=seq_list[0].dtype)
        for i, x in enumerate(seq_list):
            out[i, :len(x)] = x
        return out

    input_ids = pad_tensor([b["input_ids"] for b in batch])
    attention_mask = pad_tensor([b["attention_mask"] for b in batch])
    token_labels = pad_tensor([b["token_labels"] for b in batch])
    start_positions = pad_tensor([b["start_positions"] for b in batch])
    end_positions = pad_tensor([b["end_positions"] for b in batch])

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "token_labels": token_labels,
        "start_positions": start_positions,
        "end_positions": end_positions,
    }

# 建立 Dataset 和 DataLoader
train_dataset = DeIDDataset(train_data)
valid_dataset = DeIDDataset(valid_data)
test_dataset = DeIDDataset(test_data)

train_loader = DataLoader(
    train_dataset,
    batch_size=4,
    shuffle=True,
    collate_fn=collate_fn,
    pin_memory=True
)

valid_loader = DataLoader(
    valid_dataset,
    batch_size=4,
    shuffle=False,
    collate_fn=collate_fn,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=4,
    shuffle=False,
    collate_fn=collate_fn,
    pin_memory=True
)
```

6. 模型訓練
-------

在模型訓練的過程中，我們不再詳述基本流程一樣主要透過 AdamW 作為優化器，並搭配 `get_cosine_with_hard_restarts_schedule_with_warmup` 排程器來控制學習率。

```
import torch.optim as optim
from transformers import get_cosine_with_hard_restarts_schedule_with_warmup
from trainer import Trainer

optimizer = optim.AdamW(model.parameters(), lr=5e-5)
scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(
        optimizer, 
        num_warmup_steps=len(train_loader) * 0.2, 
        num_training_steps=len(train_loader) * 10, 
        num_cycles=1, 
)

trainer = Trainer(
    epochs=10, 
    train_loader=train_loader, 
    valid_loader=valid_loader,
    model=model, 
    optimizer=optimizer,
    scheduler=scheduler,
    early_stopping=3,
)
trainer.train()
```

訓練結果如下：大概在第 4 到第 5 個 epoch 之間，雖然 training loss 持續往下掉，不過 validation loss 有一點點上升。

```
Train Epoch 4: 100%|██████████| 400/400 [03:55<00:00,  1.70it/s, loss=0.081]
Valid Epoch 4: 100%|██████████| 100/100 [00:20<00:00,  4.92it/s, loss=0.130]
Train Loss: 0.04836 | Valid Loss: 0.14970 | Best Loss: 0.14583

Train Epoch 5: 100%|██████████| 400/400 [03:56<00:00,  1.69it/s, loss=0.026]
Valid Epoch 5: 100%|██████████| 100/100 [00:20<00:00,  4.93it/s, loss=0.154]
Train Loss: 0.03182 | Valid Loss: 0.15171 | Best Loss: 0.14583
```

不過整體來看Valid Loss 雖然有些微波動，不過還算穩定，在可接受的範圍內，模型目前應該是可以拿來用的。

6. 模型評估
-------

接下來我們要讓模型進入評估階段，這次我們採用的是實體級別的評估方式，意思是我們**不是只看每個 token 分類得對不對**，而是更進一步去看：整個實體（起始位置、結束位置、類型）是不是都預測正確。會這樣做是因為在命名實體識別任務裡，只有完整標出一個實體的範圍與類型，才算真的有抓到目標。因此我們的評估流程會長這樣：

```
模型預測 BIO 標籤
        ↓
轉換成實體（從 BIO 標籤還原出起訖位置與類別）
        ↓
比對預測實體與真實標註
        ↓
計算 TP（正確預測）、FP（錯誤預測）、FN（漏掉的實體）
        ↓
算出 Precision / Recall / F1（整體與分類別）
        ↓
把結果顯示出來並儲存
```

整個邏輯其實很直覺，但程式碼就會相對複雜了(詳情計算方式請看註解)，我們這邊直接看評估程式與最終的輸出結果。

```
import numpy as np
from sklearn.metrics import classification_report, f1_score
from collections import defaultdict

def extract_entities_from_bio(token_labels, id2bio, tokens=None):
    """
    從 BIO 標籤序列中提取實體
    返回格式: [(start_idx, end_idx, entity_type), ...]
    """
    entities = []
    current_entity = None
    
    for idx, label_id in enumerate(token_labels):
        label = id2bio[label_id]
        
        if label.startswith('B-'):
            # 如果有正在處理的實體，先保存
            if current_entity is not None:
                entities.append(current_entity)
            # 開始新實體
            entity_type = label[2:]
            current_entity = {
                'start': idx,
                'end': idx,
                'type': entity_type
            }
        elif label.startswith('I-'):
            # 繼續當前實體
            if current_entity is not None:
                entity_type = label[2:]
                if current_entity['type'] == entity_type:
                    current_entity['end'] = idx
                else:
                    # 類型不匹配，保存舊實體，開始新實體
                    entities.append(current_entity)
                    current_entity = {
                        'start': idx,
                        'end': idx,
                        'type': entity_type
                    }
        else:  # 'O' 標籤
            if current_entity is not None:
                entities.append(current_entity)
                current_entity = None
    
    # 保存最後一個實體
    if current_entity is not None:
        entities.append(current_entity)
    
    return [(e['start'], e['end'], e['type']) for e in entities]

def calculate_entity_f1(model, test_loader, id2bio, device='cuda'):
    """
    計算實體級別的 Precision, Recall, F1
    """
    model.eval()
    
    all_pred_entities = []
    all_true_entities = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(test_loader, desc="評估中")):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            token_labels = batch['token_labels']
            
            # 前向傳播
            _, _, _, token_logits, start_logits, end_logits = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            
            # 獲取預測標籤
            pred_labels = torch.argmax(token_logits, dim=-1).cpu().numpy()
            true_labels = token_labels.numpy()
            attention_mask_np = attention_mask.cpu().numpy()
            
            # 對每個樣本處理
            batch_size = input_ids.size(0)
            for i in range(batch_size):
                # 獲取有效長度（去除 padding）
                valid_length = attention_mask_np[i].sum()
                
                pred_seq = pred_labels[i][:valid_length]
                true_seq = true_labels[i][:valid_length]
                
                # 提取實體
                pred_entities = extract_entities_from_bio(pred_seq, id2bio)
                true_entities = extract_entities_from_bio(true_seq, id2bio)
                
                # 添加批次索引以區分不同樣本
                sample_id = batch_idx * test_loader.batch_size + i
                pred_entities = [(sample_id, start, end, etype) for start, end, etype in pred_entities]
                true_entities = [(sample_id, start, end, etype) for start, end, etype in true_entities]
                
                all_pred_entities.extend(pred_entities)
                all_true_entities.extend(true_entities)
    
    # 轉換為集合以便計算
    pred_set = set(all_pred_entities)
    true_set = set(all_true_entities)
    
    # 計算 TP, FP, FN
    tp = len(pred_set & true_set)
    fp = len(pred_set - true_set)
    fn = len(true_set - pred_set)
    
    # 計算指標
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    # 按類型統計
    type_stats = defaultdict(lambda: {'tp': 0, 'fp': 0, 'fn': 0})
    
    for entity in pred_set & true_set:
        entity_type = entity[3]
        type_stats[entity_type]['tp'] += 1
    
    for entity in pred_set - true_set:
        entity_type = entity[3]
        type_stats[entity_type]['fp'] += 1
    
    for entity in true_set - pred_set:
        entity_type = entity[3]
        type_stats[entity_type]['fn'] += 1
    
    # 計算每個類型的 F1
    type_f1_scores = {}
    for entity_type, stats in type_stats.items():
        tp_t = stats['tp']
        fp_t = stats['fp']
        fn_t = stats['fn']
        
        prec_t = tp_t / (tp_t + fp_t) if (tp_t + fp_t) > 0 else 0
        rec_t = tp_t / (tp_t + fn_t) if (tp_t + fn_t) > 0 else 0
        f1_t = 2 * prec_t * rec_t / (prec_t + rec_t) if (prec_t + rec_t) > 0 else 0
        
        type_f1_scores[entity_type] = {
            'precision': prec_t,
            'recall': rec_t,
            'f1': f1_t,
            'support': tp_t + fn_t
        }
    
    results = {
        'overall': {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'tp': tp,
            'fp': fp,
            'fn': fn,
            'total_pred': len(pred_set),
            'total_true': len(true_set)
        },
        'by_type': type_f1_scores
    }
    
    return results

def print_evaluation_results(results):
    """
    美化輸出評估結果
    """
    print("\n" + "="*70)
    print("整體評估結果".center(70))
    print("="*70)
    
    overall = results['overall']
    print(f"\nPrecision: {overall['precision']:.4f}")
    print(f"Recall:    {overall['recall']:.4f}")
    print(f"F1 Score:  {overall['f1']:.4f}")
    print(f"\nTP: {overall['tp']}, FP: {overall['fp']}, FN: {overall['fn']}")
    print(f"Total Predicted: {overall['total_pred']}, Total True: {overall['total_true']}")
    
    print("\n" + "="*70)
    print("各類別評估結果".center(70))
    print("="*70)
    print(f"\n{'Entity Type':<20} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Support':<10}")
    print("-"*70)
    
    for entity_type, metrics in sorted(results['by_type'].items()):
        print(f"{entity_type:<20} {metrics['precision']:<12.4f} {metrics['recall']:<12.4f} "
              f"{metrics['f1']:<12.4f} {metrics['support']:<10}")
    
    print("="*70)

# =====================
# 在測試集上評估
# =====================
print("\n開始在測試集上評估...")

# 確保模型在正確的設備上
device = next(model.parameters()).device

# 計算 F1 分數
test_results = calculate_entity_f1(model, test_loader, id2bio, device=device)

# 輸出結果
print_evaluation_results(test_results)

# 保存結果到文件
import json
with open('test_evaluation_results.json', 'w', encoding='utf-8') as f:
    json.dump(test_results, f, ensure_ascii=False, indent=2)

print("\n評估結果已保存到 test_evaluation_results.json")
```

輸出結果：

```
======================================================================
                               各類別評估結果                                
======================================================================

Entity Type          Precision    Recall       F1           Support   
----------------------------------------------------------------------
AGE                  0.7708       0.9136       0.8362       81        
BUILDINGNUM          0.8659       0.7845       0.8232       181       
CITY                 0.4593       0.7045       0.5561       264       
CREDITCARDNUMBER     0.2963       0.5000       0.3721       32        
DATE                 0.7539       0.8283       0.7894       233       
DRIVERLICENSENUM     0.0217       0.0270       0.0241       37        
EMAIL                0.4674       0.5922       0.5225       206       
GENDER               0.3810       0.5926       0.4638       27        
GIVENNAME            0.4810       0.6153       0.5399       1461      
IDCARDNUM            0.2671       0.5286       0.3549       140       
PASSPORTNUM          0.0685       0.0833       0.0752       60        
SEX                  0.5000       0.3488       0.4110       43        
SOCIALNUM            0.0702       0.1026       0.0833       39        
STREET               0.3882       0.4783       0.4286       207       
SURNAME              0.2954       0.4167       0.3457       480       
TAXNUM               0.0972       0.2414       0.1386       29        
TELEPHONENUM         0.8320       0.9192       0.8734       334       
TIME                 0.8571       0.9143       0.8848       315       
TITLE                0.5706       0.7214       0.6372       140       
ZIPCODE              0.4651       0.5714       0.5128       70        
======================================================================
```

為進一步驗證不同架構對命名實體識別任務的影響，我也使用基於 Encoder-only 架構的模型（相關實作細節可於我的 GitHub 上查閱）執行了相同任務，並將其結果與前述主模型進行對照。

```
======================================================================
                               各類別評估結果                                
======================================================================

Entity Type          Precision    Recall       F1           Support   
----------------------------------------------------------------------
AGE                  0.8736       0.9383       0.9048       81        
BUILDINGNUM          0.9313       0.8232       0.8739       181       
CITY                 0.5632       0.7424       0.6405       264       
CREDITCARDNUMBER     0.8889       1.0000       0.9412       32        
DATE                 0.9871       0.9871       0.9871       233       
DRIVERLICENSENUM     0.0000       0.0000       0.0000       37        
EMAIL                0.9670       0.9951       0.9809       206       
GENDER               0.3922       0.7407       0.5128       27        
GIVENNAME            0.7420       0.7817       0.7613       1461      
IDCARDNUM            0.6596       0.4429       0.5299       140       
PASSPORTNUM          0.0826       0.1667       0.1105       60        
SEX                  0.0000       0.0000       0.0000       24        
SOCIALNUM            0.0000       0.0000       0.0000       39        
STREET               0.8107       0.8068       0.8087       207       
SURNAME              0.5150       0.6062       0.5569       480       
TAXNUM               0.3438       0.3793       0.3607       29        
TELEPHONENUM         0.9104       0.9132       0.9118       334       
TIME                 0.9749       0.9873       0.9811       315       
TITLE                0.6387       0.7333       0.6828       135       
ZIPCODE              0.6667       0.7714       0.7152       70        
======================================================================
```

基本上可以發現 Encoder 模型的表現明顯優於LLaMA3的版本，幾乎在所有實體類別上均獲得更高的 Precision、Recall 與 F1 分數。例如在格式結構明確的實體類別如 `CREDITCARDNUMBER`、`EMAIL`、`DATE`、`TIME` 和 `TELEPHONENUM` 上，Encoder 模型的 F1 分數皆突破 0.9，部分甚至接近完美，如 `DATE` 的 F1 分數為 0.9871，`EMAIL` 達到 0.9809。相較之下，先前模型在這些類別的預測表現雖尚可，但普遍偏低，顯示 Encoder 架構在處理規則性輸入上具有極高的敏感度與穩定性。

但也並非所有實體類別在 Encoder 架構下都獲得明顯改善。以 `DRIVERLICENSENUM`、`SEX` 與 `SOCIALNUM` 這三類為例，這些屬於少數標籤類別，模型的 F1 分數皆為 0，顯示即使在其他指標全面提升的情況下，Encoder-only 架構在處理極端稀疏或上下文極度依賴的實體上仍顯吃力。相對而言使用大型語言模型在這類低資源或冷門實體上表現反而更為出色。這很可能是因為 LLM 在預訓練階段已接觸過大量多樣化的識別樣本與背景知識，使得它在面對格式不一或語境不清的資訊時，能更靈活地做出預測。

7. 實際使用
-------

在實際部署模型到應用環境時，我們常會希望有一個結構清晰的推論介面來簡化使用流程。為此我設計一個`DeIDInference`的類別，專門用來處理文本中的實體辨識與去識別化任務。這個類別不僅將模型的推論邏輯封裝起來，還結合了遮蔽敏感資訊的功能，讓使用者可以快速使用。

```
class DeIDInference:
    def __init__(self, model, tokenizer, id2bio, device='cuda'):
        self.model = model
        self.tokenizer = tokenizer
        self.id2bio = id2bio
        self.device = device
        self.model.eval()
```

進行實體辨識的主方法為 `predict()`。這個方法接收一段文字作為輸入，首先會透過 tokenizer 將文字轉換為模型所需的格式，並記錄下各 token 在原始文字中的對應位置。接著模型會輸出每個 token 的分類結果與可能為實體起始或結束的機率分數。

```
def predict(self, text, threshold=0.5):
        """對輸入文本進行去識別化預測"""
        # Tokenize
        encoding = self.tokenizer(
            text,
            return_tensors='pt',
            return_offsets_mapping=True
        )
        
        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)
        offsets = encoding['offset_mapping'][0].tolist()
        
        # 推論
        with torch.no_grad():
            _, _, _, token_logits, start_logits, end_logits = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
        
        # 取得預測結果
        token_preds = torch.argmax(token_logits, dim=-1)[0].cpu().tolist()
        start_probs = torch.sigmoid(start_logits)[0].cpu().tolist()
        end_probs = torch.sigmoid(end_logits)[0].cpu().tolist()
        
        # 解析 BIO 標籤
        entities = self._extract_entities_from_bio(
            token_preds, offsets, text, start_probs, end_probs, threshold
        )
        
        return entities
```

不過由於這些預測資料會透過 `argmax` 取得最有可能的類別編號，但取得的索引卻會是偏移前的位子因此我們要把`offset` 資訊一併傳遞給 `_extract_entities_from_bio()` 方法，以還原文字中實際的實體位置與內容。

```
def _extract_entities_from_bio(self, token_preds, offsets, text, 
                                   start_probs, end_probs, threshold):
        """從 BIO 標籤提取實體"""
        entities = []
        current_entity = None
        
        for i, (pred_id, (start_char, end_char)) in enumerate(zip(token_preds, offsets)):
            bio_label = self.id2bio[pred_id]
            
            # 跳過特殊 token
            if start_char == end_char:
                continue
            
            if bio_label.startswith('B-'):
                # 儲存前一個實體
                if current_entity is not None:
                    entities.append(current_entity)
                
                # 開始新實體
                entity_type = bio_label[2:]
                current_entity = {
                    'type': entity_type,
                    'start': start_char,
                    'end': end_char,
                    'text': text[start_char:end_char],
                    'start_prob': start_probs[i],
                    'end_prob': end_probs[i]
                }
            
            elif bio_label.startswith('I-'):
                # 延續當前實體
                if current_entity is not None:
                    entity_type = bio_label[2:]
                    if current_entity['type'] == entity_type:
                        current_entity['end'] = end_char
                        current_entity['text'] = text[current_entity['start']:end_char]
                        current_entity['end_prob'] = end_probs[i]
            
            elif bio_label == 'O':
                # 結束當前實體
                if current_entity is not None:
                    entities.append(current_entity)
                    current_entity = None
        
        # 儲存最後一個實體
        if current_entity is not None:
            entities.append(current_entity)
        
        # 過濾低信心度的預測
        filtered_entities = [
            e for e in entities 
            if e['start_prob'] >= threshold or e['end_prob'] >= threshold
        ]
        
        return filtered_entities
```

`_extract_entities_from_bio()`會專門負責解析模型預測的 BIO 標籤，並重建出完整的實體段。簡單來說只是判斷是否為實體的開頭（B-）、內部（I-）或非實體（O）。一旦偵測到新的實體開頭，就會開始記錄其起點與類型，並延續後續相關 token。整個處理流程會持續到文本結束，並在最後根據機率閾值過濾掉信心度過低的實體，藉此提升輸出的可靠性。

除了辨識功能我們還要使用整合遮蔽機制的 `predict_and_mask()` 方法。這個方法會先進行實體預測，再根據偵測到的敏感資訊，從原文中逐一將其遮蔽。為了避免遮蔽過程中因文字長度改變導致位置錯亂，我們會將實體依照出現位置由後往前排序，並以對應類型的標籤（例如【NAME】）進行替換。

```
def predict_and_mask(self, text):
        """預測並遮蔽敏感資訊"""
        entities = self.predict(text)
        
        # 按照位置反向排序，從後往前替換
        entities.sort(key=lambda x: x['start'], reverse=True)
        
        masked_text = text
        for entity in entities:
            masked_text = (
                masked_text[:entity['start']] + 
                f"【{entity['type']}】" + 
                masked_text[entity['end']:]
            )
        
        return masked_text, entities
```

而在實際使用上，只需建立一個 DeIDInference 的實例，然後輸入欲分析的文字，即可透過 `predict() 取得所有識別出的實體資訊。若希望直接取得已去識別化的版本，只需呼叫`predict_and_mask()`，就能同時取得遮蔽後的文本與對應的實體列表。

```
inferencer = DeIDInference(model, tokenizer, id2bio)

# 測試文本
test_text = "Hallo Caterino, ich habe deine Formulare für den Kleingartenverein erhalten."
entities = inferencer.predict(test_text)

print("偵測到的敏感資訊：")
for entity in entities:
    print(f"  類型: {entity['type']}, 文本: {entity['text']}, "
        f"位置: [{entity['start']}, {entity['end']}), "
        f"信心度: start={entity['start_prob']:.3f}, end={entity['end_prob']:.3f}")

masked_text, entities = inferencer.predict_and_mask(test_text)
print(f"\n遮蔽後的文本：{masked_text}")
```

輸出結果：

```
偵測到的敏感資訊：
  類型: GIVENNAME, 文本:  Caterino, 位置: [5, 14), 信心度: start=1.000, end=0.923

遮蔽後的文本：Hallo【GIVENNAME】, ich habe deine Formulare für den Kleingartenverein erhalten.
```

這樣子我們不僅能將模型的推論邏輯從主流程中抽離，也讓整合變得更加簡便。只要模型、tokenizer 與標籤對照表準備妥當，開發者幾乎可以毫無痛點地將這個類別直接嵌入現有的系統中。這種結構也特別適合於微服務架構或資料處理pipeline的設計，只需在適當的位置調用 `predict()` 或 `predict_and_mask()`，就能立刻獲得所需的辨識結果或完成敏感資訊的遮蔽處理。

看到這裡，等於我們已經走完了從模型訓練到實際應用的完整流程。從最初的理論分析、資料預處理與模型設計，一路到訓練與驗證，再到最後推論階段的封裝與應用整合，每一個環節其實都為今天的主題鋪好了道路。而這最後一步，也不只是把模型跑起來而已，它象徵的是一個具備實務彈性的框架正式成型。

更重要的是這個框架不是死的。你可以將它視為一個可移植、可調整的模組基礎，在未來處理其他任務或導入不同模型時，依照實際需求加以改造、擴充。這樣一來，無論你要處理的是不同語言的文本，還是完全不同領域的實體辨識問題，都能夠依循這樣的邏輯脈絡快速搭建起應用層，減少重工，提高效率。這，才是機器學習走入現實世界時最需要的一種能力。

下集預告
----

在今天的實作中，其實還有一個值得深思的觀察。當我們發現 Decoder-only 架構在某些實體識別任務上的表現不如預期，這並不一定意味著 Decoder 架構本身不適合用於分類任務。更可能的原因是**我們設計的線性分類層太過粗糙**，無法有效捕捉模型內部豐富的語言表示。

事實上設計一個真正能與 Decoder 輸出深度互動、並擁有足夠容量與抽象能力的分類頭，本身就是一項高難度工程。這也是為什麼在先前的教學中，要從基本漸納入 QLoRA、NEFTune、參數量化技術、Trainer 策略、模型架構的改造觀念、甚至權重共享等概念，這些都與我們之前數學推導或模型架構拆解課程中學到的知識息息相關。

而若你夠了解想要把 Decoder-only 架構發揮到極致，我們還是得理解它的本質，Decoder 模型是以 **causal language modeling** 為核心設計，它天生最擅長的任務並非分類而是文字接龍。若要讓 Decoder 模型在分類任務中發揮更強推理能力，僅僅加上線性分類頭顯然不夠，甚至加 instruction prompt 也只是開始。

因此明天最後一天我會教你一個技巧如何讓 Decoder 模型以 **生成式方式進行實體識別**，進一步超越 Encoder 架構所能達到的分數。

---

<a id="day-30"></a>

## Day 30｜【Day 30】不是模型變強是你變懂 Decoder-only 訓練中的那些事
- 原文：https://ithelp.ithome.com.tw/articles/10397113

前言
--

今天我們要進一步探索如何更有效地使用 Decoder-only 模型進行微調。不過在正式進入主題之前，我想先帶入一點小巧思如果語言模型本身已經夠強大，那我們該怎麼引導它更聚焦在特定任務上？又比如當我們要讓它做文字接龍時，怎樣的輸入格式會更有利於它產生連貫且有邏輯的輸出？所以今天順著這個思路，我們今天的討論會聚焦在這些核心問題上。

我們仍然會沿用之前使用過的資料集來進行訓練，但這次不再透過加入分類器來預測文字的前後關係。相反地我們會更深入地探索 Decoder-only 架構本身的潛力與限制，看看如果完全依賴其生成能力，是否能夠達到相似甚至更好的效果。

1. 讀取模型
-------

同樣的我們這裡用了 `load_llama_model` 這個函數，目的就是把一個LLaMA模型載入進來，而且是用 4-bit 量化 的方式來減少記憶體使用，這邊你也應該很熟悉了就是量化、預處理、加入LoRA凍結參數，而在這裡我們同樣的加入neftune進行使用。

```
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training
)

def load_llama_model(model_name='meta-llama/Meta-Llama-3-8B'):
    quantization_params = {
        'load_in_4bit': True,
        'bnb_4bit_quant_type': "nf4",
        'bnb_4bit_use_double_quant': True,
        'bnb_4bit_compute_dtype': torch.bfloat16
    }
    bnb_config = BitsAndBytesConfig(**quantization_params)

    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
    tokenizer.pad_token = tokenizer.eos_token

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        use_cache=False,
    )

    peft_params = {
        'r': 32,
        'target_modules': ["q_proj", "k_proj", "v_proj", "o_proj"],
        'lora_dropout': 0.1,
        'task_type': "CAUSAL_LM",
    }
    peft_config = LoraConfig(**peft_params)

    model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
    model = get_peft_model(model, peft_config)

    return model, tokenizer
from transformers.modeling_utils import unwrap_model

def activate_neftune(model, neftune_noise_alpha = 5):
        unwrapped_model = unwrap_model(model)
        embeddings = unwrapped_model.base_model.model.get_input_embeddings()
        embeddings.neftune_noise_alpha = neftune_noise_alpha
        # hook embedding layer
        hook_handle = embeddings.register_forward_hook(neftune_post_forward_hook)
        
        return model
        
def neftune_post_forward_hook(module, input, output):
    # 公式來源:https://github.com/neelsjain/NEFTune
    # 論文網址:https://arxiv.org/abs/2310.05914
    if module.training:
        dims = torch.tensor(output.size(1) * output.size(2))
        mag_norm = module.neftune_noise_alpha / torch.sqrt(dims)
        output = output + torch.zeros_like(output).uniform_(-mag_norm, mag_norm)
            
    return output

model, tokenizer = load_llama_model()
model = activate_neftune(model)
```

有些人會希望透過加入像 `<TASK_START>` 或 `<MY_TAG>` 這類自定義 token，來讓模型更容易理解任務的格式或邏輯流程。這個做法在直覺上蠻合理的，畢竟多一層提示似乎可以幫助模型更精準地回應。不過事情沒那麼簡單，因為你是在使用 RoPE 架構的模型，這招往往會適得其反。

前面提到 RoPE 的設計原則是它假設輸入 token 的順序是穩定、連貫、而且已知的，如果你突然插進一個模型完全沒看過的新 token，它的位置編碼會出現偏差，導致模型搞不清楚這個 token 應該怎麼被解讀。結果就是它可能學不到這個 token 的語意，甚至還會誤判整段輸入的邏輯結構。

所以想讓這些新 token 在 RoPE 架構下真正發揮作用，其實要下很多功夫，像是手動擴展 RoPE 的位置範圍、微調 embedding 層，甚至要專門訓練這些 token 的位置與語意對應關係。對一般開發者來說，這類處理不但技術門檻高，而且風險也大，很容易得不償失，所以在這裡我們不去做使用是最好的選擇，但我們還是可以使用這一些標籤我們只需要把它視為基本的文字即可，而如果你要訓練我們可以這樣設定。

```
# 新增特殊 token
special_tokens = {"additional_special_tokens": ["<|SYSTEM|>", "<|USER|>", "<|ASSISTANT|>"]}
num_added = tokenizer.add_special_tokens(special_tokens)

# 擴展詞彙表大小以配合新 token
if num_added > 0:
    model.resize_token_embeddings(len(tokenizer))
    
    # 解凍 embedding 層，讓新 token 的 embedding 能被訓練到
    model.get_input_embeddings().weight.requires_grad = True
```

2. 資料處理
-------

在進行命名實體辨識任務時，這次將採用了一種不同於傳統 BIO 標註的策略。傳統方法中模型需學習每個 token 的位置信息（如 B-PER、I-LOC 等），這在處理 tokenizer 對齊或多語言場景時常會帶來額外複雜度。

而這一次是直接讓模型生成包含實體類別與實體名稱的文本結果，例如 `PER|小明、ORG|微軟`，以此達到同樣的任務目的，同時簡化資料處理流程。

```
import json
from tqdm import tqdm
from sklearn.model_selection import train_test_split

def load_limited_json(path, limit=None):
    """限制輸入 JSON 的最大筆數"""
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    if limit is not None and len(data) > limit:
        data = data[:limit]
    return data

# 載入資料
data = load_limited_json("train_data.json", limit=2000)
test_data = load_limited_json("test_data.json", limit=2000)

# 切分訓練與驗證集
train_data, valid_data = train_test_split(data, test_size=0.2, random_state=42)
```

簡單來說我們先讀取資料後將每一筆資料會先擷取出 input 文字與標註的 spans，接著將這些 spans 根據 start 與 end 字元位置對應到原始文字中，並組合成 type|mention 的格式。這些結果會用頓號 、 串接起來，作為模型最終要生成的 target。

並且這一次我們也加入 prompt 與特定模板。每筆資料會包裝成一個帶有 `<|SYSTEM|>`、`<|USER|>`、`<|ASSISTANT|>` 標記的完整輸入，其中 `<|ASSISTANT|>`部分就是模型的輸出目標，**我們只保留 <|ASSISTANT|> 段落的 token 作為 labels，其餘部份標記為 -100，從而讓模型只學習輸出對應的實體資訊。這不僅保留了上下文語境。**

這個概念有點像是利用生成模型的特性，讓它自己看著文字就知道該怎麼回應，而不是我們一個字一個字告訴它這是什麼詞、那是什麼意思。簡單來說不是硬塞規則給它，而是讓它能學習出如何轉換成特定格式，因此我們可以如此撰寫資料處理的程式碼。

```
import torch

def extract_entities(text, spans):
    entities = []
    for s in spans:
        token = text[s['start']:s['end']]
        entities.append(f'{s["type"]}|{token}')
    return entities

def to_blocks(items, system_prompt, tokenizer):
    result = []
    eos_token_id = tokenizer.eos_token_id

    for item in items:
        text = item.get('input', '')
        spans = item.get('spans', [])
        ents = extract_entities(text, spans)
        output_line = "、".join(ents) if ents else ""

        # 組合完整 prompt
        system_part = f"<|SYSTEM|>\n{system_prompt}\n<|USER|>\n{text}<|ASSISTANT|>\n"
        full_text = system_part + output_line + tokenizer.eos_token

        # tokenize 全部
        encoded = tokenizer(full_text, add_special_tokens=False)
        input_ids = encoded.input_ids
        attention_mask = encoded.attention_mask

        # 分開 tokenize 系統部分與輸出部分
        sys_enc = tokenizer(system_part, add_special_tokens=False)
        out_enc = tokenizer(output_line + tokenizer.eos_token, add_special_tokens=False)

        sys_len = len(sys_enc.input_ids)
        out_len = len(out_enc.input_ids)

        # 建立 labels: 系統部分 -100，assistant 輸出部分保留
        labels = [-100] * sys_len + input_ids[sys_len:sys_len + out_len]

        # 確保長度一致
        if len(labels) < len(input_ids):
            labels += [-100] * (len(input_ids) - len(labels))

        assert len(labels) == len(input_ids)

        result.append({
            "input_ids": torch.tensor(input_ids, dtype=torch.long),
            "attention_mask": torch.tensor(attention_mask, dtype=torch.long),
            "labels": torch.tensor(labels, dtype=torch.long)
        })

    return result

# 範例呼叫
train_blocks = to_blocks(train_data, "Extract entities from text.", tokenizer)
valid_blocks = to_blocks(valid_data, "Extract entities from text.", tokenizer)
test_blocks = to_blocks(test_data, "Extract entities from text.", tokenizer)
```

3. 建立Pytorch DataLoader
-----------------------

這次我們在生成的時候通常會用 left padding 的方式，所以在 Dataloader 裡就直接用 left padding 來處理。這樣做的主要好處是，當我們要拿到模型實際輸入的那部分文本時，用這種方式會比較直覺、方便地把它取出來。

```
input_len = (inputs["input_ids"][j] != tokenizer.pad_token_id).sum().item()
output_text = tokenizer.decode(output[input_len:], skip_special_tokens=True).strip()
```

這裡的程式碼其實跟我們之前寫的差不多，沒什麼太大的不同。

```
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

class DeIDDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

def left_pad_sequence(sequences, batch_first=False, padding_value=0):
    # 計算最長序列長度
    max_len = max([seq.size(0) for seq in sequences])
    padded_seqs = []
    for seq in sequences:
        pad_len = max_len - seq.size(0)
        # 在左側補 padding
        padded_seq = torch.cat([
            torch.full((pad_len,), padding_value, dtype=seq.dtype, device=seq.device),
            seq
        ], dim=0)
        padded_seqs.append(padded_seq)
    return torch.stack(padded_seqs, dim=0 if batch_first else 1)

def collate_fn(batch):
    input_ids = [item["input_ids"] for item in batch]
    attention_mask = [item["attention_mask"] for item in batch]
    labels = [item["labels"] for item in batch]

    # 左側補齊
    input_ids = left_pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.eos_token_id)
    attention_mask = left_pad_sequence(attention_mask, batch_first=True, padding_value=0)
    labels = left_pad_sequence(labels, batch_first=True, padding_value=-100)

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels,
    }

# 建立 Dataset 和 DataLoader
train_dataset = DeIDDataset(train_blocks)
valid_dataset = DeIDDataset(valid_blocks)
test_dataset = DeIDDataset(test_blocks)

train_loader = DataLoader(
    train_dataset,
    batch_size=4,
    shuffle=True,
    collate_fn=collate_fn,
    pin_memory=True
)

valid_loader = DataLoader(
    valid_dataset,
    batch_size=4,
    shuffle=False,
    collate_fn=collate_fn,
    pin_memory=True
)

test_loader = DataLoader(
    test_dataset,
    batch_size=4,
    shuffle=False,
    collate_fn=collate_fn,
    pin_memory=True
)
```

4. 訓練模型與驗證
----------

我們同樣延續昨天設定的訓練參數來繼續訓練，而最終輸出的結果如下：

```
Train Epoch 7: 100%|██████████| 400/400 [05:31<00:00,  1.21it/s, loss=0.017]
Valid Epoch 7: 100%|██████████| 100/100 [00:29<00:00,  3.43it/s, loss=0.089]
Train Loss: 0.02598 | Valid Loss: 0.12116 | Best Loss: 0.10862

Train Epoch 8: 100%|██████████| 400/400 [05:35<00:00,  1.19it/s, loss=0.025]
Valid Epoch 8: 100%|██████████| 100/100 [00:29<00:00,  3.41it/s, loss=0.089]
Train Loss: 0.02164 | Valid Loss: 0.12271 | Best Loss: 0.10862

--------------------------------------
| Model can't improve, stop training |
--------------------------------------
```

但語言模型在進行生成任務時，其最終的損失值未必能準確反映在下游任務中的效能，因此為了驗證實體擷取任務上的真實表現。我們同樣的使用 span-level 進行評估針對模型輸出的文字進行解析，不過在這裡的文字檢索策略我們只是進行簡單的文字匹配來找尋真實的索引值。

```
import re
import torch
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device).eval()

tokenizer.padding_side = "left"
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id
model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = True

system_prompt = "Extract entities from text."
pattern = r'([A-Z]+)\|([^、]+)'

batch_size = 8 
total_tp = total_fp = total_fn = 0
type_stats = {}

# 建立所有 prompt
prompts = [
    f"<|SYSTEM|>\n{system_prompt}\n<|USER|>\n{item['input']}\n<|ASSISTANT|>\n"
    for item in test_data
]

# 批次處理
for i in tqdm(range(0, len(prompts), batch_size), desc="Processing", ncols=80):
    batch_prompts = prompts[i:i + batch_size]
    inputs = tokenizer(batch_prompts, return_tensors="pt", padding=True).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id
        )

    for j, output in enumerate(outputs):
        text = test_data[i + j]["input"]
        item = test_data[i + j]
        input_len = (inputs["input_ids"][j] != tokenizer.pad_token_id).sum().item()
        output_text = tokenizer.decode(output[input_len:], skip_special_tokens=True).strip()

        entities = re.findall(pattern, output_text)
        preds = [{"type": t, "entity": e.strip()} for t, e in entities]

        used_positions = []
        pred_spans = []
        for p in preds:
            entity = p["entity"]
            search_start = 0
            while True:
                start_idx = text.find(entity, search_start)
                if start_idx == -1:
                    break
                end_idx = start_idx + len(entity)
                overlap = any(s < end_idx and e > start_idx for s, e in used_positions)
                if not overlap:
                    used_positions.append((start_idx, end_idx))
                    pred_spans.append((p["type"], start_idx, end_idx))
                    break
                search_start = start_idx + 1

        gold_spans = [(s["type"], s["start"], s["end"]) for s in item["spans"]]
        pred_set = set(pred_spans)
        gold_set = set(gold_spans)
        types = set(t for t, _, _ in gold_spans + pred_spans)

        for t in types:
            gold_t = {(a, b) for ty, a, b in gold_spans if ty == t}
            pred_t = {(a, b) for ty, a, b in pred_spans if ty == t}
            tp = len(gold_t & pred_t)
            fp = len(pred_t - gold_t)
            fn = len(gold_t - pred_t)

            total_tp += tp
            total_fp += fp
            total_fn += fn

            if t not in type_stats:
                type_stats[t] = {"tp": 0, "fp": 0, "fn": 0, "support": 0}
            type_stats[t]["tp"] += tp
            type_stats[t]["fp"] += fp
            type_stats[t]["fn"] += fn
            type_stats[t]["support"] += len(gold_t)

# 統計指標
precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) else 0.0
recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) else 0.0
f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0
support_total = sum(t["support"] for t in type_stats.values())

report_lines = []
report_lines.append("Span-level Entity Extraction Report\n")
report_lines.append(f"{'Entity Type':<20} {'Precision':<10} {'Recall':<10} {'F1':<10} {'Support':<10}\n")

for t, stats in type_stats.items():
    tp, fp, fn, sup = stats["tp"], stats["fp"], stats["fn"], stats["support"]
    p = tp / (tp + fp) if (tp + fp) else 0.0
    r = tp / (tp + fn) if (tp + fn) else 0.0
    f = 2 * p * r / (p + r) if (p + r) else 0.0
    report_lines.append(f"{t:<20} {p:<10.4f} {r:<10.4f} {f:<10.4f} {sup:<10d}\n")

report_lines.append("\nOverall:\n")
report_lines.append(f"Precision: {precision:.4f}\n")
report_lines.append(f"Recall:    {recall:.4f}\n")
report_lines.append(f"F1-score:  {f1:.4f}\n")
report_lines.append(f"Support:   {support_total}\n")

report = "".join(report_lines)
print(report)
```

輸出結果如下：

```
======================================================================
                               各類別評估結果                                
======================================================================

Entity Type          Precision    Recall       F1           Support   
----------------------------------------------------------------------
TELEPHONENUM         0.9515       0.5868       0.7259       334       
EMAIL                0.9928       0.6650       0.7965       206       
AGE                  0.8909       0.5833       0.7050       84        
SEX                  0.5333       0.1778       0.2667       45        
GIVENNAME            0.7504       0.3457       0.4733       1461      
DATE                 0.9815       0.6824       0.8051       233       
CITY                 0.7980       0.6136       0.6938       264       
STREET               0.8298       0.5652       0.6724       207       
BUILDINGNUM          0.9769       0.6318       0.7674       201       
ZIPCODE              0.8281       0.7571       0.7910       70        
SURNAME              0.5995       0.5333       0.5645       480       
TITLE                0.6444       0.2071       0.3135       140       
TIME                 0.9651       0.2635       0.4140       315       
IDCARDNUM            0.7195       0.4214       0.5315       140       
DRIVERLICENSENUM     0.2857       0.1081       0.1569       37        
CREDITCARDNUMBER     0.9091       0.3125       0.4651       32        
GENDER               0.5294       0.3333       0.4091       27        
PASSPORTNUM          0.4865       0.3000       0.3711       60        
SOCIALNUM            0.3333       0.3333       0.3333       39        
TAXNUM               0.2500       0.1379       0.1778       29        
======================================================================
```

在這次最終的實體擷取測試中，我們注意到一個蠻有意思的現象之前表現比較差、而且在資料中出現得不多的幾個類別像是`駕照號碼（DRIVERLICENSENUM）`、`信用卡號碼（CREDITCARDNUMBER）`、`性別（GENDER）`、`社會安全號碼（SOCIALNUM）`以及`稅號（TAXNUM）`這次的表現竟然有明顯進步。

這樣的結果某種程度上說明大型語言模型在理解語意標籤這塊，本身就有一定的優勢，即使這些實體在訓練資料中出現得很少，模型還是能夠靠語境中的語意線索，去推敲出這些標籤背後代表的是什麼類型的資訊。只是它還需要額外學習怎麼正確地分類跟標註，因此對於那些傳統方法不太容易處理的低資源類別，其實是很有幫助的。同樣的我也用正常的方式訓練了一次這個資料集而你可以看到，改良後的資料處理方式還是有比較佳的效果的。

```
======================================================================
                               各類別評估結果                                
======================================================================

Entity Type          Precision    Recall       F1           Support   
----------------------------------------------------------------------
EMAIL                1.0000       0.6311       0.7738       206       
TELEPHONENUM         0.9303       0.5599       0.6991       334       
SEX                  0.6250       0.1111       0.1887       45        
AGE                  0.8750       0.5833       0.7000       84        
GIVENNAME            0.6868       0.3032       0.4207       1461      
DATE                 0.9630       0.6695       0.7899       233       
STREET               0.7113       0.4879       0.5788       207       
CITY                 0.7766       0.5530       0.6460       264       
BUILDINGNUM          0.8779       0.5721       0.6928       201       
ZIPCODE              0.8200       0.5857       0.6833       70        
SURNAME              0.4771       0.4771       0.4771       480       
TITLE                0.6667       0.1571       0.2543       140       
TIME                 0.9767       0.2667       0.4190       315       
IDCARDNUM            0.6618       0.3214       0.4327       140       
DRIVERLICENSENUM     0.2727       0.2432       0.2571       37        
CREDITCARDNUMBER     0.6667       0.2500       0.3636       32        
GENDER               0.3500       0.2593       0.2979       27        
PASSPORTNUM          0.3600       0.1500       0.2118       60        
SOCIALNUM            0.3143       0.2821       0.2973       39        
TAXNUM               0.2500       0.1034       0.1463       29        
======================================================================
```

模型分析與討論
-------

這幾天的實驗下來其實我們已經可以觀察出一些有趣的現象。像是昨天我們把 LLaMA3 的隱狀態拿來做顯性分類，然後跟直接用 BERT 分類的結果做比較。結果蠻明確的第一個發現是：整體來說LLaMA3 在處理那種超級稀疏的資料時表現稍微好一點；但反過來，BERT 在面對那種雖然低頻但還是有語義線索的類別時，穩定性比較高。

不過我自己在猜啦LLaMA3 一旦加上線性層之後，它原本比較擅長處理稀疏資料的特性好像就被削弱了。這其實也蠻值得注意的，因為 decoder-only 的模型本來在資訊量比較少的情況下就已經不太容易抓到細節，再多一層線性轉換，可能就更難保留那些微弱但關鍵的訊號了。

所以我們今天就沒再加線性層，而是直接用比較傳統的推理方式來測原始的模型架構。結果也蠻有意思的，那些原本比較稀疏的類別，平均表現比昨天還要好一些。這某種程度上應該可以說明加上線性層反而讓模型在處理這類訊號的時候失去了一些敏感度，找不到原本應該能抓到的特徵了。

其實我們現在用的方法還有很多可以優化的空間。比方說，我們可以在 instruction 裡加入更細緻的特徵抽取規則，讓模型在推理時有更多指引。或者，也可以讓模型先學習這些規則，但之後在推理階段不定時把規則 Mask 掉，看看它在缺乏明確提示下的表現，這樣可以測試它內部學到的結構到底穩不穩定。另外像 NEFtune 這種方法，其實也可以先暫時移除來觀察它本身對模型的干擾程度。

decoder-only 的模型在這類應用上，其實還有很大的探索空間。像我們目前只能觀察單向輸出，那就可以試著引入 label attention 的機制，幫助模型對輸出標籤有更多理解，甚至建立起某種程度的「回推能力」。這樣的設計，或許能部分彌補它在單向處理上先天的限制。

但這些想法最終還是得靠你自己去深入思考。也正因如此我這 30 天一直反覆強調的，不只是模型能做什麼，而是你要理解它「為什麼能做、為什麼不能做」，這背後的架構設計、數學原理、學習機制，才是你真正該掌握的東西。

完賽心得
----

這 30 天的系列說長不長、說短也不短，反正就是夠我們折騰一輪了。大家能一路陪我走到這裡真的很不簡單，我知道內容節奏其實有點緊湊，有些地方可能還挺燒腦的。不過我盡量讓每一篇都有點連貫、有點鋪陳，不是單純丟概念，而是一步步從最基本的 wx + b 開始，慢慢帶出它怎麼跟 PyTorch 實作串起來，還有像 `cat`、加法這些數學符號在實務裡到底長什麼樣、怎麼用。

整個系列我最希望你們能真正掌握的，其實是 `Transformer`。這個架構說實話剛接觸的時候真的會讓人覺得：「到底在寫什麼？」所以我花了很多力氣在程式碼拆解上，不是拆完一次就丟給你們，而是每拆一次，就多加點新東西，像是循序漸進地把這個龐然大物拆小塊進行學習。

從 wx + b 開始，到 MLP、再到序列建模、Transformer，然後進入 GPT-2、Whisper 語音模型，甚至模型工程化和效能優化，這一路走來，說穿了就是希望讓這些抽象又艱澀的東西，變得「能看懂、能動手做」，讓你真的能感受到：「原來是這樣喔，我也能寫出來。」

所以如果你能把這些核心能力內化，那接下來的世界就更大了。你可以開始挑戰閱讀研究論文、試著理解那些前沿架構的設計邏輯，甚至自己動手實作一個模型出來。因為你現在已經會：

*   拆解並理解主流模型的運作方式，
*   根據任務需求選擇合適的策略，
*   把大問題分拆成一小步一小步的執行流程，
*   懂得怎麼處理資料、怎麼 debug、怎麼優化推論效能。

如果你有跟上這 30 天的節奏，那我相信你接下來一定能學得更深、更廣。接下來我會建議你可以開始碰一碰「強化學習」這塊，因為這個領域其實跟現在大型語言模型背後的一些關鍵技術有很深的關聯。那今天就先到這裡啦。如果明年還有機會的話，我很樂意再陪你們繼續走下一段路，一起把 AI 這條路走得更紮實、更有趣。咱們有緣再見！

這30天的完整程式碼在這裡：[https://github.com/AUSTIN2526/learning-wx-b-in-30-days](https://github.com/AUSTIN2526/learning-wx-b-in-30-days)
